python main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 500 --nn_refresh 0 --alpha_scale 0.5 --store_models testrun_regavg_200_300


/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 11.045, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.581, Central accuracy on global test data 15.938, Ensemble accuracy on global test data 27.793, Local accuracy on global train data 16.895, Local accuracy on local train data 82.033


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.333, Central accuracy on global test data 28.887, Ensemble accuracy on global test data 35.957, Local accuracy on global train data 18.647, Local accuracy on local train data 91.533


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.228, Central accuracy on global test data 40.713, Ensemble accuracy on global test data 41.455, Local accuracy on global train data 20.652, Local accuracy on local train data 94.233


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.177, Central accuracy on global test data 46.807, Ensemble accuracy on global test data 46.406, Local accuracy on global train data 18.516, Local accuracy on local train data 95.633


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.158, Central accuracy on global test data 41.553, Ensemble accuracy on global test data 40.547, Local accuracy on global train data 20.747, Local accuracy on local train data 95.883


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.142, Central accuracy on global test data 40.898, Ensemble accuracy on global test data 50.088, Local accuracy on global train data 19.507, Local accuracy on local train data 95.800


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.187, Central accuracy on global test data 66.494, Ensemble accuracy on global test data 63.086, Local accuracy on global train data 22.139, Local accuracy on local train data 94.700


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.122, Central accuracy on global test data 59.717, Ensemble accuracy on global test data 61.748, Local accuracy on global train data 23.013, Local accuracy on local train data 96.600


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.106, Central accuracy on global test data 68.125, Ensemble accuracy on global test data 66.094, Local accuracy on global train data 24.995, Local accuracy on local train data 96.717


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.103, Central accuracy on global test data 66.025, Ensemble accuracy on global test data 72.881, Local accuracy on global train data 26.353, Local accuracy on local train data 96.700


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.074, Central accuracy on global test data 57.021, Ensemble accuracy on global test data 63.223, Local accuracy on global train data 30.852, Local accuracy on local train data 97.817


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.130, Central accuracy on global test data 62.461, Ensemble accuracy on global test data 69.873, Local accuracy on global train data 31.140, Local accuracy on local train data 95.783


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.113, Central accuracy on global test data 77.656, Ensemble accuracy on global test data 76.914, Local accuracy on global train data 28.203, Local accuracy on local train data 96.533


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.114, Central accuracy on global test data 80.176, Ensemble accuracy on global test data 82.705, Local accuracy on global train data 38.223, Local accuracy on local train data 96.250


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.083, Central accuracy on global test data 82.803, Ensemble accuracy on global test data 83.584, Local accuracy on global train data 37.764, Local accuracy on local train data 97.267


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 83 {9, 2}
2 22 {0, 5}
3 69 {2, 3}
4 1 {8, 4}
5 54 {1, 6}
6 62 {0, 7}
7 36 {1, 3}
8 58 {9, 4}
9 63 {4, 5}
Round  16, Devices participated 10, Average loss 0.096, Central accuracy on global test data 84.463, Ensemble accuracy on global test data 84.326, Local accuracy on global train data 37.939, Local accuracy on local train data 97.050


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 68 {9, 6}
2 4 {8, 7}
3 10 {3, 7}
4 33 {5, 6}
5 96 {1, 3}
6 15 {5, 7}
7 64 {1, 2}
8 89 {0, 9}
9 54 {1, 6}
Round  17, Devices participated 10, Average loss 0.066, Central accuracy on global test data 85.479, Ensemble accuracy on global test data 83.662, Local accuracy on global train data 37.146, Local accuracy on local train data 98.100


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {1, 3}
1 22 {0, 5}
2 1 {8, 4}
3 40 {3, 5}
4 41 {0, 3}
5 52 {8, 7}
6 23 {1, 5}
7 75 {8, 3}
8 62 {0, 7}
9 11 {4, 5}
Round  18, Devices participated 10, Average loss 0.106, Central accuracy on global test data 73.027, Ensemble accuracy on global test data 81.572, Local accuracy on global train data 39.341, Local accuracy on local train data 96.317


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 27 {2, 5}
2 8 {8, 7}
3 34 {3, 7}
4 98 {4}
5 74 {5}
6 36 {1, 3}
7 78 {2, 3, 4}
8 85 {1, 4}
9 88 {0}
Round  19, Devices participated 10, Average loss 0.064, Central accuracy on global test data 82.295, Ensemble accuracy on global test data 81.533, Local accuracy on global train data 40.874, Local accuracy on local train data 98.117


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 2 {9}
2 83 {9, 2}
3 32 {0, 9}
4 78 {2, 3, 4}
5 39 {8, 0}
6 25 {8, 2}
7 56 {9, 2}
8 76 {1, 3}
9 35 {8, 2}
Round  20, Devices participated 10, Average loss 0.088, Central accuracy on global test data 76.309, Ensemble accuracy on global test data 74.453, Local accuracy on global train data 38.008, Local accuracy on local train data 97.100


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 2 {9}
2 40 {3, 5}
3 64 {1, 2}
4 12 {8, 9, 3}
5 94 {0, 1}
6 60 {2}
7 24 {4, 5}
8 61 {6}
9 29 {9}
Round  21, Devices participated 10, Average loss 0.067, Central accuracy on global test data 81.523, Ensemble accuracy on global test data 80.977, Local accuracy on global train data 42.888, Local accuracy on local train data 97.767


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 26 {9, 7}
2 80 {6}
3 38 {2, 7}
4 85 {1, 4}
5 86 {1, 2}
6 11 {4, 5}
7 5 {0, 6, 7}
8 98 {4}
9 96 {1, 3}
Round  22, Devices participated 10, Average loss 0.067, Central accuracy on global test data 85.693, Ensemble accuracy on global test data 85.557, Local accuracy on global train data 48.564, Local accuracy on local train data 97.733


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 41 {0, 3}
1 36 {1, 3}
2 92 {1, 3}
3 48 {3, 4}
4 96 {1, 3}
5 76 {1, 3}
6 77 {8, 1}
7 57 {4, 6}
8 78 {2, 3, 4}
9 28 {2, 7}
Round  23, Devices participated 10, Average loss 0.089, Central accuracy on global test data 68.555, Ensemble accuracy on global test data 67.881, Local accuracy on global train data 46.379, Local accuracy on local train data 97.167


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 65 {3, 4}
2 87 {8, 9}
3 31 {0, 1}
4 89 {0, 9}
5 10 {3, 7}
6 70 {8, 5}
7 44 {6, 7}
8 28 {2, 7}
9 41 {0, 3}
Round  24, Devices participated 10, Average loss 0.080, Central accuracy on global test data 84.990, Ensemble accuracy on global test data 84.824, Local accuracy on global train data 40.977, Local accuracy on local train data 97.450


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 18 {0, 6}
2 59 {0, 7}
3 53 {0, 7}
4 99 {8, 9}
5 72 {1, 9}
6 90 {8, 6, 7}
7 14 {9, 5}
8 13 {8, 5, 6}
9 80 {6}
Round  25, Devices participated 10, Average loss 0.066, Central accuracy on global test data 75.410, Ensemble accuracy on global test data 80.127, Local accuracy on global train data 45.725, Local accuracy on local train data 98.000


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 61 {6}
2 33 {5, 6}
3 12 {8, 9, 3}
4 87 {8, 9}
5 20 {2, 7}
6 3 {8, 6}
7 34 {3, 7}
8 9 {1}
9 63 {4, 5}
Round  26, Devices participated 10, Average loss 0.077, Central accuracy on global test data 75.293, Ensemble accuracy on global test data 81.338, Local accuracy on global train data 43.406, Local accuracy on local train data 97.583


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 31 {0, 1}
2 28 {2, 7}
3 90 {8, 6, 7}
4 19 {5, 6}
5 26 {9, 7}
6 73 {0, 6}
7 23 {1, 5}
8 11 {4, 5}
9 96 {1, 3}
Round  27, Devices participated 10, Average loss 0.075, Central accuracy on global test data 82.617, Ensemble accuracy on global test data 79.170, Local accuracy on global train data 47.524, Local accuracy on local train data 97.583


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 55 {4, 7}
2 1 {8, 4}
3 5 {0, 6, 7}
4 93 {9, 6}
5 53 {0, 7}
6 7 {0, 1}
7 96 {1, 3}
8 57 {4, 6}
9 64 {1, 2}
Round  28, Devices participated 10, Average loss 0.053, Central accuracy on global test data 83.398, Ensemble accuracy on global test data 84.287, Local accuracy on global train data 50.042, Local accuracy on local train data 98.400


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 46 {9, 6}
2 99 {8, 9}
3 78 {2, 3, 4}
4 33 {5, 6}
5 79 {0, 8}
6 63 {4, 5}
7 13 {8, 5, 6}
8 26 {9, 7}
9 10 {3, 7}
Round  29, Devices participated 10, Average loss 0.091, Central accuracy on global test data 85.908, Ensemble accuracy on global test data 86.670, Local accuracy on global train data 44.399, Local accuracy on local train data 97.000


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 21 {2, 5}
2 40 {3, 5}
3 74 {5}
4 49 {2, 7}
5 36 {1, 3}
6 1 {8, 4}
7 24 {4, 5}
8 71 {1, 5}
9 79 {0, 8}
Round  30, Devices participated 10, Average loss 0.070, Central accuracy on global test data 64.902, Ensemble accuracy on global test data 80.752, Local accuracy on global train data 40.940, Local accuracy on local train data 97.917


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(11.2373, device='cuda:0', grad_fn=<MaxBackward1>)
0 96 {1, 3}
1 68 {9, 6}
2 43 {8, 3}
3 85 {1, 4}
4 10 {3, 7}
5 2 {9}
6 83 {9, 2}
7 94 {0, 1}
8 0 {1, 2}
9 66 {1, 3}
Round  31, Devices participated 10, Average loss 0.070, Central accuracy on global test data 82.549, Ensemble accuracy on global test data 80.801, Local accuracy on global train data 44.243, Local accuracy on local train data 97.783


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 16 {0, 3}
2 32 {0, 9}
3 76 {1, 3}
4 4 {8, 7}
5 51 {4, 7}
6 92 {1, 3}
7 33 {5, 6}
8 27 {2, 5}
9 86 {1, 2}
Round  32, Devices participated 10, Average loss 0.060, Central accuracy on global test data 85.137, Ensemble accuracy on global test data 86.621, Local accuracy on global train data 47.891, Local accuracy on local train data 98.100


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 40 {3, 5}
2 15 {5, 7}
3 16 {0, 3}
4 66 {1, 3}
5 10 {3, 7}
6 87 {8, 9}
7 1 {8, 4}
8 2 {9}
9 41 {0, 3}
Round  33, Devices participated 10, Average loss 0.080, Central accuracy on global test data 83.965, Ensemble accuracy on global test data 81.191, Local accuracy on global train data 44.131, Local accuracy on local train data 97.333


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 15 {5, 7}
2 38 {2, 7}
3 44 {6, 7}
4 88 {0}
5 56 {9, 2}
6 19 {5, 6}
7 71 {1, 5}
8 29 {9}
9 61 {6}
Round  34, Devices participated 10, Average loss 0.044, Central accuracy on global test data 81.826, Ensemble accuracy on global test data 84.893, Local accuracy on global train data 49.885, Local accuracy on local train data 98.583


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 95 {3, 4}
1 67 {8, 1}
2 46 {9, 6}
3 57 {4, 6}
4 1 {8, 4}
5 27 {2, 5}
6 48 {3, 4}
7 44 {6, 7}
8 16 {0, 3}
9 87 {8, 9}
Round  35, Devices participated 10, Average loss 0.070, Central accuracy on global test data 77.793, Ensemble accuracy on global test data 86.504, Local accuracy on global train data 44.980, Local accuracy on local train data 97.867


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 0 {1, 2}
2 40 {3, 5}
3 32 {0, 9}
4 39 {8, 0}
5 76 {1, 3}
6 73 {0, 6}
7 79 {0, 8}
8 42 {8, 1}
9 24 {4, 5}
Round  36, Devices participated 10, Average loss 0.076, Central accuracy on global test data 80.566, Ensemble accuracy on global test data 86.650, Local accuracy on global train data 48.464, Local accuracy on local train data 97.550


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 91 {2, 6}
2 93 {9, 6}
3 36 {1, 3}
4 92 {1, 3}
5 58 {9, 4}
6 73 {0, 6}
7 77 {8, 1}
8 13 {8, 5, 6}
9 35 {8, 2}
Round  37, Devices participated 10, Average loss 0.092, Central accuracy on global test data 79.756, Ensemble accuracy on global test data 82.725, Local accuracy on global train data 51.716, Local accuracy on local train data 96.617


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 10 {3, 7}
2 35 {8, 2}
3 76 {1, 3}
4 90 {8, 6, 7}
5 41 {0, 3}
6 60 {2}
7 97 {4, 7}
8 19 {5, 6}
9 92 {1, 3}
Round  38, Devices participated 10, Average loss 0.067, Central accuracy on global test data 83.018, Ensemble accuracy on global test data 86.299, Local accuracy on global train data 45.269, Local accuracy on local train data 97.817


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 56 {9, 2}
2 50 {2, 3}
3 98 {4}
4 88 {0}
5 2 {9}
6 39 {8, 0}
7 22 {0, 5}
8 30 {0, 1, 2}
9 23 {1, 5}
Round  39, Devices participated 10, Average loss 0.059, Central accuracy on global test data 82.012, Ensemble accuracy on global test data 87.656, Local accuracy on global train data 50.259, Local accuracy on local train data 98.100


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 21 {2, 5}
2 87 {8, 9}
3 35 {8, 2}
4 34 {3, 7}
5 5 {0, 6, 7}
6 89 {0, 9}
7 44 {6, 7}
8 17 {3, 4}
9 65 {3, 4}
Round  40, Devices participated 10, Average loss 0.070, Central accuracy on global test data 88.115, Ensemble accuracy on global test data 88.467, Local accuracy on global train data 46.401, Local accuracy on local train data 97.733


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 42 {8, 1}
2 5 {0, 6, 7}
3 15 {5, 7}
4 71 {1, 5}
5 28 {2, 7}
6 12 {8, 9, 3}
7 22 {0, 5}
8 35 {8, 2}
9 14 {9, 5}
Round  41, Devices participated 10, Average loss 0.076, Central accuracy on global test data 88.252, Ensemble accuracy on global test data 86.367, Local accuracy on global train data 51.069, Local accuracy on local train data 97.683


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 45 {4, 5}
2 55 {4, 7}
3 25 {8, 2}
4 38 {2, 7}
5 91 {2, 6}
6 57 {4, 6}
7 61 {6}
8 83 {9, 2}
9 53 {0, 7}
Round  42, Devices participated 10, Average loss 0.060, Central accuracy on global test data 85.898, Ensemble accuracy on global test data 87.783, Local accuracy on global train data 52.854, Local accuracy on local train data 98.017


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 42 {8, 1}
2 89 {0, 9}
3 35 {8, 2}
4 69 {2, 3}
5 36 {1, 3}
6 74 {5}
7 53 {0, 7}
8 11 {4, 5}
9 68 {9, 6}
Round  43, Devices participated 10, Average loss 0.061, Central accuracy on global test data 86.113, Ensemble accuracy on global test data 87.881, Local accuracy on global train data 46.113, Local accuracy on local train data 97.983


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 75 {8, 3}
2 26 {9, 7}
3 31 {0, 1}
4 47 {9, 5}
5 13 {8, 5, 6}
6 36 {1, 3}
7 79 {0, 8}
8 60 {2}
9 12 {8, 9, 3}
Round  44, Devices participated 10, Average loss 0.089, Central accuracy on global test data 72.852, Ensemble accuracy on global test data 86.719, Local accuracy on global train data 49.397, Local accuracy on local train data 97.200


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 91 {2, 6}
2 12 {8, 9, 3}
3 7 {0, 1}
4 9 {1}
5 23 {1, 5}
6 51 {4, 7}
7 6 {1, 6}
8 43 {8, 3}
9 44 {6, 7}
Round  45, Devices participated 10, Average loss 0.068, Central accuracy on global test data 86.416, Ensemble accuracy on global test data 86.592, Local accuracy on global train data 58.521, Local accuracy on local train data 97.800


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 78 {2, 3, 4}
2 82 {8, 4}
3 68 {9, 6}
4 92 {1, 3}
5 30 {0, 1, 2}
6 12 {8, 9, 3}
7 58 {9, 4}
8 89 {0, 9}
9 44 {6, 7}
Round  46, Devices participated 10, Average loss 0.081, Central accuracy on global test data 87.012, Ensemble accuracy on global test data 85.957, Local accuracy on global train data 58.372, Local accuracy on local train data 97.667


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 65 {3, 4}
1 80 {6}
2 62 {0, 7}
3 19 {5, 6}
4 51 {4, 7}
5 92 {1, 3}
6 71 {1, 5}
7 48 {3, 4}
8 6 {1, 6}
9 60 {2}
Round  47, Devices participated 10, Average loss 0.047, Central accuracy on global test data 84.570, Ensemble accuracy on global test data 84.561, Local accuracy on global train data 56.404, Local accuracy on local train data 98.550


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 26 {9, 7}
2 5 {0, 6, 7}
3 88 {0}
4 83 {9, 2}
5 62 {0, 7}
6 75 {8, 3}
7 80 {6}
8 87 {8, 9}
9 36 {1, 3}
Round  48, Devices participated 10, Average loss 0.062, Central accuracy on global test data 87.227, Ensemble accuracy on global test data 86.670, Local accuracy on global train data 58.923, Local accuracy on local train data 97.933


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 16 {0, 3}
1 93 {9, 6}
2 17 {3, 4}
3 77 {8, 1}
4 88 {0}
5 91 {2, 6}
6 73 {0, 6}
7 76 {1, 3}
8 31 {0, 1}
9 47 {9, 5}
Round  49, Devices participated 10, Average loss 0.055, Central accuracy on global test data 81.914, Ensemble accuracy on global test data 85.342, Local accuracy on global train data 60.757, Local accuracy on local train data 98.417


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 5 {0, 6, 7}
2 24 {4, 5}
3 47 {9, 5}
4 98 {4}
5 15 {5, 7}
6 55 {4, 7}
7 58 {9, 4}
8 95 {3, 4}
9 32 {0, 9}
Round  50, Devices participated 10, Average loss 0.067, Central accuracy on global test data 77.676, Ensemble accuracy on global test data 87.842, Local accuracy on global train data 55.601, Local accuracy on local train data 97.833


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 11 {4, 5}
1 80 {6}
2 87 {8, 9}
3 70 {8, 5}
4 6 {1, 6}
5 91 {2, 6}
6 51 {4, 7}
7 89 {0, 9}
8 42 {8, 1}
9 62 {0, 7}
Round  51, Devices participated 10, Average loss 0.059, Central accuracy on global test data 87.119, Ensemble accuracy on global test data 87.559, Local accuracy on global train data 58.792, Local accuracy on local train data 98.200


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 11 {4, 5}
2 16 {0, 3}
3 81 {2, 3, 7}
4 13 {8, 5, 6}
5 25 {8, 2}
6 33 {5, 6}
7 92 {1, 3}
8 41 {0, 3}
9 48 {3, 4}
Round  52, Devices participated 10, Average loss 0.069, Central accuracy on global test data 84.834, Ensemble accuracy on global test data 86.533, Local accuracy on global train data 53.386, Local accuracy on local train data 97.883


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 99 {8, 9}
2 22 {0, 5}
3 33 {5, 6}
4 7 {0, 1}
5 8 {8, 7}
6 49 {2, 7}
7 23 {1, 5}
8 79 {0, 8}
9 15 {5, 7}
Round  53, Devices participated 10, Average loss 0.054, Central accuracy on global test data 74.990, Ensemble accuracy on global test data 84.199, Local accuracy on global train data 51.169, Local accuracy on local train data 98.217


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 46 {9, 6}
2 23 {1, 5}
3 84 {0, 9}
4 27 {2, 5}
5 4 {8, 7}
6 32 {0, 9}
7 64 {1, 2}
8 44 {6, 7}
9 79 {0, 8}
Round  54, Devices participated 10, Average loss 0.040, Central accuracy on global test data 78.232, Ensemble accuracy on global test data 84.043, Local accuracy on global train data 52.000, Local accuracy on local train data 98.767


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 55 {4, 7}
2 23 {1, 5}
3 56 {9, 2}
4 21 {2, 5}
5 52 {8, 7}
6 17 {3, 4}
7 57 {4, 6}
8 99 {8, 9}
9 71 {1, 5}
Round  55, Devices participated 10, Average loss 0.054, Central accuracy on global test data 86.445, Ensemble accuracy on global test data 84.922, Local accuracy on global train data 52.468, Local accuracy on local train data 98.333


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 43 {8, 3}
2 85 {1, 4}
3 22 {0, 5}
4 84 {0, 9}
5 0 {1, 2}
6 78 {2, 3, 4}
7 24 {4, 5}
8 65 {3, 4}
9 44 {6, 7}
Round  56, Devices participated 10, Average loss 0.058, Central accuracy on global test data 82.939, Ensemble accuracy on global test data 88.369, Local accuracy on global train data 57.427, Local accuracy on local train data 98.183


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 49 {2, 7}
2 0 {1, 2}
3 98 {4}
4 24 {4, 5}
5 42 {8, 1}
6 31 {0, 1}
7 57 {4, 6}
8 67 {8, 1}
9 19 {5, 6}
Round  57, Devices participated 10, Average loss 0.053, Central accuracy on global test data 82.979, Ensemble accuracy on global test data 83.486, Local accuracy on global train data 58.203, Local accuracy on local train data 98.483


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 50 {2, 3}
2 82 {8, 4}
3 54 {1, 6}
4 60 {2}
5 74 {5}
6 75 {8, 3}
7 26 {9, 7}
8 7 {0, 1}
9 1 {8, 4}
Round  58, Devices participated 10, Average loss 0.058, Central accuracy on global test data 86.338, Ensemble accuracy on global test data 87.334, Local accuracy on global train data 53.647, Local accuracy on local train data 97.950


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 79 {0, 8}
2 92 {1, 3}
3 43 {8, 3}
4 17 {3, 4}
5 66 {1, 3}
6 19 {5, 6}
7 51 {4, 7}
8 46 {9, 6}
9 32 {0, 9}
Round  59, Devices participated 10, Average loss 0.061, Central accuracy on global test data 89.170, Ensemble accuracy on global test data 88.252, Local accuracy on global train data 58.938, Local accuracy on local train data 98.067


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 19 {5, 6}
2 42 {8, 1}
3 21 {2, 5}
4 15 {5, 7}
5 50 {2, 3}
6 35 {8, 2}
7 81 {2, 3, 7}
8 65 {3, 4}
9 95 {3, 4}
Round  60, Devices participated 10, Average loss 0.065, Central accuracy on global test data 89.141, Ensemble accuracy on global test data 87.773, Local accuracy on global train data 52.256, Local accuracy on local train data 98.000


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 29 {9}
2 19 {5, 6}
3 60 {2}
4 35 {8, 2}
5 25 {8, 2}
6 93 {9, 6}
7 24 {4, 5}
8 18 {0, 6}
9 85 {1, 4}
Round  61, Devices participated 10, Average loss 0.050, Central accuracy on global test data 86.055, Ensemble accuracy on global test data 88.867, Local accuracy on global train data 57.285, Local accuracy on local train data 98.483


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 20 {2, 7}
1 78 {2, 3, 4}
2 96 {1, 3}
3 8 {8, 7}
4 39 {8, 0}
5 71 {1, 5}
6 28 {2, 7}
7 9 {1}
8 32 {0, 9}
9 29 {9}
Round  62, Devices participated 10, Average loss 0.051, Central accuracy on global test data 87.920, Ensemble accuracy on global test data 90.264, Local accuracy on global train data 53.882, Local accuracy on local train data 98.250


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 82 {8, 4}
1 17 {3, 4}
2 75 {8, 3}
3 68 {9, 6}
4 15 {5, 7}
5 33 {5, 6}
6 22 {0, 5}
7 23 {1, 5}
8 38 {2, 7}
9 90 {8, 6, 7}
Round  63, Devices participated 10, Average loss 0.050, Central accuracy on global test data 89.619, Ensemble accuracy on global test data 89.219, Local accuracy on global train data 57.188, Local accuracy on local train data 98.483


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {8, 9}
1 59 {0, 7}
2 39 {8, 0}
3 9 {1}
4 35 {8, 2}
5 73 {0, 6}
6 30 {0, 1, 2}
7 75 {8, 3}
8 60 {2}
9 4 {8, 7}
Round  64, Devices participated 10, Average loss 0.052, Central accuracy on global test data 87.051, Ensemble accuracy on global test data 86.875, Local accuracy on global train data 57.466, Local accuracy on local train data 98.400


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {3, 5}
1 97 {4, 7}
2 6 {1, 6}
3 12 {8, 9, 3}
4 13 {8, 5, 6}
5 98 {4}
6 70 {8, 5}
7 9 {1}
8 5 {0, 6, 7}
9 77 {8, 1}
Round  65, Devices participated 10, Average loss 0.072, Central accuracy on global test data 88.340, Ensemble accuracy on global test data 88.594, Local accuracy on global train data 64.771, Local accuracy on local train data 97.567


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 93 {9, 6}
2 88 {0}
3 16 {0, 3}
4 90 {8, 6, 7}
5 26 {9, 7}
6 49 {2, 7}
7 44 {6, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round  66, Devices participated 10, Average loss 0.053, Central accuracy on global test data 87.412, Ensemble accuracy on global test data 88.916, Local accuracy on global train data 62.007, Local accuracy on local train data 98.200


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {2, 3, 4}
1 11 {4, 5}
2 52 {8, 7}
3 82 {8, 4}
4 7 {0, 1}
5 62 {0, 7}
6 38 {2, 7}
7 41 {0, 3}
8 46 {9, 6}
9 13 {8, 5, 6}
Round  67, Devices participated 10, Average loss 0.045, Central accuracy on global test data 87.412, Ensemble accuracy on global test data 89.463, Local accuracy on global train data 59.602, Local accuracy on local train data 98.733


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {9, 2}
1 85 {1, 4}
2 24 {4, 5}
3 89 {0, 9}
4 44 {6, 7}
5 18 {0, 6}
6 97 {4, 7}
7 15 {5, 7}
8 6 {1, 6}
9 3 {8, 6}
Round  68, Devices participated 10, Average loss 0.036, Central accuracy on global test data 86.553, Ensemble accuracy on global test data 89.434, Local accuracy on global train data 62.261, Local accuracy on local train data 99.050


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {9, 5}
1 19 {5, 6}
2 62 {0, 7}
3 61 {6}
4 60 {2}
5 2 {9}
6 87 {8, 9}
7 16 {0, 3}
8 7 {0, 1}
9 89 {0, 9}
Round  69, Devices participated 10, Average loss 0.043, Central accuracy on global test data 78.193, Ensemble accuracy on global test data 86.230, Local accuracy on global train data 56.648, Local accuracy on local train data 98.700


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 70 {8, 5}
3 26 {9, 7}
4 48 {3, 4}
5 76 {1, 3}
6 97 {4, 7}
7 71 {1, 5}
8 59 {0, 7}
9 30 {0, 1, 2}
Round  70, Devices participated 10, Average loss 0.061, Central accuracy on global test data 88.154, Ensemble accuracy on global test data 89.062, Local accuracy on global train data 61.038, Local accuracy on local train data 97.967


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 49 {2, 7}
1 2 {9}
2 29 {9}
3 96 {1, 3}
4 71 {1, 5}
5 18 {0, 6}
6 88 {0}
7 40 {3, 5}
8 59 {0, 7}
9 65 {3, 4}
Round  71, Devices participated 10, Average loss 0.040, Central accuracy on global test data 86.670, Ensemble accuracy on global test data 87.568, Local accuracy on global train data 61.672, Local accuracy on local train data 98.700


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 31 {0, 1}
2 50 {2, 3}
3 38 {2, 7}
4 33 {5, 6}
5 51 {4, 7}
6 91 {2, 6}
7 68 {9, 6}
8 81 {2, 3, 7}
9 30 {0, 1, 2}
Round  72, Devices participated 10, Average loss 0.048, Central accuracy on global test data 90.205, Ensemble accuracy on global test data 88.682, Local accuracy on global train data 62.644, Local accuracy on local train data 98.433


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 18 {0, 6}
2 65 {3, 4}
3 32 {0, 9}
4 7 {0, 1}
5 92 {1, 3}
6 24 {4, 5}
7 86 {1, 2}
8 53 {0, 7}
9 67 {8, 1}
Round  73, Devices participated 10, Average loss 0.038, Central accuracy on global test data 90.342, Ensemble accuracy on global test data 89.658, Local accuracy on global train data 66.367, Local accuracy on local train data 98.867


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 86 {1, 2}
2 64 {1, 2}
3 57 {4, 6}
4 67 {8, 1}
5 94 {0, 1}
6 3 {8, 6}
7 19 {5, 6}
8 8 {8, 7}
9 69 {2, 3}
Round  74, Devices participated 10, Average loss 0.057, Central accuracy on global test data 89.580, Ensemble accuracy on global test data 89.326, Local accuracy on global train data 60.100, Local accuracy on local train data 98.050


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 26 {9, 7}
2 37 {4, 5, 7}
3 90 {8, 6, 7}
4 47 {9, 5}
5 89 {0, 9}
6 16 {0, 3}
7 65 {3, 4}
8 36 {1, 3}
9 61 {6}
Round  75, Devices participated 10, Average loss 0.054, Central accuracy on global test data 89.326, Ensemble accuracy on global test data 90.049, Local accuracy on global train data 62.012, Local accuracy on local train data 98.283


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {2, 5}
1 63 {4, 5}
2 77 {8, 1}
3 16 {0, 3}
4 36 {1, 3}
5 28 {2, 7}
6 59 {0, 7}
7 12 {8, 9, 3}
8 97 {4, 7}
9 55 {4, 7}
Round  76, Devices participated 10, Average loss 0.058, Central accuracy on global test data 87.314, Ensemble accuracy on global test data 90.029, Local accuracy on global train data 59.419, Local accuracy on local train data 97.917


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 55 {4, 7}
2 14 {9, 5}
3 80 {6}
4 74 {5}
5 25 {8, 2}
6 26 {9, 7}
7 57 {4, 6}
8 41 {0, 3}
9 22 {0, 5}
Round  77, Devices participated 10, Average loss 0.045, Central accuracy on global test data 89.004, Ensemble accuracy on global test data 89.980, Local accuracy on global train data 60.896, Local accuracy on local train data 98.483


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 79 {0, 8}
2 86 {1, 2}
3 13 {8, 5, 6}
4 21 {2, 5}
5 32 {0, 9}
6 95 {3, 4}
7 52 {8, 7}
8 27 {2, 5}
9 47 {9, 5}
Round  78, Devices participated 10, Average loss 0.042, Central accuracy on global test data 85.283, Ensemble accuracy on global test data 89.609, Local accuracy on global train data 56.843, Local accuracy on local train data 98.767


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 42 {8, 1}
2 0 {1, 2}
3 39 {8, 0}
4 54 {1, 6}
5 16 {0, 3}
6 33 {5, 6}
7 65 {3, 4}
8 83 {9, 2}
9 5 {0, 6, 7}
Round  79, Devices participated 10, Average loss 0.055, Central accuracy on global test data 89.102, Ensemble accuracy on global test data 90.869, Local accuracy on global train data 59.082, Local accuracy on local train data 98.450


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 19 {5, 6}
2 70 {8, 5}
3 91 {2, 6}
4 10 {3, 7}
5 86 {1, 2}
6 78 {2, 3, 4}
7 81 {2, 3, 7}
8 97 {4, 7}
9 22 {0, 5}
Round  80, Devices participated 10, Average loss 0.055, Central accuracy on global test data 89.248, Ensemble accuracy on global test data 90.068, Local accuracy on global train data 60.996, Local accuracy on local train data 98.283


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 12 {8, 9, 3}
1 79 {0, 8}
2 66 {1, 3}
3 20 {2, 7}
4 2 {9}
5 13 {8, 5, 6}
6 6 {1, 6}
7 19 {5, 6}
8 91 {2, 6}
9 29 {9}
Round  81, Devices participated 10, Average loss 0.047, Central accuracy on global test data 90.762, Ensemble accuracy on global test data 88.613, Local accuracy on global train data 62.778, Local accuracy on local train data 98.633


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 10 {3, 7}
2 21 {2, 5}
3 83 {9, 2}
4 59 {0, 7}
5 3 {8, 6}
6 46 {9, 6}
7 0 {1, 2}
8 16 {0, 3}
9 94 {0, 1}
Round  82, Devices participated 10, Average loss 0.038, Central accuracy on global test data 87.422, Ensemble accuracy on global test data 89.668, Local accuracy on global train data 59.919, Local accuracy on local train data 98.783


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {2, 5}
1 96 {1, 3}
2 36 {1, 3}
3 17 {3, 4}
4 74 {5}
5 46 {9, 6}
6 73 {0, 6}
7 80 {6}
8 20 {2, 7}
9 40 {3, 5}
Round  83, Devices participated 10, Average loss 0.042, Central accuracy on global test data 87.988, Ensemble accuracy on global test data 88.359, Local accuracy on global train data 60.366, Local accuracy on local train data 98.750


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 81 {2, 3, 7}
2 10 {3, 7}
3 91 {2, 6}
4 31 {0, 1}
5 4 {8, 7}
6 42 {8, 1}
7 26 {9, 7}
8 77 {8, 1}
9 99 {8, 9}
Round  84, Devices participated 10, Average loss 0.059, Central accuracy on global test data 90.674, Ensemble accuracy on global test data 89.307, Local accuracy on global train data 60.679, Local accuracy on local train data 98.117


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 78 {2, 3, 4}
2 93 {9, 6}
3 75 {8, 3}
4 31 {0, 1}
5 86 {1, 2}
6 7 {0, 1}
7 17 {3, 4}
8 77 {8, 1}
9 28 {2, 7}
Round  85, Devices participated 10, Average loss 0.046, Central accuracy on global test data 89.287, Ensemble accuracy on global test data 90.508, Local accuracy on global train data 63.789, Local accuracy on local train data 98.500


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 22 {0, 5}
2 44 {6, 7}
3 18 {0, 6}
4 68 {9, 6}
5 34 {3, 7}
6 91 {2, 6}
7 98 {4}
8 84 {0, 9}
9 83 {9, 2}
Round  86, Devices participated 10, Average loss 0.035, Central accuracy on global test data 87.256, Ensemble accuracy on global test data 89.961, Local accuracy on global train data 66.729, Local accuracy on local train data 98.950


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 59 {0, 7}
2 51 {4, 7}
3 42 {8, 1}
4 49 {2, 7}
5 69 {2, 3}
6 52 {8, 7}
7 37 {4, 5, 7}
8 2 {9}
9 4 {8, 7}
Round  87, Devices participated 10, Average loss 0.047, Central accuracy on global test data 90.195, Ensemble accuracy on global test data 89.795, Local accuracy on global train data 58.374, Local accuracy on local train data 98.600


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 10 {3, 7}
2 27 {2, 5}
3 34 {3, 7}
4 7 {0, 1}
5 76 {1, 3}
6 74 {5}
7 3 {8, 6}
8 59 {0, 7}
9 52 {8, 7}
Round  88, Devices participated 10, Average loss 0.034, Central accuracy on global test data 86.533, Ensemble accuracy on global test data 90.205, Local accuracy on global train data 57.673, Local accuracy on local train data 98.917


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {6, 7}
1 9 {1}
2 73 {0, 6}
3 3 {8, 6}
4 55 {4, 7}
5 81 {2, 3, 7}
6 17 {3, 4}
7 59 {0, 7}
8 97 {4, 7}
9 63 {4, 5}
Round  89, Devices participated 10, Average loss 0.034, Central accuracy on global test data 88.701, Ensemble accuracy on global test data 86.396, Local accuracy on global train data 67.126, Local accuracy on local train data 99.033


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 63 {4, 5}
1 0 {1, 2}
2 33 {5, 6}
3 88 {0}
4 93 {9, 6}
5 87 {8, 9}
6 72 {1, 9}
7 84 {0, 9}
8 86 {1, 2}
9 20 {2, 7}
Round  90, Devices participated 10, Average loss 0.039, Central accuracy on global test data 89.453, Ensemble accuracy on global test data 90.498, Local accuracy on global train data 60.254, Local accuracy on local train data 98.867


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 19 {5, 6}
1 95 {3, 4}
2 33 {5, 6}
3 92 {1, 3}
4 65 {3, 4}
5 69 {2, 3}
6 81 {2, 3, 7}
7 26 {9, 7}
8 14 {9, 5}
9 52 {8, 7}
Round  91, Devices participated 10, Average loss 0.061, Central accuracy on global test data 88.672, Ensemble accuracy on global test data 89.248, Local accuracy on global train data 60.186, Local accuracy on local train data 97.983


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 22 {0, 5}
1 62 {0, 7}
2 3 {8, 6}
3 14 {9, 5}
4 64 {1, 2}
5 96 {1, 3}
6 33 {5, 6}
7 97 {4, 7}
8 31 {0, 1}
9 42 {8, 1}
Round  92, Devices participated 10, Average loss 0.036, Central accuracy on global test data 90.742, Ensemble accuracy on global test data 90.713, Local accuracy on global train data 63.474, Local accuracy on local train data 98.750


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 37 {4, 5, 7}
1 10 {3, 7}
2 88 {0}
3 63 {4, 5}
4 53 {0, 7}
5 68 {9, 6}
6 2 {9}
7 52 {8, 7}
8 26 {9, 7}
9 59 {0, 7}
Round  93, Devices participated 10, Average loss 0.034, Central accuracy on global test data 88.789, Ensemble accuracy on global test data 90.166, Local accuracy on global train data 64.258, Local accuracy on local train data 98.983


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 71 {1, 5}
2 85 {1, 4}
3 49 {2, 7}
4 20 {2, 7}
5 10 {3, 7}
6 3 {8, 6}
7 86 {1, 2}
8 88 {0}
9 33 {5, 6}
Round  94, Devices participated 10, Average loss 0.032, Central accuracy on global test data 88.379, Ensemble accuracy on global test data 89.248, Local accuracy on global train data 66.580, Local accuracy on local train data 98.983


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {1}
1 0 {1, 2}
2 18 {0, 6}
3 66 {1, 3}
4 44 {6, 7}
5 17 {3, 4}
6 92 {1, 3}
7 95 {3, 4}
8 64 {1, 2}
9 96 {1, 3}
Round  95, Devices participated 10, Average loss 0.035, Central accuracy on global test data 75.439, Ensemble accuracy on global test data 85.908, Local accuracy on global train data 69.719, Local accuracy on local train data 98.933


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {8, 5}
1 58 {9, 4}
2 24 {4, 5}
3 57 {4, 6}
4 32 {0, 9}
5 19 {5, 6}
6 43 {8, 3}
7 65 {3, 4}
8 5 {0, 6, 7}
9 88 {0}
Round  96, Devices participated 10, Average loss 0.071, Central accuracy on global test data 84.434, Ensemble accuracy on global test data 86.914, Local accuracy on global train data 61.792, Local accuracy on local train data 97.750


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {6}
1 75 {8, 3}
2 68 {9, 6}
3 2 {9}
4 6 {1, 6}
5 26 {9, 7}
6 1 {8, 4}
7 56 {9, 2}
8 78 {2, 3, 4}
9 74 {5}
Round  97, Devices participated 10, Average loss 0.040, Central accuracy on global test data 87.861, Ensemble accuracy on global test data 87.109, Local accuracy on global train data 58.787, Local accuracy on local train data 98.850


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 57 {4, 6}
2 22 {0, 5}
3 90 {8, 6, 7}
4 46 {9, 6}
5 9 {1}
6 56 {9, 2}
7 99 {8, 9}
8 89 {0, 9}
9 4 {8, 7}
Round  98, Devices participated 10, Average loss 0.037, Central accuracy on global test data 85.547, Ensemble accuracy on global test data 87.676, Local accuracy on global train data 65.061, Local accuracy on local train data 98.883


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {9, 2}
1 41 {0, 3}
2 69 {2, 3}
3 7 {0, 1}
4 63 {4, 5}
5 51 {4, 7}
6 78 {2, 3, 4}
7 89 {0, 9}
8 43 {8, 3}
9 81 {2, 3, 7}
Round  99, Devices participated 10, Average loss 0.061, Central accuracy on global test data 87.451, Ensemble accuracy on global test data 90.273, Local accuracy on global train data 65.183, Local accuracy on local train data 98.067


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {5}
1 76 {1, 3}
2 14 {9, 5}
3 98 {4}
4 73 {0, 6}
5 37 {4, 5, 7}
6 94 {0, 1}
7 68 {9, 6}
8 39 {8, 0}
9 11 {4, 5}
Round 100, Devices participated 10, Average loss 0.033, Central accuracy on global test data 89.619, Ensemble accuracy on global test data 91.348, Local accuracy on global train data 64.751, Local accuracy on local train data 99.033


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 38 {2, 7}
1 78 {2, 3, 4}
2 17 {3, 4}
3 56 {9, 2}
4 90 {8, 6, 7}
5 34 {3, 7}
6 42 {8, 1}
7 0 {1, 2}
8 1 {8, 4}
9 12 {8, 9, 3}
Round 101, Devices participated 10, Average loss 0.046, Central accuracy on global test data 91.318, Ensemble accuracy on global test data 90.830, Local accuracy on global train data 63.904, Local accuracy on local train data 98.667


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 36 {1, 3}
2 95 {3, 4}
3 8 {8, 7}
4 99 {8, 9}
5 75 {8, 3}
6 93 {9, 6}
7 4 {8, 7}
8 24 {4, 5}
9 58 {9, 4}
Round 102, Devices participated 10, Average loss 0.054, Central accuracy on global test data 89.795, Ensemble accuracy on global test data 90.195, Local accuracy on global train data 62.090, Local accuracy on local train data 98.200


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 66 {1, 3}
1 46 {9, 6}
2 89 {0, 9}
3 23 {1, 5}
4 4 {8, 7}
5 97 {4, 7}
6 78 {2, 3, 4}
7 26 {9, 7}
8 79 {0, 8}
9 52 {8, 7}
Round 103, Devices participated 10, Average loss 0.038, Central accuracy on global test data 89.590, Ensemble accuracy on global test data 90.908, Local accuracy on global train data 64.290, Local accuracy on local train data 99.000


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 79 {0, 8}
2 23 {1, 5}
3 7 {0, 1}
4 97 {4, 7}
5 86 {1, 2}
6 72 {1, 9}
7 57 {4, 6}
8 70 {8, 5}
9 64 {1, 2}
Round 104, Devices participated 10, Average loss 0.037, Central accuracy on global test data 89.824, Ensemble accuracy on global test data 90.439, Local accuracy on global train data 64.441, Local accuracy on local train data 98.617


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 55 {4, 7}
1 14 {9, 5}
2 21 {2, 5}
3 43 {8, 3}
4 5 {0, 6, 7}
5 30 {0, 1, 2}
6 35 {8, 2}
7 15 {5, 7}
8 28 {2, 7}
9 34 {3, 7}
Round 105, Devices participated 10, Average loss 0.050, Central accuracy on global test data 90.146, Ensemble accuracy on global test data 91.426, Local accuracy on global train data 64.094, Local accuracy on local train data 98.267


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 51 {4, 7}
2 22 {0, 5}
3 85 {1, 4}
4 73 {0, 6}
5 90 {8, 6, 7}
6 4 {8, 7}
7 69 {2, 3}
8 75 {8, 3}
9 6 {1, 6}
Round 106, Devices participated 10, Average loss 0.048, Central accuracy on global test data 90.557, Ensemble accuracy on global test data 90.967, Local accuracy on global train data 69.692, Local accuracy on local train data 98.467


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {2, 3, 4}
1 98 {4}
2 48 {3, 4}
3 28 {2, 7}
4 76 {1, 3}
5 96 {1, 3}
6 97 {4, 7}
7 69 {2, 3}
8 66 {1, 3}
9 65 {3, 4}
Round 107, Devices participated 10, Average loss 0.050, Central accuracy on global test data 72.568, Ensemble accuracy on global test data 81.279, Local accuracy on global train data 65.869, Local accuracy on local train data 98.550


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 54 {1, 6}
2 70 {8, 5}
3 39 {8, 0}
4 89 {0, 9}
5 14 {9, 5}
6 77 {8, 1}
7 74 {5}
8 15 {5, 7}
9 88 {0}
Round 108, Devices participated 10, Average loss 0.051, Central accuracy on global test data 77.256, Ensemble accuracy on global test data 89.346, Local accuracy on global train data 59.734, Local accuracy on local train data 98.300


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 12 {8, 9, 3}
2 87 {8, 9}
3 84 {0, 9}
4 91 {2, 6}
5 62 {0, 7}
6 82 {8, 4}
7 57 {4, 6}
8 6 {1, 6}
9 21 {2, 5}
Round 109, Devices participated 10, Average loss 0.063, Central accuracy on global test data 85.996, Ensemble accuracy on global test data 87.744, Local accuracy on global train data 64.976, Local accuracy on local train data 98.067


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 99 {8, 9}
2 54 {1, 6}
3 55 {4, 7}
4 14 {9, 5}
5 63 {4, 5}
6 7 {0, 1}
7 88 {0}
8 91 {2, 6}
9 52 {8, 7}
Round 110, Devices participated 10, Average loss 0.030, Central accuracy on global test data 86.973, Ensemble accuracy on global test data 87.852, Local accuracy on global train data 69.453, Local accuracy on local train data 99.183


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {6, 7}
1 73 {0, 6}
2 39 {8, 0}
3 28 {2, 7}
4 18 {0, 6}
5 92 {1, 3}
6 52 {8, 7}
7 79 {0, 8}
8 54 {1, 6}
9 77 {8, 1}
Round 111, Devices participated 10, Average loss 0.034, Central accuracy on global test data 89.766, Ensemble accuracy on global test data 88.330, Local accuracy on global train data 69.844, Local accuracy on local train data 98.933


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 70 {8, 5}
2 54 {1, 6}
3 27 {2, 5}
4 89 {0, 9}
5 21 {2, 5}
6 42 {8, 1}
7 25 {8, 2}
8 60 {2}
9 49 {2, 7}
Round 112, Devices participated 10, Average loss 0.041, Central accuracy on global test data 88.613, Ensemble accuracy on global test data 89.092, Local accuracy on global train data 66.863, Local accuracy on local train data 98.633


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 56 {9, 2}
2 59 {0, 7}
3 80 {6}
4 62 {0, 7}
5 19 {5, 6}
6 42 {8, 1}
7 33 {5, 6}
8 72 {1, 9}
9 35 {8, 2}
Round 113, Devices participated 10, Average loss 0.036, Central accuracy on global test data 85.654, Ensemble accuracy on global test data 88.252, Local accuracy on global train data 66.807, Local accuracy on local train data 98.783


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 53 {0, 7}
1 17 {3, 4}
2 11 {4, 5}
3 78 {2, 3, 4}
4 66 {1, 3}
5 39 {8, 0}
6 94 {0, 1}
7 4 {8, 7}
8 47 {9, 5}
9 90 {8, 6, 7}
Round 114, Devices participated 10, Average loss 0.041, Central accuracy on global test data 91.270, Ensemble accuracy on global test data 92.402, Local accuracy on global train data 66.506, Local accuracy on local train data 98.783


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 48 {3, 4}
1 51 {4, 7}
2 34 {3, 7}
3 78 {2, 3, 4}
4 92 {1, 3}
5 54 {1, 6}
6 96 {1, 3}
7 84 {0, 9}
8 88 {0}
9 74 {5}
Round 115, Devices participated 10, Average loss 0.030, Central accuracy on global test data 88.066, Ensemble accuracy on global test data 88.115, Local accuracy on global train data 70.088, Local accuracy on local train data 99.200


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 15 {5, 7}
1 25 {8, 2}
2 50 {2, 3}
3 89 {0, 9}
4 72 {1, 9}
5 20 {2, 7}
6 85 {1, 4}
7 47 {9, 5}
8 1 {8, 4}
9 3 {8, 6}
Round 116, Devices participated 10, Average loss 0.043, Central accuracy on global test data 91.455, Ensemble accuracy on global test data 92.217, Local accuracy on global train data 62.996, Local accuracy on local train data 98.650


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 91 {2, 6}
1 39 {8, 0}
2 11 {4, 5}
3 12 {8, 9, 3}
4 93 {9, 6}
5 64 {1, 2}
6 30 {0, 1, 2}
7 79 {0, 8}
8 29 {9}
9 49 {2, 7}
Round 117, Devices participated 10, Average loss 0.035, Central accuracy on global test data 89.707, Ensemble accuracy on global test data 91.816, Local accuracy on global train data 69.277, Local accuracy on local train data 98.967


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 88 {0}
2 95 {3, 4}
3 65 {3, 4}
4 60 {2}
5 8 {8, 7}
6 46 {9, 6}
7 31 {0, 1}
8 63 {4, 5}
9 72 {1, 9}
Round 118, Devices participated 10, Average loss 0.031, Central accuracy on global test data 91.045, Ensemble accuracy on global test data 91.768, Local accuracy on global train data 65.393, Local accuracy on local train data 99.200


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 63 {4, 5}
2 58 {9, 4}
3 77 {8, 1}
4 95 {3, 4}
5 23 {1, 5}
6 61 {6}
7 29 {9}
8 91 {2, 6}
9 59 {0, 7}
Round 119, Devices participated 10, Average loss 0.039, Central accuracy on global test data 91.572, Ensemble accuracy on global test data 91.562, Local accuracy on global train data 71.621, Local accuracy on local train data 98.733


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 69 {2, 3}
2 44 {6, 7}
3 89 {0, 9}
4 49 {2, 7}
5 36 {1, 3}
6 77 {8, 1}
7 19 {5, 6}
8 92 {1, 3}
9 72 {1, 9}
Round 120, Devices participated 10, Average loss 0.044, Central accuracy on global test data 92.451, Ensemble accuracy on global test data 92.178, Local accuracy on global train data 67.329, Local accuracy on local train data 98.517


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 49 {2, 7}
2 1 {8, 4}
3 80 {6}
4 27 {2, 5}
5 9 {1}
6 83 {9, 2}
7 72 {1, 9}
8 76 {1, 3}
9 92 {1, 3}
Round 121, Devices participated 10, Average loss 0.028, Central accuracy on global test data 92.100, Ensemble accuracy on global test data 91.006, Local accuracy on global train data 71.663, Local accuracy on local train data 99.150


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 57 {4, 6}
2 54 {1, 6}
3 1 {8, 4}
4 80 {6}
5 76 {1, 3}
6 56 {9, 2}
7 39 {8, 0}
8 14 {9, 5}
9 20 {2, 7}
Round 122, Devices participated 10, Average loss 0.032, Central accuracy on global test data 91.914, Ensemble accuracy on global test data 92.432, Local accuracy on global train data 69.102, Local accuracy on local train data 99.133


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 6 {1, 6}
1 62 {0, 7}
2 31 {0, 1}
3 90 {8, 6, 7}
4 3 {8, 6}
5 51 {4, 7}
6 24 {4, 5}
7 91 {2, 6}
8 23 {1, 5}
9 55 {4, 7}
Round 123, Devices participated 10, Average loss 0.028, Central accuracy on global test data 91.924, Ensemble accuracy on global test data 90.312, Local accuracy on global train data 75.327, Local accuracy on local train data 99.217


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 18 {0, 6}
2 78 {2, 3, 4}
3 89 {0, 9}
4 27 {2, 5}
5 80 {6}
6 33 {5, 6}
7 86 {1, 2}
8 88 {0}
9 15 {5, 7}
Round 124, Devices participated 10, Average loss 0.026, Central accuracy on global test data 90.283, Ensemble accuracy on global test data 91.611, Local accuracy on global train data 76.047, Local accuracy on local train data 99.200


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 12 {8, 9, 3}
2 59 {0, 7}
3 60 {2}
4 73 {0, 6}
5 80 {6}
6 0 {1, 2}
7 10 {3, 7}
8 23 {1, 5}
9 13 {8, 5, 6}
Round 125, Devices participated 10, Average loss 0.031, Central accuracy on global test data 89.580, Ensemble accuracy on global test data 91.406, Local accuracy on global train data 73.862, Local accuracy on local train data 99.100


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 46 {9, 6}
2 54 {1, 6}
3 87 {8, 9}
4 84 {0, 9}
5 51 {4, 7}
6 27 {2, 5}
7 95 {3, 4}
8 21 {2, 5}
9 61 {6}
Round 126, Devices participated 10, Average loss 0.029, Central accuracy on global test data 91.064, Ensemble accuracy on global test data 91.592, Local accuracy on global train data 71.191, Local accuracy on local train data 99.133


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 6 {1, 6}
2 96 {1, 3}
3 98 {4}
4 27 {2, 5}
5 71 {1, 5}
6 18 {0, 6}
7 3 {8, 6}
8 1 {8, 4}
9 95 {3, 4}
Round 127, Devices participated 10, Average loss 0.026, Central accuracy on global test data 92.061, Ensemble accuracy on global test data 90.518, Local accuracy on global train data 73.337, Local accuracy on local train data 99.300


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 88 {0}
1 90 {8, 6, 7}
2 96 {1, 3}
3 35 {8, 2}
4 69 {2, 3}
5 11 {4, 5}
6 60 {2}
7 48 {3, 4}
8 23 {1, 5}
9 42 {8, 1}
Round 128, Devices participated 10, Average loss 0.035, Central accuracy on global test data 91.602, Ensemble accuracy on global test data 91.973, Local accuracy on global train data 69.600, Local accuracy on local train data 98.783


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 88 {0}
2 43 {8, 3}
3 57 {4, 6}
4 48 {3, 4}
5 99 {8, 9}
6 63 {4, 5}
7 53 {0, 7}
8 94 {0, 1}
9 55 {4, 7}
Round 129, Devices participated 10, Average loss 0.035, Central accuracy on global test data 91.455, Ensemble accuracy on global test data 89.775, Local accuracy on global train data 69.646, Local accuracy on local train data 99.017


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 75 {8, 3}
2 74 {5}
3 76 {1, 3}
4 39 {8, 0}
5 80 {6}
6 34 {3, 7}
7 12 {8, 9, 3}
8 60 {2}
9 20 {2, 7}
Round 130, Devices participated 10, Average loss 0.039, Central accuracy on global test data 90.400, Ensemble accuracy on global test data 91.025, Local accuracy on global train data 62.183, Local accuracy on local train data 98.817


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 20 {2, 7}
1 47 {9, 5}
2 38 {2, 7}
3 59 {0, 7}
4 64 {1, 2}
5 82 {8, 4}
6 60 {2}
7 45 {4, 5}
8 63 {4, 5}
9 85 {1, 4}
Round 131, Devices participated 10, Average loss 0.025, Central accuracy on global test data 90.957, Ensemble accuracy on global test data 88.848, Local accuracy on global train data 64.148, Local accuracy on local train data 99.200


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 6 {1, 6}
2 51 {4, 7}
3 4 {8, 7}
4 8 {8, 7}
5 22 {0, 5}
6 72 {1, 9}
7 86 {1, 2}
8 55 {4, 7}
9 12 {8, 9, 3}
Round 132, Devices participated 10, Average loss 0.033, Central accuracy on global test data 90.557, Ensemble accuracy on global test data 92.627, Local accuracy on global train data 71.624, Local accuracy on local train data 99.083


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 71 {1, 5}
2 84 {0, 9}
3 65 {3, 4}
4 61 {6}
5 83 {9, 2}
6 87 {8, 9}
7 53 {0, 7}
8 77 {8, 1}
9 51 {4, 7}
Round 133, Devices participated 10, Average loss 0.029, Central accuracy on global test data 91.611, Ensemble accuracy on global test data 92.070, Local accuracy on global train data 70.110, Local accuracy on local train data 99.117


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 71 {1, 5}
2 56 {9, 2}
3 63 {4, 5}
4 49 {2, 7}
5 44 {6, 7}
6 85 {1, 4}
7 76 {1, 3}
8 50 {2, 3}
9 93 {9, 6}
Round 134, Devices participated 10, Average loss 0.032, Central accuracy on global test data 92.363, Ensemble accuracy on global test data 92.559, Local accuracy on global train data 66.165, Local accuracy on local train data 98.950


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 52 {8, 7}
2 95 {3, 4}
3 68 {9, 6}
4 64 {1, 2}
5 15 {5, 7}
6 29 {9}
7 54 {1, 6}
8 43 {8, 3}
9 93 {9, 6}
Round 135, Devices participated 10, Average loss 0.027, Central accuracy on global test data 92.676, Ensemble accuracy on global test data 92.295, Local accuracy on global train data 69.192, Local accuracy on local train data 99.317


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 70 {8, 5}
2 25 {8, 2}
3 56 {9, 2}
4 87 {8, 9}
5 27 {2, 5}
6 94 {0, 1}
7 1 {8, 4}
8 74 {5}
9 44 {6, 7}
Round 136, Devices participated 10, Average loss 0.043, Central accuracy on global test data 92.305, Ensemble accuracy on global test data 90.469, Local accuracy on global train data 65.281, Local accuracy on local train data 98.500


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 7 {0, 1}
2 31 {0, 1}
3 45 {4, 5}
4 81 {2, 3, 7}
5 62 {0, 7}
6 23 {1, 5}
7 97 {4, 7}
8 18 {0, 6}
9 27 {2, 5}
Round 137, Devices participated 10, Average loss 0.023, Central accuracy on global test data 92.588, Ensemble accuracy on global test data 91.797, Local accuracy on global train data 73.440, Local accuracy on local train data 99.267


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 13 {8, 5, 6}
2 40 {3, 5}
3 64 {1, 2}
4 52 {8, 7}
5 44 {6, 7}
6 22 {0, 5}
7 48 {3, 4}
8 3 {8, 6}
9 46 {9, 6}
Round 138, Devices participated 10, Average loss 0.034, Central accuracy on global test data 92.627, Ensemble accuracy on global test data 92.109, Local accuracy on global train data 69.309, Local accuracy on local train data 99.067


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 9 {1}
2 72 {1, 9}
3 56 {9, 2}
4 13 {8, 5, 6}
5 98 {4}
6 21 {2, 5}
7 79 {0, 8}
8 71 {1, 5}
9 32 {0, 9}
Round 139, Devices participated 10, Average loss 0.017, Central accuracy on global test data 92.822, Ensemble accuracy on global test data 90.811, Local accuracy on global train data 73.213, Local accuracy on local train data 99.683


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 2 {9}
2 36 {1, 3}
3 91 {2, 6}
4 58 {9, 4}
5 81 {2, 3, 7}
6 87 {8, 9}
7 68 {9, 6}
8 25 {8, 2}
9 5 {0, 6, 7}
Round 140, Devices participated 10, Average loss 0.044, Central accuracy on global test data 88.740, Ensemble accuracy on global test data 89.561, Local accuracy on global train data 70.823, Local accuracy on local train data 98.533


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 41 {0, 3}
2 74 {5}
3 21 {2, 5}
4 97 {4, 7}
5 69 {2, 3}
6 4 {8, 7}
7 17 {3, 4}
8 3 {8, 6}
9 88 {0}
Round 141, Devices participated 10, Average loss 0.031, Central accuracy on global test data 91.885, Ensemble accuracy on global test data 92.500, Local accuracy on global train data 64.854, Local accuracy on local train data 99.000


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 19 {5, 6}
2 97 {4, 7}
3 75 {8, 3}
4 60 {2}
5 93 {9, 6}
6 78 {2, 3, 4}
7 40 {3, 5}
8 32 {0, 9}
9 87 {8, 9}
Round 142, Devices participated 10, Average loss 0.040, Central accuracy on global test data 91.504, Ensemble accuracy on global test data 91.914, Local accuracy on global train data 67.827, Local accuracy on local train data 98.850


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {8, 5}
1 59 {0, 7}
2 68 {9, 6}
3 16 {0, 3}
4 36 {1, 3}
5 72 {1, 9}
6 71 {1, 5}
7 45 {4, 5}
8 91 {2, 6}
9 56 {9, 2}
Round 143, Devices participated 10, Average loss 0.028, Central accuracy on global test data 91.523, Ensemble accuracy on global test data 92.305, Local accuracy on global train data 69.683, Local accuracy on local train data 99.083


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {9, 2}
1 81 {2, 3, 7}
2 9 {1}
3 39 {8, 0}
4 2 {9}
5 67 {8, 1}
6 66 {1, 3}
7 98 {4}
8 80 {6}
9 51 {4, 7}
Round 144, Devices participated 10, Average loss 0.026, Central accuracy on global test data 91.221, Ensemble accuracy on global test data 92.490, Local accuracy on global train data 71.152, Local accuracy on local train data 99.317


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 43 {8, 3}
1 64 {1, 2}
2 42 {8, 1}
3 4 {8, 7}
4 78 {2, 3, 4}
5 0 {1, 2}
6 39 {8, 0}
7 57 {4, 6}
8 37 {4, 5, 7}
9 98 {4}
Round 145, Devices participated 10, Average loss 0.036, Central accuracy on global test data 91.416, Ensemble accuracy on global test data 91.045, Local accuracy on global train data 70.479, Local accuracy on local train data 98.900


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 11 {4, 5}
1 6 {1, 6}
2 16 {0, 3}
3 48 {3, 4}
4 14 {9, 5}
5 19 {5, 6}
6 68 {9, 6}
7 82 {8, 4}
8 94 {0, 1}
9 39 {8, 0}
Round 146, Devices participated 10, Average loss 0.030, Central accuracy on global test data 90.752, Ensemble accuracy on global test data 91.904, Local accuracy on global train data 67.937, Local accuracy on local train data 99.067


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 7 {0, 1}
2 93 {9, 6}
3 77 {8, 1}
4 78 {2, 3, 4}
5 38 {2, 7}
6 46 {9, 6}
7 99 {8, 9}
8 42 {8, 1}
9 54 {1, 6}
Round 147, Devices participated 10, Average loss 0.029, Central accuracy on global test data 91.992, Ensemble accuracy on global test data 91.357, Local accuracy on global train data 72.646, Local accuracy on local train data 99.183


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 70 {8, 5}
2 71 {1, 5}
3 57 {4, 6}
4 63 {4, 5}
5 73 {0, 6}
6 69 {2, 3}
7 17 {3, 4}
8 48 {3, 4}
9 47 {9, 5}
Round 148, Devices participated 10, Average loss 0.042, Central accuracy on global test data 91.113, Ensemble accuracy on global test data 89.561, Local accuracy on global train data 63.904, Local accuracy on local train data 98.667


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 42 {8, 1}
2 60 {2}
3 43 {8, 3}
4 85 {1, 4}
5 48 {3, 4}
6 1 {8, 4}
7 91 {2, 6}
8 22 {0, 5}
9 26 {9, 7}
Round 149, Devices participated 10, Average loss 0.038, Central accuracy on global test data 87.930, Ensemble accuracy on global test data 91.143, Local accuracy on global train data 68.901, Local accuracy on local train data 98.683


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 3 {8, 6}
2 23 {1, 5}
3 96 {1, 3}
4 47 {9, 5}
5 32 {0, 9}
6 52 {8, 7}
7 50 {2, 3}
8 82 {8, 4}
9 28 {2, 7}
Round 150, Devices participated 10, Average loss 0.032, Central accuracy on global test data 91.895, Ensemble accuracy on global test data 92.822, Local accuracy on global train data 62.654, Local accuracy on local train data 98.983


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 55 {4, 7}
2 26 {9, 7}
3 94 {0, 1}
4 14 {9, 5}
5 72 {1, 9}
6 5 {0, 6, 7}
7 50 {2, 3}
8 91 {2, 6}
9 42 {8, 1}
Round 151, Devices participated 10, Average loss 0.032, Central accuracy on global test data 92.500, Ensemble accuracy on global test data 92.266, Local accuracy on global train data 72.180, Local accuracy on local train data 99.083


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 75 {8, 3}
1 69 {2, 3}
2 0 {1, 2}
3 7 {0, 1}
4 53 {0, 7}
5 77 {8, 1}
6 67 {8, 1}
7 64 {1, 2}
8 62 {0, 7}
9 73 {0, 6}
Round 152, Devices participated 10, Average loss 0.040, Central accuracy on global test data 92.314, Ensemble accuracy on global test data 91.992, Local accuracy on global train data 75.754, Local accuracy on local train data 98.750


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {9, 5}
1 27 {2, 5}
2 66 {1, 3}
3 5 {0, 6, 7}
4 15 {5, 7}
5 75 {8, 3}
6 91 {2, 6}
7 28 {2, 7}
8 63 {4, 5}
9 62 {0, 7}
Round 153, Devices participated 10, Average loss 0.027, Central accuracy on global test data 92.871, Ensemble accuracy on global test data 92.812, Local accuracy on global train data 69.749, Local accuracy on local train data 99.133


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {5}
1 39 {8, 0}
2 76 {1, 3}
3 51 {4, 7}
4 46 {9, 6}
5 12 {8, 9, 3}
6 95 {3, 4}
7 17 {3, 4}
8 68 {9, 6}
9 93 {9, 6}
Round 154, Devices participated 10, Average loss 0.029, Central accuracy on global test data 93.145, Ensemble accuracy on global test data 91.885, Local accuracy on global train data 69.431, Local accuracy on local train data 99.117


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 28 {2, 7}
2 88 {0}
3 96 {1, 3}
4 20 {2, 7}
5 23 {1, 5}
6 19 {5, 6}
7 15 {5, 7}
8 91 {2, 6}
9 11 {4, 5}
Round 155, Devices participated 10, Average loss 0.025, Central accuracy on global test data 92.852, Ensemble accuracy on global test data 91.768, Local accuracy on global train data 74.456, Local accuracy on local train data 99.233


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 18 {0, 6}
1 46 {9, 6}
2 87 {8, 9}
3 61 {6}
4 79 {0, 8}
5 50 {2, 3}
6 69 {2, 3}
7 97 {4, 7}
8 72 {1, 9}
9 37 {4, 5, 7}
Round 156, Devices participated 10, Average loss 0.034, Central accuracy on global test data 92.715, Ensemble accuracy on global test data 92.070, Local accuracy on global train data 70.796, Local accuracy on local train data 98.850


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 57 {4, 6}
2 53 {0, 7}
3 30 {0, 1, 2}
4 72 {1, 9}
5 9 {1}
6 37 {4, 5, 7}
7 19 {5, 6}
8 5 {0, 6, 7}
9 14 {9, 5}
Round 157, Devices participated 10, Average loss 0.028, Central accuracy on global test data 92.354, Ensemble accuracy on global test data 91.992, Local accuracy on global train data 74.360, Local accuracy on local train data 99.233


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 67 {8, 1}
2 61 {6}
3 35 {8, 2}
4 96 {1, 3}
5 86 {1, 2}
6 63 {4, 5}
7 9 {1}
8 8 {8, 7}
9 36 {1, 3}
Round 158, Devices participated 10, Average loss 0.026, Central accuracy on global test data 92.520, Ensemble accuracy on global test data 92.217, Local accuracy on global train data 69.382, Local accuracy on local train data 99.167


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {0, 9}
1 41 {0, 3}
2 20 {2, 7}
3 92 {1, 3}
4 67 {8, 1}
5 44 {6, 7}
6 61 {6}
7 77 {8, 1}
8 3 {8, 6}
9 9 {1}
Round 159, Devices participated 10, Average loss 0.022, Central accuracy on global test data 90.156, Ensemble accuracy on global test data 91.436, Local accuracy on global train data 77.424, Local accuracy on local train data 99.417


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {8, 9}
1 20 {2, 7}
2 97 {4, 7}
3 50 {2, 3}
4 35 {8, 2}
5 68 {9, 6}
6 33 {5, 6}
7 98 {4}
8 53 {0, 7}
9 96 {1, 3}
Round 160, Devices participated 10, Average loss 0.037, Central accuracy on global test data 92.666, Ensemble accuracy on global test data 92.510, Local accuracy on global train data 68.813, Local accuracy on local train data 98.950


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {0, 9}
1 57 {4, 6}
2 26 {9, 7}
3 69 {2, 3}
4 23 {1, 5}
5 54 {1, 6}
6 68 {9, 6}
7 92 {1, 3}
8 4 {8, 7}
9 82 {8, 4}
Round 161, Devices participated 10, Average loss 0.033, Central accuracy on global test data 92.686, Ensemble accuracy on global test data 92.285, Local accuracy on global train data 73.760, Local accuracy on local train data 98.883


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 31 {0, 1}
2 11 {4, 5}
3 28 {2, 7}
4 39 {8, 0}
5 71 {1, 5}
6 7 {0, 1}
7 91 {2, 6}
8 4 {8, 7}
9 93 {9, 6}
Round 162, Devices participated 10, Average loss 0.023, Central accuracy on global test data 93.086, Ensemble accuracy on global test data 92.930, Local accuracy on global train data 72.119, Local accuracy on local train data 99.350


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 85 {1, 4}
2 50 {2, 3}
3 23 {1, 5}
4 46 {9, 6}
5 82 {8, 4}
6 83 {9, 2}
7 20 {2, 7}
8 49 {2, 7}
9 6 {1, 6}
Round 163, Devices participated 10, Average loss 0.025, Central accuracy on global test data 93.232, Ensemble accuracy on global test data 92.656, Local accuracy on global train data 69.697, Local accuracy on local train data 99.367


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 42 {8, 1}
2 84 {0, 9}
3 62 {0, 7}
4 69 {2, 3}
5 25 {8, 2}
6 23 {1, 5}
7 76 {1, 3}
8 52 {8, 7}
9 75 {8, 3}
Round 164, Devices participated 10, Average loss 0.031, Central accuracy on global test data 92.822, Ensemble accuracy on global test data 92.285, Local accuracy on global train data 73.140, Local accuracy on local train data 98.917


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 95 {3, 4}
2 1 {8, 4}
3 21 {2, 5}
4 83 {9, 2}
5 23 {1, 5}
6 48 {3, 4}
7 78 {2, 3, 4}
8 42 {8, 1}
9 31 {0, 1}
Round 165, Devices participated 10, Average loss 0.027, Central accuracy on global test data 92.588, Ensemble accuracy on global test data 92.412, Local accuracy on global train data 68.423, Local accuracy on local train data 99.150


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 38 {2, 7}
1 6 {1, 6}
2 37 {4, 5, 7}
3 55 {4, 7}
4 34 {3, 7}
5 97 {4, 7}
6 98 {4}
7 74 {5}
8 56 {9, 2}
9 45 {4, 5}
Round 166, Devices participated 10, Average loss 0.021, Central accuracy on global test data 92.100, Ensemble accuracy on global test data 91.016, Local accuracy on global train data 70.950, Local accuracy on local train data 99.383


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 93 {9, 6}
2 74 {5}
3 85 {1, 4}
4 16 {0, 3}
5 98 {4}
6 50 {2, 3}
7 59 {0, 7}
8 70 {8, 5}
9 3 {8, 6}
Round 167, Devices participated 10, Average loss 0.028, Central accuracy on global test data 90.117, Ensemble accuracy on global test data 92.793, Local accuracy on global train data 70.166, Local accuracy on local train data 99.183


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 39 {8, 0}
1 20 {2, 7}
2 43 {8, 3}
3 1 {8, 4}
4 78 {2, 3, 4}
5 0 {1, 2}
6 60 {2}
7 31 {0, 1}
8 15 {5, 7}
9 6 {1, 6}
Round 168, Devices participated 10, Average loss 0.029, Central accuracy on global test data 92.246, Ensemble accuracy on global test data 91.582, Local accuracy on global train data 67.690, Local accuracy on local train data 99.100


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 76 {1, 3}
2 2 {9}
3 31 {0, 1}
4 19 {5, 6}
5 17 {3, 4}
6 83 {9, 2}
7 92 {1, 3}
8 44 {6, 7}
9 35 {8, 2}
Round 169, Devices participated 10, Average loss 0.025, Central accuracy on global test data 92.725, Ensemble accuracy on global test data 92.871, Local accuracy on global train data 74.626, Local accuracy on local train data 99.150


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {9, 2}
1 91 {2, 6}
2 92 {1, 3}
3 28 {2, 7}
4 38 {2, 7}
5 82 {8, 4}
6 32 {0, 9}
7 98 {4}
8 63 {4, 5}
9 85 {1, 4}
Round 170, Devices participated 10, Average loss 0.019, Central accuracy on global test data 92.793, Ensemble accuracy on global test data 92.295, Local accuracy on global train data 72.578, Local accuracy on local train data 99.500


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 5 {0, 6, 7}
1 84 {0, 9}
2 60 {2}
3 47 {9, 5}
4 3 {8, 6}
5 13 {8, 5, 6}
6 65 {3, 4}
7 92 {1, 3}
8 19 {5, 6}
9 74 {5}
Round 171, Devices participated 10, Average loss 0.023, Central accuracy on global test data 93.105, Ensemble accuracy on global test data 92.842, Local accuracy on global train data 71.943, Local accuracy on local train data 99.333


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 45 {4, 5}
2 39 {8, 0}
3 67 {8, 1}
4 6 {1, 6}
5 78 {2, 3, 4}
6 84 {0, 9}
7 80 {6}
8 22 {0, 5}
9 57 {4, 6}
Round 172, Devices participated 10, Average loss 0.021, Central accuracy on global test data 92.803, Ensemble accuracy on global test data 92.363, Local accuracy on global train data 75.039, Local accuracy on local train data 99.467


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 42 {8, 1}
2 43 {8, 3}
3 38 {2, 7}
4 94 {0, 1}
5 36 {1, 3}
6 68 {9, 6}
7 8 {8, 7}
8 11 {4, 5}
9 89 {0, 9}
Round 173, Devices participated 10, Average loss 0.023, Central accuracy on global test data 92.783, Ensemble accuracy on global test data 93.047, Local accuracy on global train data 72.471, Local accuracy on local train data 99.350


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 69 {2, 3}
2 89 {0, 9}
3 9 {1}
4 42 {8, 1}
5 74 {5}
6 62 {0, 7}
7 68 {9, 6}
8 57 {4, 6}
9 93 {9, 6}
Round 174, Devices participated 10, Average loss 0.021, Central accuracy on global test data 92.422, Ensemble accuracy on global test data 92.041, Local accuracy on global train data 74.612, Local accuracy on local train data 99.367


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 66 {1, 3}
2 93 {9, 6}
3 44 {6, 7}
4 48 {3, 4}
5 13 {8, 5, 6}
6 3 {8, 6}
7 21 {2, 5}
8 23 {1, 5}
9 39 {8, 0}
Round 175, Devices participated 10, Average loss 0.023, Central accuracy on global test data 92.080, Ensemble accuracy on global test data 92.539, Local accuracy on global train data 72.249, Local accuracy on local train data 99.250


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 94 {0, 1}
2 18 {0, 6}
3 68 {9, 6}
4 65 {3, 4}
5 53 {0, 7}
6 66 {1, 3}
7 75 {8, 3}
8 12 {8, 9, 3}
9 92 {1, 3}
Round 176, Devices participated 10, Average loss 0.031, Central accuracy on global test data 92.197, Ensemble accuracy on global test data 90.967, Local accuracy on global train data 73.408, Local accuracy on local train data 99.100


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 75 {8, 3}
2 39 {8, 0}
3 64 {1, 2}
4 49 {2, 7}
5 44 {6, 7}
6 66 {1, 3}
7 50 {2, 3}
8 60 {2}
9 98 {4}
Round 177, Devices participated 10, Average loss 0.032, Central accuracy on global test data 91.680, Ensemble accuracy on global test data 92.090, Local accuracy on global train data 67.598, Local accuracy on local train data 98.800


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {6}
1 27 {2, 5}
2 77 {8, 1}
3 23 {1, 5}
4 66 {1, 3}
5 33 {5, 6}
6 26 {9, 7}
7 42 {8, 1}
8 37 {4, 5, 7}
9 78 {2, 3, 4}
Round 178, Devices participated 10, Average loss 0.036, Central accuracy on global test data 92.148, Ensemble accuracy on global test data 93.174, Local accuracy on global train data 77.588, Local accuracy on local train data 98.783


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 56 {9, 2}
2 5 {0, 6, 7}
3 30 {0, 1, 2}
4 76 {1, 3}
5 95 {3, 4}
6 53 {0, 7}
7 73 {0, 6}
8 63 {4, 5}
9 88 {0}
Round 179, Devices participated 10, Average loss 0.025, Central accuracy on global test data 92.656, Ensemble accuracy on global test data 92.422, Local accuracy on global train data 79.646, Local accuracy on local train data 99.367


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 89 {0, 9}
2 68 {9, 6}
3 64 {1, 2}
4 78 {2, 3, 4}
5 62 {0, 7}
6 28 {2, 7}
7 30 {0, 1, 2}
8 49 {2, 7}
9 41 {0, 3}
Round 180, Devices participated 10, Average loss 0.021, Central accuracy on global test data 92.734, Ensemble accuracy on global test data 92.373, Local accuracy on global train data 75.969, Local accuracy on local train data 99.333


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 67 {8, 1}
1 45 {4, 5}
2 94 {0, 1}
3 35 {8, 2}
4 39 {8, 0}
5 72 {1, 9}
6 5 {0, 6, 7}
7 22 {0, 5}
8 7 {0, 1}
9 62 {0, 7}
Round 181, Devices participated 10, Average loss 0.022, Central accuracy on global test data 92.500, Ensemble accuracy on global test data 92.373, Local accuracy on global train data 78.342, Local accuracy on local train data 99.383


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 91 {2, 6}
2 9 {1}
3 58 {9, 4}
4 98 {4}
5 5 {0, 6, 7}
6 97 {4, 7}
7 92 {1, 3}
8 55 {4, 7}
9 12 {8, 9, 3}
Round 182, Devices participated 10, Average loss 0.039, Central accuracy on global test data 93.018, Ensemble accuracy on global test data 92.402, Local accuracy on global train data 80.295, Local accuracy on local train data 98.817


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 9 {1}
2 22 {0, 5}
3 95 {3, 4}
4 91 {2, 6}
5 55 {4, 7}
6 21 {2, 5}
7 15 {5, 7}
8 13 {8, 5, 6}
9 1 {8, 4}
Round 183, Devices participated 10, Average loss 0.020, Central accuracy on global test data 92.979, Ensemble accuracy on global test data 92.812, Local accuracy on global train data 77.280, Local accuracy on local train data 99.467


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 53 {0, 7}
2 10 {3, 7}
3 7 {0, 1}
4 6 {1, 6}
5 90 {8, 6, 7}
6 21 {2, 5}
7 92 {1, 3}
8 78 {2, 3, 4}
9 46 {9, 6}
Round 184, Devices participated 10, Average loss 0.025, Central accuracy on global test data 92.988, Ensemble accuracy on global test data 92.559, Local accuracy on global train data 75.042, Local accuracy on local train data 99.417


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 92 {1, 3}
2 66 {1, 3}
3 58 {9, 4}
4 24 {4, 5}
5 86 {1, 2}
6 82 {8, 4}
7 2 {9}
8 7 {0, 1}
9 64 {1, 2}
Round 185, Devices participated 10, Average loss 0.031, Central accuracy on global test data 92.383, Ensemble accuracy on global test data 92.754, Local accuracy on global train data 74.053, Local accuracy on local train data 98.933


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {3, 5}
1 50 {2, 3}
2 36 {1, 3}
3 99 {8, 9}
4 33 {5, 6}
5 79 {0, 8}
6 62 {0, 7}
7 74 {5}
8 16 {0, 3}
9 22 {0, 5}
Round 186, Devices participated 10, Average loss 0.039, Central accuracy on global test data 92.920, Ensemble accuracy on global test data 92.705, Local accuracy on global train data 71.165, Local accuracy on local train data 98.650


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 33 {5, 6}
2 10 {3, 7}
3 14 {9, 5}
4 87 {8, 9}
5 8 {8, 7}
6 51 {4, 7}
7 83 {9, 2}
8 69 {2, 3}
9 57 {4, 6}
Round 187, Devices participated 10, Average loss 0.038, Central accuracy on global test data 93.115, Ensemble accuracy on global test data 93.438, Local accuracy on global train data 69.065, Local accuracy on local train data 98.800


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 15 {5, 7}
2 38 {2, 7}
3 19 {5, 6}
4 52 {8, 7}
5 46 {9, 6}
6 71 {1, 5}
7 9 {1}
8 47 {9, 5}
9 75 {8, 3}
Round 188, Devices participated 10, Average loss 0.030, Central accuracy on global test data 93.301, Ensemble accuracy on global test data 92.822, Local accuracy on global train data 72.866, Local accuracy on local train data 99.100


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 86 {1, 2}
1 92 {1, 3}
2 14 {9, 5}
3 8 {8, 7}
4 45 {4, 5}
5 38 {2, 7}
6 60 {2}
7 63 {4, 5}
8 82 {8, 4}
9 46 {9, 6}
Round 189, Devices participated 10, Average loss 0.021, Central accuracy on global test data 93.340, Ensemble accuracy on global test data 93.008, Local accuracy on global train data 67.905, Local accuracy on local train data 99.483


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {0, 6}
1 57 {4, 6}
2 67 {8, 1}
3 14 {9, 5}
4 40 {3, 5}
5 24 {4, 5}
6 63 {4, 5}
7 35 {8, 2}
8 72 {1, 9}
9 69 {2, 3}
Round 190, Devices participated 10, Average loss 0.035, Central accuracy on global test data 93.174, Ensemble accuracy on global test data 92.324, Local accuracy on global train data 73.081, Local accuracy on local train data 99.000


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 77 {8, 1}
1 55 {4, 7}
2 61 {6}
3 34 {3, 7}
4 18 {0, 6}
5 35 {8, 2}
6 0 {1, 2}
7 97 {4, 7}
8 98 {4}
9 26 {9, 7}
Round 191, Devices participated 10, Average loss 0.035, Central accuracy on global test data 93.037, Ensemble accuracy on global test data 92.959, Local accuracy on global train data 78.564, Local accuracy on local train data 98.817


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 30 {0, 1, 2}
2 53 {0, 7}
3 77 {8, 1}
4 76 {1, 3}
5 85 {1, 4}
6 8 {8, 7}
7 35 {8, 2}
8 81 {2, 3, 7}
9 69 {2, 3}
Round 192, Devices participated 10, Average loss 0.031, Central accuracy on global test data 92.324, Ensemble accuracy on global test data 92.139, Local accuracy on global train data 76.484, Local accuracy on local train data 99.017


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 30 {0, 1, 2}
2 46 {9, 6}
3 49 {2, 7}
4 35 {8, 2}
5 90 {8, 6, 7}
6 6 {1, 6}
7 40 {3, 5}
8 20 {2, 7}
9 97 {4, 7}
Round 193, Devices participated 10, Average loss 0.032, Central accuracy on global test data 91.865, Ensemble accuracy on global test data 91.875, Local accuracy on global train data 75.737, Local accuracy on local train data 98.900


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 21 {2, 5}
2 97 {4, 7}
3 18 {0, 6}
4 44 {6, 7}
5 48 {3, 4}
6 98 {4}
7 45 {4, 5}
8 7 {0, 1}
9 95 {3, 4}
Round 194, Devices participated 10, Average loss 0.022, Central accuracy on global test data 91.025, Ensemble accuracy on global test data 89.258, Local accuracy on global train data 77.297, Local accuracy on local train data 99.333


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 67 {8, 1}
1 72 {1, 9}
2 49 {2, 7}
3 58 {9, 4}
4 2 {9}
5 66 {1, 3}
6 55 {4, 7}
7 98 {4}
8 25 {8, 2}
9 53 {0, 7}
Round 195, Devices participated 10, Average loss 0.041, Central accuracy on global test data 92.432, Ensemble accuracy on global test data 93.076, Local accuracy on global train data 73.484, Local accuracy on local train data 98.450


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 16 {0, 3}
1 2 {9}
2 36 {1, 3}
3 77 {8, 1}
4 86 {1, 2}
5 40 {3, 5}
6 87 {8, 9}
7 33 {5, 6}
8 95 {3, 4}
9 75 {8, 3}
Round 196, Devices participated 10, Average loss 0.037, Central accuracy on global test data 92.812, Ensemble accuracy on global test data 91.533, Local accuracy on global train data 72.317, Local accuracy on local train data 98.767


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 66 {1, 3}
2 81 {2, 3, 7}
3 52 {8, 7}
4 47 {9, 5}
5 99 {8, 9}
6 38 {2, 7}
7 56 {9, 2}
8 65 {3, 4}
9 4 {8, 7}
Round 197, Devices participated 10, Average loss 0.031, Central accuracy on global test data 91.973, Ensemble accuracy on global test data 92.490, Local accuracy on global train data 69.148, Local accuracy on local train data 99.150


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {0, 6}
1 93 {9, 6}
2 90 {8, 6, 7}
3 66 {1, 3}
4 22 {0, 5}
5 52 {8, 7}
6 77 {8, 1}
7 18 {0, 6}
8 71 {1, 5}
9 39 {8, 0}
Round 198, Devices participated 10, Average loss 0.025, Central accuracy on global test data 93.018, Ensemble accuracy on global test data 92.246, Local accuracy on global train data 75.566, Local accuracy on local train data 99.150


/workspace/deepedge/models/DFAN.py:367: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:388: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 81 {2, 3, 7}
2 11 {4, 5}
3 2 {9}
4 36 {1, 3}
5 79 {0, 8}
6 23 {1, 5}
7 5 {0, 6, 7}
8 64 {1, 2}
9 30 {0, 1, 2}
Round 199, Devices participated 10, Average loss 0.033, Central accuracy on global test data 92.012, Ensemble accuracy on global test data 93.320, Local accuracy on global train data 78.992, Local accuracy on local train data 99.050


Testing accuracy on test data: 92.01, Testing loss: 0.17
]0;root@3562d63b26d0: /workspace/deepedgeroot@3562d63b26d0:/workspace/deepedge# [K
[K]0;root@3562d63b26d0: /workspace/deepedgeroot@3562d63b26d0:/workspace/deepedge# python main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 500 --nn_
_refresh 0 --alpha_scale 0.5 --store_models testrun_regavg_200_300
_refresh 0 --[18Pstore_models testrun_regavg_200_300[A[Ain_dfan_multigen.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 500 --nn_refresh 0 --store_models testtemp_mgen_mlpt_async --alpha_scale 1.0 --async_s2d 1[1@0.25 --async_s2d 1
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
^CTraceback (most recent call last):
  File "main_dfan_multigen.py", line 103, in <module>
    local_user.append(LocalUpdate(args=args, net=copy.deepcopy(net_glob).to(args.device), dataset=dataset_train, idxs=dict_users[idx]))
  File "/workspace/deepedge/models/Update.py", line 34, in __init__
    for (images, labels) in self.ldr_train:
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/workspace/deepedge/models/Update.py", line 20, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py", line 97, in __getitem__
    img = self.transform(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 70, in __call__
    img = t(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 207, in __call__
    return F.resize(img, self.size, self.interpolation)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py", line 256, in resize
    return img.resize(size[::-1], interpolation)
  File "/opt/conda/lib/python3.7/site-packages/PIL/Image.py", line 1873, in resize
    return self._new(self.im.resize(size, resample, box))
KeyboardInterrupt
]0;root@3562d63b26d0: /workspace/deepedgeroot@3562d63b26d0:/workspace/deepedge# [Kpython main_dfan_multigen.py --dataset mnist --model mlp --epochs 200 --gpu 0 --n
num_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 500 --n
nn_refresh 0 --store_models testtemp_mgen_mlpt_async --alpha_scale 0.25 --async_s2d 1[A[A[K

[K
