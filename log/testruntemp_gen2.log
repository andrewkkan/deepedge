python main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_refresh 0 --store_models testtemp


def DFAN_regavg(args, teacher, student, generator, optimizer, epoch):

    for local in teacher:
        local.eval()
    generator.train()
    student.train()
    loss_G = torch.tensor(0.0)
    loss_S = torch.tensor(0.0)
    optimizer_S, optimizer_G = optimizer
    sm = torch.nn.Softmax()

    w_locals = []
    for local in teacher:
        w_locals.append(copy.deepcopy(local.state_dict()))
    w_fedavg = FedAvg(w_locals)

    # mlpa, mlpt = model_fusion_MLP(teacher, 1024, 200, 10)
    # mlpa, mlpt = mlpa.to(args.device), mlpt.to(args.device)
    # w_mlpt = mlpt.state_dict()
    # mlpa, mlpt = fusion_layer_MLP(teacher, student, 1024, 200, 10)
    # mlpa = mlpa.to(args.device)
    # mlpt = mlpt.to(args.device)
    # mlpa.eval()
    # mlpt.eval()
    # print(mlpt.alpha)

    
    # w_fedavg = mlpt.state_dict()

    for i in range(args.epoch_itrs):
        for k in range(2):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            optimizer_G.zero_grad()
            fake = generator(z)
            fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
            s_logit = student(fake)
            # t_sm = ensemble(teacher, fake, detach=False, mode=args.ensemble_mode)
            # t_logit = torch.zeros_like(teacher[0](fake))
            # for k in range(10):
            #     t_logit += teacher[k](fake) 
            t_logit = mlpa(fake)
            oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
            max_Gout = torch.max(torch.abs(fake))
            if max_Gout > 8.0:
                loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0) + torch.pow(max_Gout - 8.0, 2.0)
                print(max_Gout)
            else:
                loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0)
            loss_G.backward()                   
            optimizer_G.step()
        for j in range(5):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            optimizer_S.zero_grad()
            fake = generator(z).detach()
            fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
            s_logit = student(fake)
            # t_sm = ensemble(teacher, fake, detach=True, mode=args.ensemble_mode)
            # t_logit = torch.zeros_like(teacher[0](fake).detach())
            # for k in range(10):
            #     t_logit += teacher[k](fake).detach()
            t_logit = mlpa(fake).detach()
            oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
            loss_S1 = torch.log(1. / (2. - oneMinus_P_S))

            diff_L2 = torch.FloatTensor([0.]).to(args.device)
            for ln, student_w, fedavg_w in zip(student.state_dict().keys(), student.parameters(), w_fedavg.values()):
                diff_L2 += (fedavg_w - student_w).norm(2) * mlpt.alpha[ln] * args.alpha_scale
            if epoch == 0:
                loss_S2 = 0
            else:
                loss_S2 = diff_L2

            # diff_L2 = torch.FloatTensor([0.]).to(args.device)
            # for student_w, mlpt_w in zip(student.parameters(), w_mlpt.values()):
            #     diff_L2 += ((mlpt_w - student_w)*mlpt_w.abs()).norm(2)
            # loss_S2 = diff_L2 * alpha

            loss_S = loss_S1 + loss_S2
            loss_S.backward()
            optimizer_S.step()

        if args.verbose and i % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tG_Loss: {:.6f} S_loss: {:.6f}'.format(
                epoch, i, args.epoch_itrs, 100 * float(i) / float(args.epoch_itrs), loss_G.item(), loss_S.item()))        #vp.add_scalar('Loss_S', (epoch-1)*args.epoch_itrs+i, loss_S.item())
            #vp.add_scalar('Loss_G', (epoch-1)*args.epoch_itrs+i, loss_G.item())




/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input': array([0, 1, 2, 0, 3, 3, 4, 4, 2, 5]), 'layer_hidden1': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 1, 2, 0, 0, 0, 3, 3, 2, 0])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 49 {7}
1 29 {3}
2 56 {0}
3 42 {7}
4 35 {8}
5 17 {8}
6 45 {6}
7 88 {6, 7}
8 62 {0}
9 0 {8, 7}
Round   0, Devices participated 10, Average loss 0.257, Central accuracy on global test data 17.666, Ensemble accuracy on global test data 19.756, Local accuracy on global train data 12.100, Local accuracy on local train data 96.250


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 1, 2, 3, 4, 5, 4, 6, 7, 1])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 53 {1}
1 34 {3}
2 14 {6}
3 50 {8}
4 37 {9}
5 48 {4}
6 89 {9}
7 86 {0}
8 15 {2}
9 55 {3}
Round   1, Devices participated 10, Average loss 0.145, Central accuracy on global test data 22.227, Ensemble accuracy on global test data 30.469, Local accuracy on global train data 9.946, Local accuracy on local train data 96.300


{'layer_input': array([0, 0, 1, 2, 0, 0, 2, 3, 4, 5]), 'layer_hidden1': array([0, 0, 1, 2, 0, 0, 2, 3, 4, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])}
{'layer_input.weight': 0.3333333333333333, 'layer_input.bias': 0.3333333333333333, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {7}
1 4 {8}
2 94 {2, 3}
3 27 {9}
4 42 {7}
5 23 {7}
6 19 {9}
7 33 {0}
8 70 {5}
9 59 {3}
Round   2, Devices participated 10, Average loss 0.169, Central accuracy on global test data 25.410, Ensemble accuracy on global test data 19.902, Local accuracy on global train data 10.779, Local accuracy on local train data 95.617


{'layer_input': array([0, 1, 2, 3, 1, 0, 1, 2, 3, 4]), 'layer_hidden1': array([0, 1, 2, 1, 1, 0, 1, 2, 1, 3]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {1}
1 46 {3}
2 3 {9}
3 1 {4}
4 55 {3}
5 39 {1}
6 8 {0}
7 93 {9}
8 31 {4}
9 47 {5}
Round   3, Devices participated 10, Average loss 0.124, Central accuracy on global test data 39.570, Ensemble accuracy on global test data 26.250, Local accuracy on global train data 11.655, Local accuracy on local train data 97.317


{'layer_input': array([0, 0, 1, 2, 0, 3, 4, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 84 {1}
2 20 {2}
3 10 {7}
4 95 {3}
5 73 {1, 2}
6 17 {8}
7 67 {1}
8 94 {2, 3}
9 3 {9}
Round   4, Devices participated 10, Average loss 0.054, Central accuracy on global test data 33.857, Ensemble accuracy on global test data 36.699, Local accuracy on global train data 14.329, Local accuracy on local train data 98.667


{'layer_input': array([0, 0, 0, 0, 0, 0, 1, 0, 2, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 1, 0, 2, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])}
{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 29 {3}
1 80 {1}
2 55 {3}
3 64 {7}
4 19 {9}
5 10 {7}
6 58 {8}
7 67 {1}
8 63 {5}
9 83 {7}
Round   5, Devices participated 10, Average loss 0.074, Central accuracy on global test data 52.461, Ensemble accuracy on global test data 49.385, Local accuracy on global train data 12.932, Local accuracy on local train data 98.183


{'layer_input': array([0, 0, 1, 0, 2, 0, 0, 3, 0, 0]), 'layer_hidden1': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.6666666666666666, 'layer_input.bias': 0.6666666666666666, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {7}
1 40 {2}
2 99 {5}
3 23 {7}
4 60 {8}
5 26 {1}
6 27 {9}
7 41 {8, 9}
8 79 {6}
9 28 {6}
Round   6, Devices participated 10, Average loss 0.064, Central accuracy on global test data 54.229, Ensemble accuracy on global test data 54.580, Local accuracy on global train data 14.978, Local accuracy on local train data 98.483


{'layer_input': array([0, 0, 0, 0, 0, 1, 2, 2, 2, 2]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1])}
{'layer_input.weight': 0.4444444444444444, 'layer_input.bias': 0.4444444444444444, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {3}
1 79 {6}
2 30 {8}
3 34 {3}
4 84 {1}
5 2 {4}
6 90 {5}
7 99 {5}
8 69 {5}
9 98 {5}
Round   7, Devices participated 10, Average loss 0.105, Central accuracy on global test data 54.463, Ensemble accuracy on global test data 42.754, Local accuracy on global train data 12.544, Local accuracy on local train data 97.650


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {9}
1 81 {0}
2 96 {0}
3 35 {8}
4 56 {0}
5 24 {2}
6 41 {8, 9}
7 17 {8}
8 70 {5}
9 22 {3}
Round   8, Devices participated 10, Average loss 0.044, Central accuracy on global test data 47.773, Ensemble accuracy on global test data 39.609, Local accuracy on global train data 12.312, Local accuracy on local train data 98.583


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 51 {5, 6}
1 91 {1}
2 83 {7}
3 28 {6}
4 87 {1}
5 16 {7}
6 59 {3}
7 55 {3}
8 52 {1}
9 92 {8}
Round   9, Devices participated 10, Average loss 0.036, Central accuracy on global test data 52.734, Ensemble accuracy on global test data 52.432, Local accuracy on global train data 16.567, Local accuracy on local train data 98.767


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8}
1 86 {0}
2 66 {6}
3 56 {0}
4 91 {1}
5 50 {8}
6 77 {6}
7 96 {0}
8 48 {4}
9 81 {0}
Round  10, Devices participated 10, Average loss 0.025, Central accuracy on global test data 56.631, Ensemble accuracy on global test data 53.154, Local accuracy on global train data 20.847, Local accuracy on local train data 99.367


{'layer_input': array([0, 1, 1, 1, 1, 0, 0, 1, 1, 1]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {5}
1 43 {0}
2 38 {4, 5}
3 64 {7}
4 39 {1}
5 36 {5}
6 90 {5}
7 28 {6}
8 74 {4}
9 34 {3}
Round  11, Devices participated 10, Average loss 0.042, Central accuracy on global test data 56.270, Ensemble accuracy on global test data 54.590, Local accuracy on global train data 16.006, Local accuracy on local train data 99.050


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {5}
1 47 {5}
2 62 {0}
3 28 {6}
4 77 {6}
5 37 {9}
6 14 {6}
7 36 {5}
8 88 {6, 7}
9 71 {3, 4}
Round  12, Devices participated 10, Average loss 0.032, Central accuracy on global test data 49.414, Ensemble accuracy on global test data 43.936, Local accuracy on global train data 16.472, Local accuracy on local train data 99.433


{'layer_input': array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 48 {4}
1 24 {2}
2 40 {2}
3 33 {0}
4 91 {1}
5 70 {5}
6 30 {8}
7 77 {6}
8 69 {5}
9 63 {5}
Round  13, Devices participated 10, Average loss 0.034, Central accuracy on global test data 55.654, Ensemble accuracy on global test data 53.037, Local accuracy on global train data 14.185, Local accuracy on local train data 99.133


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {9}
1 93 {9}
2 67 {1}
3 11 {4}
4 75 {4}
5 56 {0}
6 52 {1}
7 48 {4}
8 18 {0}
9 64 {7}
Round  14, Devices participated 10, Average loss 0.022, Central accuracy on global test data 59.736, Ensemble accuracy on global test data 53.564, Local accuracy on global train data 17.842, Local accuracy on local train data 99.350


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 17 {8}
1 56 {0}
2 6 {2}
3 55 {3}
4 57 {9}
5 4 {8}
6 11 {4}
7 44 {2}
8 74 {4}
9 27 {9}
Round  15, Devices participated 10, Average loss 0.027, Central accuracy on global test data 72.910, Ensemble accuracy on global test data 67.207, Local accuracy on global train data 15.271, Local accuracy on local train data 99.083


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 29 {3}
1 25 {6}
2 28 {6}
3 22 {3}
4 72 {5}
5 5 {7}
6 33 {0}
7 93 {9}
8 70 {5}
9 84 {1}
Round  16, Devices participated 10, Average loss 0.022, Central accuracy on global test data 68.340, Ensemble accuracy on global test data 67.842, Local accuracy on global train data 17.375, Local accuracy on local train data 99.367


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {2}
1 10 {7}
2 22 {3}
3 5 {7}
4 1 {4}
5 76 {0, 1}
6 61 {1}
7 96 {0}
8 94 {2, 3}
9 38 {4, 5}
Round  17, Devices participated 10, Average loss 0.029, Central accuracy on global test data 67.881, Ensemble accuracy on global test data 66.865, Local accuracy on global train data 25.217, Local accuracy on local train data 99.167


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 50 {8}
1 66 {6}
2 76 {0, 1}
3 47 {5}
4 84 {1}
5 54 {7}
6 91 {1}
7 56 {0}
8 42 {7}
9 96 {0}
Round  18, Devices participated 10, Average loss 0.016, Central accuracy on global test data 64.131, Ensemble accuracy on global test data 63.721, Local accuracy on global train data 28.279, Local accuracy on local train data 99.650


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 54 {7}
2 42 {7}
3 2 {4}
4 60 {8}
5 56 {0}
6 49 {7}
7 4 {8}
8 88 {6, 7}
9 66 {6}
Round  19, Devices participated 10, Average loss 0.016, Central accuracy on global test data 63.037, Ensemble accuracy on global test data 59.902, Local accuracy on global train data 23.977, Local accuracy on local train data 99.550


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {6}
1 63 {5}
2 20 {2}
3 82 {6}
4 33 {0}
5 71 {3, 4}
6 86 {0}
7 75 {4}
8 38 {4, 5}
9 81 {0}
Round  20, Devices participated 10, Average loss 0.024, Central accuracy on global test data 68.623, Ensemble accuracy on global test data 65.928, Local accuracy on global train data 23.899, Local accuracy on local train data 99.400


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {8}
1 27 {9}
2 45 {6}
3 43 {0}
4 35 {8}
5 86 {0}
6 24 {2}
7 33 {0}
8 73 {1, 2}
9 88 {6, 7}
Round  21, Devices participated 10, Average loss 0.029, Central accuracy on global test data 73.359, Ensemble accuracy on global test data 68.154, Local accuracy on global train data 24.897, Local accuracy on local train data 99.083


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 14 {6}
2 98 {5}
3 54 {7}
4 97 {3}
5 8 {0}
6 16 {7}
7 31 {4}
8 40 {2}
9 83 {7}
Round  22, Devices participated 10, Average loss 0.021, Central accuracy on global test data 75.430, Ensemble accuracy on global test data 72.119, Local accuracy on global train data 22.292, Local accuracy on local train data 99.483


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {1}
1 87 {1}
2 31 {4}
3 70 {5}
4 36 {5}
5 99 {5}
6 83 {7}
7 11 {4}
8 45 {6}
9 47 {5}
Round  23, Devices participated 10, Average loss 0.021, Central accuracy on global test data 54.551, Ensemble accuracy on global test data 52.549, Local accuracy on global train data 22.141, Local accuracy on local train data 99.550


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {6}
1 18 {0}
2 27 {9}
3 93 {9}
4 87 {1}
5 70 {5}
6 14 {6}
7 88 {6, 7}
8 24 {2}
9 91 {1}
Round  24, Devices participated 10, Average loss 0.022, Central accuracy on global test data 62.549, Ensemble accuracy on global test data 62.637, Local accuracy on global train data 21.521, Local accuracy on local train data 99.217


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {0}
1 46 {3}
2 65 {9}
3 5 {7}
4 25 {6}
5 63 {5}
6 35 {8}
7 32 {2}
8 36 {5}
9 33 {0}
Round  25, Devices participated 10, Average loss 0.020, Central accuracy on global test data 76.025, Ensemble accuracy on global test data 74.443, Local accuracy on global train data 21.909, Local accuracy on local train data 99.450


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 94 {2, 3}
1 6 {2}
2 34 {3}
3 55 {3}
4 87 {1}
5 23 {7}
6 82 {6}
7 41 {8, 9}
8 63 {5}
9 93 {9}
Round  26, Devices participated 10, Average loss 0.027, Central accuracy on global test data 69.229, Ensemble accuracy on global test data 68.750, Local accuracy on global train data 22.810, Local accuracy on local train data 99.317


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 43 {0}
1 96 {0}
2 57 {9}
3 69 {5}
4 91 {1}
5 17 {8}
6 33 {0}
7 39 {1}
8 93 {9}
9 36 {5}
Round  27, Devices participated 10, Average loss 0.015, Central accuracy on global test data 65.957, Ensemble accuracy on global test data 65.361, Local accuracy on global train data 27.078, Local accuracy on local train data 99.600


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 22 {3}
1 47 {5}
2 61 {1}
3 27 {9}
4 50 {8}
5 96 {0}
6 89 {9}
7 97 {3}
8 85 {2}
9 30 {8}
Round  28, Devices participated 10, Average loss 0.017, Central accuracy on global test data 71.230, Ensemble accuracy on global test data 69.580, Local accuracy on global train data 20.566, Local accuracy on local train data 99.533


{'layer_input': array([0, 0, 0, 1, 1, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 1, 1, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {2}
1 70 {5}
2 23 {7}
3 38 {4, 5}
4 74 {4}
5 21 {2}
6 82 {6}
7 4 {8}
8 88 {6, 7}
9 98 {5}
Round  29, Devices participated 10, Average loss 0.029, Central accuracy on global test data 69.111, Ensemble accuracy on global test data 70.537, Local accuracy on global train data 24.094, Local accuracy on local train data 99.183


{'layer_input': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {5}
1 72 {5}
2 65 {9}
3 50 {8}
4 85 {2}
5 56 {0}
6 4 {8}
7 80 {1}
8 31 {4}
9 17 {8}
Round  30, Devices participated 10, Average loss 0.025, Central accuracy on global test data 60.742, Ensemble accuracy on global test data 55.195, Local accuracy on global train data 17.786, Local accuracy on local train data 99.333


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 40 {2}
2 41 {8, 9}
3 34 {3}
4 97 {3}
5 51 {5, 6}
6 50 {8}
7 13 {4}
8 22 {3}
9 8 {0}
Round  31, Devices participated 10, Average loss 0.035, Central accuracy on global test data 64.971, Ensemble accuracy on global test data 66.025, Local accuracy on global train data 25.034, Local accuracy on local train data 99.033


{'layer_input': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {1}
1 54 {7}
2 63 {5}
3 83 {7}
4 96 {0}
5 26 {1}
6 0 {8, 7}
7 59 {3}
8 69 {5}
9 14 {6}
Round  32, Devices participated 10, Average loss 0.028, Central accuracy on global test data 72.637, Ensemble accuracy on global test data 71.836, Local accuracy on global train data 29.521, Local accuracy on local train data 99.333


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {8}
1 47 {5}
2 83 {7}
3 36 {5}
4 8 {0}
5 2 {4}
6 56 {0}
7 49 {7}
8 10 {7}
9 38 {4, 5}
Round  33, Devices participated 10, Average loss 0.018, Central accuracy on global test data 68.740, Ensemble accuracy on global test data 66.660, Local accuracy on global train data 29.573, Local accuracy on local train data 99.517


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 90 {5}
2 81 {0}
3 54 {7}
4 59 {3}
5 4 {8}
6 53 {1}
7 47 {5}
8 52 {1}
9 19 {9}
Round  34, Devices participated 10, Average loss 0.021, Central accuracy on global test data 77.607, Ensemble accuracy on global test data 78.770, Local accuracy on global train data 24.653, Local accuracy on local train data 99.400


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 24 {2}
1 13 {4}
2 81 {0}
3 73 {1, 2}
4 19 {9}
5 55 {3}
6 40 {2}
7 76 {0, 1}
8 57 {9}
9 96 {0}
Round  35, Devices participated 10, Average loss 0.024, Central accuracy on global test data 78.477, Ensemble accuracy on global test data 75.371, Local accuracy on global train data 28.923, Local accuracy on local train data 99.317


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {4}
1 75 {4}
2 94 {2, 3}
3 20 {2}
4 87 {1}
5 26 {1}
6 29 {3}
7 70 {5}
8 2 {4}
9 50 {8}
Round  36, Devices participated 10, Average loss 0.020, Central accuracy on global test data 73.545, Ensemble accuracy on global test data 72.168, Local accuracy on global train data 27.581, Local accuracy on local train data 99.417


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {8, 7}
1 9 {3}
2 46 {3}
3 37 {9}
4 72 {5}
5 83 {7}
6 73 {1, 2}
7 16 {7}
8 55 {3}
9 27 {9}
Round  37, Devices participated 10, Average loss 0.032, Central accuracy on global test data 72.393, Ensemble accuracy on global test data 71.006, Local accuracy on global train data 19.846, Local accuracy on local train data 99.033


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {1}
1 29 {3}
2 9 {3}
3 50 {8}
4 26 {1}
5 81 {0}
6 48 {4}
7 82 {6}
8 37 {9}
9 15 {2}
Round  38, Devices participated 10, Average loss 0.016, Central accuracy on global test data 82.441, Ensemble accuracy on global test data 81.533, Local accuracy on global train data 25.837, Local accuracy on local train data 99.550


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 82 {6}
1 6 {2}
2 87 {1}
3 74 {4}
4 29 {3}
5 50 {8}
6 62 {0}
7 67 {1}
8 12 {1}
9 83 {7}
Round  39, Devices participated 10, Average loss 0.013, Central accuracy on global test data 82.852, Ensemble accuracy on global test data 81.895, Local accuracy on global train data 36.545, Local accuracy on local train data 99.667


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {5}
1 75 {4}
2 89 {9}
3 43 {0}
4 12 {1}
5 21 {2}
6 95 {3}
7 72 {5}
8 24 {2}
9 63 {5}
Round  40, Devices participated 10, Average loss 0.018, Central accuracy on global test data 78.955, Ensemble accuracy on global test data 77.822, Local accuracy on global train data 26.514, Local accuracy on local train data 99.567


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3}
1 15 {2}
2 31 {4}
3 94 {2, 3}
4 46 {3}
5 70 {5}
6 19 {9}
7 80 {1}
8 6 {2}
9 88 {6, 7}
Round  41, Devices participated 10, Average loss 0.020, Central accuracy on global test data 78.721, Ensemble accuracy on global test data 77.451, Local accuracy on global train data 26.179, Local accuracy on local train data 99.433


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {5}
1 92 {8}
2 72 {5}
3 77 {6}
4 23 {7}
5 53 {1}
6 1 {4}
7 12 {1}
8 97 {3}
9 93 {9}
Round  42, Devices participated 10, Average loss 0.015, Central accuracy on global test data 81.611, Ensemble accuracy on global test data 81.338, Local accuracy on global train data 28.975, Local accuracy on local train data 99.550


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 38 {4, 5}
2 36 {5}
3 14 {6}
4 57 {9}
5 77 {6}
6 66 {6}
7 73 {1, 2}
8 3 {9}
9 0 {8, 7}
Round  43, Devices participated 10, Average loss 0.020, Central accuracy on global test data 81.807, Ensemble accuracy on global test data 80.986, Local accuracy on global train data 29.861, Local accuracy on local train data 99.433


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 19 {9}
2 10 {7}
3 24 {2}
4 76 {0, 1}
5 57 {9}
6 88 {6, 7}
7 23 {7}
8 61 {1}
9 31 {4}
Round  44, Devices participated 10, Average loss 0.014, Central accuracy on global test data 83.115, Ensemble accuracy on global test data 82.744, Local accuracy on global train data 31.243, Local accuracy on local train data 99.650


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 18 {0}
1 45 {6}
2 94 {2, 3}
3 24 {2}
4 56 {0}
5 67 {1}
6 10 {7}
7 14 {6}
8 13 {4}
9 9 {3}
Round  45, Devices participated 10, Average loss 0.017, Central accuracy on global test data 81.172, Ensemble accuracy on global test data 81.299, Local accuracy on global train data 36.614, Local accuracy on local train data 99.583


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 48 {4}
2 73 {1, 2}
3 93 {9}
4 70 {5}
5 1 {4}
6 43 {0}
7 77 {6}
8 76 {0, 1}
9 88 {6, 7}
Round  46, Devices participated 10, Average loss 0.018, Central accuracy on global test data 82.676, Ensemble accuracy on global test data 80.762, Local accuracy on global train data 34.990, Local accuracy on local train data 99.533


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2}
1 45 {6}
2 83 {7}
3 68 {7}
4 75 {4}
5 65 {9}
6 96 {0}
7 1 {4}
8 77 {6}
9 73 {1, 2}
Round  47, Devices participated 10, Average loss 0.015, Central accuracy on global test data 81.914, Ensemble accuracy on global test data 81.025, Local accuracy on global train data 33.743, Local accuracy on local train data 99.550


{'layer_input': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 81 {0}
1 84 {1}
2 60 {8}
3 18 {0}
4 45 {6}
5 47 {5}
6 94 {2, 3}
7 83 {7}
8 76 {0, 1}
9 22 {3}
Round  48, Devices participated 10, Average loss 0.022, Central accuracy on global test data 81.621, Ensemble accuracy on global test data 81.211, Local accuracy on global train data 35.676, Local accuracy on local train data 99.317


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2}
1 69 {5}
2 47 {5}
3 34 {3}
4 65 {9}
5 81 {0}
6 93 {9}
7 44 {2}
8 30 {8}
9 84 {1}
Round  49, Devices participated 10, Average loss 0.017, Central accuracy on global test data 79.385, Ensemble accuracy on global test data 79.199, Local accuracy on global train data 23.855, Local accuracy on local train data 99.500


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 22 {3}
1 38 {4, 5}
2 77 {6}
3 15 {2}
4 11 {4}
5 61 {1}
6 37 {9}
7 34 {3}
8 87 {1}
9 3 {9}
Round  50, Devices participated 10, Average loss 0.016, Central accuracy on global test data 81.074, Ensemble accuracy on global test data 80.908, Local accuracy on global train data 29.978, Local accuracy on local train data 99.500


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {0}
1 9 {3}
2 26 {1}
3 58 {8}
4 69 {5}
5 44 {2}
6 13 {4}
7 93 {9}
8 61 {1}
9 43 {0}
Round  51, Devices participated 10, Average loss 0.011, Central accuracy on global test data 84.258, Ensemble accuracy on global test data 84.414, Local accuracy on global train data 35.315, Local accuracy on local train data 99.683


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 34 {3}
2 27 {9}
3 16 {7}
4 72 {5}
5 98 {5}
6 14 {6}
7 19 {9}
8 9 {3}
9 46 {3}
Round  52, Devices participated 10, Average loss 0.013, Central accuracy on global test data 79.736, Ensemble accuracy on global test data 77.803, Local accuracy on global train data 22.031, Local accuracy on local train data 99.633


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 29 {3}
1 72 {5}
2 81 {0}
3 80 {1}
4 68 {7}
5 85 {2}
6 14 {6}
7 0 {8, 7}
8 93 {9}
9 52 {1}
Round  53, Devices participated 10, Average loss 0.016, Central accuracy on global test data 85.527, Ensemble accuracy on global test data 85.996, Local accuracy on global train data 31.531, Local accuracy on local train data 99.567


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 17 {8}
1 69 {5}
2 35 {8}
3 86 {0}
4 83 {7}
5 84 {1}
6 23 {7}
7 56 {0}
8 89 {9}
9 60 {8}
Round  54, Devices participated 10, Average loss 0.013, Central accuracy on global test data 74.590, Ensemble accuracy on global test data 73.857, Local accuracy on global train data 30.066, Local accuracy on local train data 99.683


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 94 {2, 3}
1 79 {6}
2 36 {5}
3 8 {0}
4 19 {9}
5 16 {7}
6 13 {4}
7 30 {8}
8 25 {6}
9 3 {9}
Round  55, Devices participated 10, Average loss 0.019, Central accuracy on global test data 78.271, Ensemble accuracy on global test data 79.902, Local accuracy on global train data 25.693, Local accuracy on local train data 99.367


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {6}
1 82 {6}
2 32 {2}
3 52 {1}
4 17 {8}
5 96 {0}
6 6 {2}
7 57 {9}
8 67 {1}
9 74 {4}
Round  56, Devices participated 10, Average loss 0.011, Central accuracy on global test data 83.486, Ensemble accuracy on global test data 84.912, Local accuracy on global train data 35.637, Local accuracy on local train data 99.733


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 20 {2}
2 59 {3}
3 99 {5}
4 39 {1}
5 42 {7}
6 73 {1, 2}
7 53 {1}
8 45 {6}
9 8 {0}
Round  57, Devices participated 10, Average loss 0.014, Central accuracy on global test data 78.936, Ensemble accuracy on global test data 82.158, Local accuracy on global train data 40.339, Local accuracy on local train data 99.650


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {8}
1 38 {4, 5}
2 5 {7}
3 54 {7}
4 66 {6}
5 33 {0}
6 80 {1}
7 83 {7}
8 9 {3}
9 4 {8}
Round  58, Devices participated 10, Average loss 0.017, Central accuracy on global test data 76.914, Ensemble accuracy on global test data 78.115, Local accuracy on global train data 33.870, Local accuracy on local train data 99.533


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 39 {1}
1 44 {2}
2 62 {0}
3 4 {8}
4 42 {7}
5 25 {6}
6 0 {8, 7}
7 45 {6}
8 29 {3}
9 93 {9}
Round  59, Devices participated 10, Average loss 0.014, Central accuracy on global test data 83.633, Ensemble accuracy on global test data 82.539, Local accuracy on global train data 27.820, Local accuracy on local train data 99.467


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {0}
1 54 {7}
2 19 {9}
3 97 {3}
4 92 {8}
5 31 {4}
6 81 {0}
7 2 {4}
8 24 {2}
9 9 {3}
Round  60, Devices participated 10, Average loss 0.011, Central accuracy on global test data 78.486, Ensemble accuracy on global test data 79.072, Local accuracy on global train data 28.884, Local accuracy on local train data 99.750


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {3}
1 19 {9}
2 7 {8}
3 49 {7}
4 45 {6}
5 35 {8}
6 80 {1}
7 85 {2}
8 40 {2}
9 89 {9}
Round  61, Devices participated 10, Average loss 0.012, Central accuracy on global test data 72.607, Ensemble accuracy on global test data 74.404, Local accuracy on global train data 23.320, Local accuracy on local train data 99.600


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {1, 2}
1 66 {6}
2 18 {0}
3 74 {4}
4 33 {0}
5 2 {4}
6 87 {1}
7 29 {3}
8 98 {5}
9 91 {1}
Round  62, Devices participated 10, Average loss 0.019, Central accuracy on global test data 75.361, Ensemble accuracy on global test data 75.215, Local accuracy on global train data 35.916, Local accuracy on local train data 99.317


{'layer_input': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {2}
1 31 {4}
2 27 {9}
3 50 {8}
4 71 {3, 4}
5 81 {0}
6 99 {5}
7 97 {3}
8 73 {1, 2}
9 49 {7}
Round  63, Devices participated 10, Average loss 0.026, Central accuracy on global test data 83.105, Ensemble accuracy on global test data 84.531, Local accuracy on global train data 25.942, Local accuracy on local train data 99.217


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 64 {7}
2 42 {7}
3 88 {6, 7}
4 27 {9}
5 95 {3}
6 92 {8}
7 50 {8}
8 45 {6}
9 85 {2}
Round  64, Devices participated 10, Average loss 0.010, Central accuracy on global test data 82.578, Ensemble accuracy on global test data 82.979, Local accuracy on global train data 36.848, Local accuracy on local train data 99.733


{'layer_input': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {9}
1 8 {0}
2 49 {7}
3 32 {2}
4 15 {2}
5 35 {8}
6 70 {5}
7 59 {3}
8 31 {4}
9 30 {8}
Round  65, Devices participated 10, Average loss 0.014, Central accuracy on global test data 84.932, Ensemble accuracy on global test data 84.668, Local accuracy on global train data 24.048, Local accuracy on local train data 99.483


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 69 {5}
1 35 {8}
2 52 {1}
3 51 {5, 6}
4 26 {1}
5 15 {2}
6 76 {0, 1}
7 48 {4}
8 98 {5}
9 12 {1}
Round  66, Devices participated 10, Average loss 0.018, Central accuracy on global test data 77.812, Ensemble accuracy on global test data 79.395, Local accuracy on global train data 36.426, Local accuracy on local train data 99.483


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {3}
1 57 {9}
2 0 {8, 7}
3 86 {0}
4 26 {1}
5 80 {1}
6 73 {1, 2}
7 20 {2}
8 63 {5}
9 98 {5}
Round  67, Devices participated 10, Average loss 0.020, Central accuracy on global test data 82.695, Ensemble accuracy on global test data 83.096, Local accuracy on global train data 31.003, Local accuracy on local train data 99.350


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 81 {0}
1 6 {2}
2 64 {7}
3 84 {1}
4 20 {2}
5 2 {4}
6 8 {0}
7 4 {8}
8 66 {6}
9 92 {8}
Round  68, Devices participated 10, Average loss 0.010, Central accuracy on global test data 83.701, Ensemble accuracy on global test data 83.877, Local accuracy on global train data 37.173, Local accuracy on local train data 99.750


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 34 {3}
2 64 {7}
3 11 {4}
4 96 {0}
5 18 {0}
6 82 {6}
7 52 {1}
8 72 {5}
9 92 {8}
Round  69, Devices participated 10, Average loss 0.010, Central accuracy on global test data 83.906, Ensemble accuracy on global test data 85.098, Local accuracy on global train data 40.393, Local accuracy on local train data 99.767


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 93 {9}
1 68 {7}
2 94 {2, 3}
3 13 {4}
4 36 {5}
5 11 {4}
6 5 {7}
7 1 {4}
8 40 {2}
9 7 {8}
Round  70, Devices participated 10, Average loss 0.016, Central accuracy on global test data 63.242, Ensemble accuracy on global test data 84.922, Local accuracy on global train data 30.850, Local accuracy on local train data 99.467


{'layer_input': array([0, 0, 1, 0, 0, 0, 2, 3, 4, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {1, 2}
1 64 {7}
2 37 {9}
3 69 {5}
4 31 {4}
5 48 {4}
6 77 {6}
7 41 {8, 9}
8 53 {1}
9 75 {4}
Round  71, Devices participated 10, Average loss 0.039, Central accuracy on global test data 68.799, Ensemble accuracy on global test data 80.684, Local accuracy on global train data 29.436, Local accuracy on local train data 98.817


{'layer_input': array([0, 1, 2, 2, 2, 2, 2, 2, 1, 2]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.4180, device='cuda:0', grad_fn=<MaxBackward1>)
0 46 {3}
1 43 {0}
2 84 {1}
3 11 {4}
4 66 {6}
5 92 {8}
6 16 {7}
7 19 {9}
8 96 {0}
9 37 {9}
Round  72, Devices participated 10, Average loss 0.021, Central accuracy on global test data 74.014, Ensemble accuracy on global test data 74.619, Local accuracy on global train data 36.221, Local accuracy on local train data 99.333


{'layer_input': array([0, 0, 1, 2, 0, 0, 0, 0, 0, 3]), 'layer_hidden1': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.6666666666666666, 'layer_input.bias': 0.6666666666666666, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 43 {0}
1 96 {0}
2 9 {3}
3 0 {8, 7}
4 74 {4}
5 19 {9}
6 1 {4}
7 23 {7}
8 38 {4, 5}
9 66 {6}
Round  73, Devices participated 10, Average loss 0.051, Central accuracy on global test data 76.055, Ensemble accuracy on global test data 75.361, Local accuracy on global train data 50.630, Local accuracy on local train data 99.183


{'layer_input': array([0, 1, 2, 1, 3, 4, 1, 5, 1, 6]), 'layer_hidden1': array([0, 1, 1, 1, 2, 1, 1, 1, 1, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {9}
1 62 {0}
2 32 {2}
3 10 {7}
4 22 {3}
5 72 {5}
6 64 {7}
7 25 {6}
8 86 {0}
9 60 {8}
Round  74, Devices participated 10, Average loss 0.040, Central accuracy on global test data 76.602, Ensemble accuracy on global test data 82.119, Local accuracy on global train data 42.444, Local accuracy on local train data 99.200


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 1, 0, 0, 2, 3, 4, 5, 2, 5]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {1}
1 7 {8}
2 64 {7}
3 76 {0, 1}
4 15 {2}
5 46 {3}
6 27 {9}
7 47 {5}
8 40 {2}
9 69 {5}
Round  75, Devices participated 10, Average loss 0.097, Central accuracy on global test data 82.881, Ensemble accuracy on global test data 83.145, Local accuracy on global train data 35.757, Local accuracy on local train data 98.933


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 3]), 'layer_hidden1': array([0, 1, 2, 0, 0, 3, 4, 3, 5, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 75 {4}
1 97 {3}
2 79 {6}
3 87 {1}
4 11 {4}
5 7 {8}
6 37 {9}
7 92 {8}
8 41 {8, 9}
9 12 {1}
Round  76, Devices participated 10, Average loss 0.078, Central accuracy on global test data 79.521, Ensemble accuracy on global test data 80.098, Local accuracy on global train data 47.800, Local accuracy on local train data 99.000


{'layer_input': array([0, 1, 1, 2, 3, 4, 0, 0, 5, 6]), 'layer_hidden1': array([0, 1, 1, 2, 1, 1, 0, 1, 3, 4]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 4 {8}
2 17 {8}
3 21 {2}
4 76 {0, 1}
5 74 {4}
6 19 {9}
7 61 {1}
8 45 {6}
9 86 {0}
Round  77, Devices participated 10, Average loss 0.050, Central accuracy on global test data 79.131, Ensemble accuracy on global test data 78.174, Local accuracy on global train data 45.315, Local accuracy on local train data 99.317


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 1, 2, 3, 4, 0, 5, 0, 1, 3]), 'layer_hidden2': array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {7}
1 41 {8, 9}
2 75 {4}
3 36 {5}
4 26 {1}
5 49 {7}
6 95 {3}
7 16 {7}
8 93 {9}
9 72 {5}
Round  78, Devices participated 10, Average loss 0.092, Central accuracy on global test data 76.270, Ensemble accuracy on global test data 76.768, Local accuracy on global train data 42.986, Local accuracy on local train data 98.883


{'layer_input': array([0, 1, 2, 3, 1, 4, 5, 6, 1, 7]), 'layer_hidden1': array([0, 1, 2, 3, 1, 4, 5, 0, 1, 0]), 'layer_hidden2': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {3}
1 53 {1}
2 65 {9}
3 8 {0}
4 68 {7}
5 35 {8}
6 13 {4}
7 55 {3}
8 23 {7}
9 97 {3}
Round  79, Devices participated 10, Average loss 0.096, Central accuracy on global test data 77.773, Ensemble accuracy on global test data 79.355, Local accuracy on global train data 40.093, Local accuracy on local train data 99.233


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {4}
1 98 {5}
2 22 {3}
3 45 {6}
4 97 {3}
5 87 {1}
6 92 {8}
7 71 {3, 4}
8 47 {5}
9 79 {6}
Round  80, Devices participated 10, Average loss 0.077, Central accuracy on global test data 77.910, Ensemble accuracy on global test data 79.092, Local accuracy on global train data 45.686, Local accuracy on local train data 99.250


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 5, 8]), 'layer_hidden1': array([0, 1, 2, 3, 1, 1, 4, 0, 1, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8}
1 70 {5}
2 59 {3}
3 49 {7}
4 36 {5}
5 91 {1}
6 82 {6}
7 50 {8}
8 26 {1}
9 72 {5}
Round  81, Devices participated 10, Average loss 0.057, Central accuracy on global test data 71.670, Ensemble accuracy on global test data 73.779, Local accuracy on global train data 42.485, Local accuracy on local train data 99.217


{'layer_input': array([0, 1, 2, 3, 4, 2, 5, 6, 7, 2]), 'layer_hidden1': array([0, 1, 1, 0, 2, 1, 3, 1, 4, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 22 {3}
1 83 {7}
2 75 {4}
3 29 {3}
4 66 {6}
5 48 {4}
6 12 {1}
7 10 {7}
8 86 {0}
9 1 {4}
Round  82, Devices participated 10, Average loss 0.055, Central accuracy on global test data 71.699, Ensemble accuracy on global test data 72.148, Local accuracy on global train data 43.950, Local accuracy on local train data 99.217


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 0, 1, 2, 3, 0, 4, 0, 0, 5]), 'layer_hidden2': array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 76 {0, 1}
1 33 {0}
2 40 {2}
3 59 {3}
4 65 {9}
5 63 {5}
6 73 {1, 2}
7 36 {5}
8 99 {5}
9 82 {6}
Round  83, Devices participated 10, Average loss 0.127, Central accuracy on global test data 79.854, Ensemble accuracy on global test data 80.283, Local accuracy on global train data 36.311, Local accuracy on local train data 98.650


{'layer_input': array([0, 1, 2, 0, 3, 4, 5, 6, 7, 2]), 'layer_hidden1': array([0, 1, 2, 0, 2, 2, 2, 2, 3, 2]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {2}
1 88 {6, 7}
2 54 {7}
3 40 {2}
4 70 {5}
5 67 {1}
6 86 {0}
7 38 {4, 5}
8 58 {8}
9 42 {7}
Round  84, Devices participated 10, Average loss 0.069, Central accuracy on global test data 79.414, Ensemble accuracy on global test data 79.365, Local accuracy on global train data 46.438, Local accuracy on local train data 99.033


{'layer_input': array([0, 1, 0, 2, 3, 4, 0, 0, 5, 6]), 'layer_hidden1': array([0, 1, 1, 1, 2, 1, 3, 0, 4, 5]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.3333333333333333, 'layer_input.bias': 0.3333333333333333, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 56 {0}
2 12 {1}
3 90 {5}
4 94 {2, 3}
5 86 {0}
6 19 {9}
7 78 {9}
8 92 {8}
9 59 {3}
Round  85, Devices participated 10, Average loss 0.093, Central accuracy on global test data 67.881, Ensemble accuracy on global test data 67.617, Local accuracy on global train data 37.065, Local accuracy on local train data 98.817


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 8]), 'layer_hidden1': array([0, 1, 2, 3, 0, 3, 4, 3, 3, 3]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 7 {8}
1 94 {2, 3}
2 69 {5}
3 44 {2}
4 50 {8}
5 26 {1}
6 75 {4}
7 24 {2}
8 33 {0}
9 18 {0}
Round  86, Devices participated 10, Average loss 0.081, Central accuracy on global test data 76.914, Ensemble accuracy on global test data 78.066, Local accuracy on global train data 39.514, Local accuracy on local train data 98.950


{'layer_input': array([0, 1, 0, 0, 2, 3, 0, 4, 5, 6]), 'layer_hidden1': array([0, 1, 1, 0, 2, 3, 0, 1, 4, 5]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.3333333333333333, 'layer_input.bias': 0.3333333333333333, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 75 {4}
2 84 {1}
3 93 {9}
4 35 {8}
5 71 {3, 4}
6 19 {9}
7 63 {5}
8 59 {3}
9 94 {2, 3}
Round  87, Devices participated 10, Average loss 0.089, Central accuracy on global test data 71.211, Ensemble accuracy on global test data 70.889, Local accuracy on global train data 35.276, Local accuracy on local train data 98.900


{'layer_input': array([0, 1, 2, 3, 4, 5, 3, 6, 2, 3]), 'layer_hidden1': array([0, 1, 2, 3, 3, 4, 3, 5, 2, 3]), 'layer_hidden2': array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {7}
1 66 {6}
2 17 {8}
3 43 {0}
4 53 {1}
5 40 {2}
6 76 {0, 1}
7 1 {4}
8 4 {8}
9 81 {0}
Round  88, Devices participated 10, Average loss 0.055, Central accuracy on global test data 74.111, Ensemble accuracy on global test data 75.830, Local accuracy on global train data 41.099, Local accuracy on local train data 99.117


{'layer_input': array([0, 1, 2, 3, 4, 0, 5, 6, 7, 0]), 'layer_hidden1': array([0, 1, 2, 3, 4, 0, 2, 1, 5, 0]), 'layer_hidden2': array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {1}
1 51 {5, 6}
2 97 {3}
3 7 {8}
4 41 {8, 9}
5 61 {1}
6 29 {3}
7 88 {6, 7}
8 49 {7}
9 26 {1}
Round  89, Devices participated 10, Average loss 0.123, Central accuracy on global test data 81.895, Ensemble accuracy on global test data 80.947, Local accuracy on global train data 48.372, Local accuracy on local train data 98.667


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 1, 0, 2, 0, 3, 1, 1, 3, 3]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2}
1 47 {5}
2 20 {2}
3 59 {3}
4 85 {2}
5 2 {4}
6 99 {5}
7 70 {5}
8 54 {7}
9 50 {8}
Round  90, Devices participated 10, Average loss 0.105, Central accuracy on global test data 76.748, Ensemble accuracy on global test data 78.369, Local accuracy on global train data 40.945, Local accuracy on local train data 98.867


{'layer_input': array([0, 1, 2, 3, 4, 5, 0, 6, 7, 8]), 'layer_hidden1': array([0, 1, 2, 1, 3, 3, 0, 0, 4, 4]), 'layer_hidden2': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 65 {9}
1 98 {5}
2 79 {6}
3 6 {2}
4 58 {8}
5 92 {8}
6 19 {9}
7 37 {9}
8 33 {0}
9 75 {4}
Round  91, Devices participated 10, Average loss 0.083, Central accuracy on global test data 74.912, Ensemble accuracy on global test data 72.764, Local accuracy on global train data 39.302, Local accuracy on local train data 99.033


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 1, 2, 3, 4, 0, 5, 6, 7, 8]), 'layer_hidden2': array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {2}
1 84 {1}
2 56 {0}
3 0 {8, 7}
4 69 {5}
5 15 {2}
6 66 {6}
7 78 {9}
8 42 {7}
9 95 {3}
Round  92, Devices participated 10, Average loss 0.109, Central accuracy on global test data 80.391, Ensemble accuracy on global test data 82.061, Local accuracy on global train data 43.274, Local accuracy on local train data 99.050


{'layer_input': array([0, 1, 2, 2, 2, 2, 3, 4, 2, 5]), 'layer_hidden1': array([0, 1, 2, 2, 2, 2, 3, 4, 2, 5]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {5}
1 20 {2}
2 53 {1}
3 26 {1}
4 80 {1}
5 67 {1}
6 25 {6}
7 37 {9}
8 39 {1}
9 10 {7}
Round  93, Devices participated 10, Average loss 0.077, Central accuracy on global test data 73.027, Ensemble accuracy on global test data 73.799, Local accuracy on global train data 53.169, Local accuracy on local train data 99.267


{'layer_input': array([0, 1, 2, 3, 4, 5, 0, 6, 7, 8]), 'layer_hidden1': array([0, 1, 0, 2, 3, 4, 0, 2, 4, 5]), 'layer_hidden2': array([0, 1, 2, 0, 0, 3, 0, 0, 3, 4])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {7}
1 62 {0}
2 65 {9}
3 69 {5}
4 82 {6}
5 46 {3}
6 64 {7}
7 70 {5}
8 59 {3}
9 11 {4}
Round  94, Devices participated 10, Average loss 0.140, Central accuracy on global test data 81.846, Ensemble accuracy on global test data 81.592, Local accuracy on global train data 41.169, Local accuracy on local train data 98.950


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 3]), 'layer_hidden1': array([0, 0, 0, 0, 1, 2, 0, 0, 2, 0]), 'layer_hidden2': array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 89 {9}
2 64 {7}
3 25 {6}
4 85 {2}
5 9 {3}
6 57 {9}
7 41 {8, 9}
8 97 {3}
9 45 {6}
Round  95, Devices participated 10, Average loss 0.102, Central accuracy on global test data 71.729, Ensemble accuracy on global test data 72.793, Local accuracy on global train data 46.343, Local accuracy on local train data 98.900


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 1, 2, 3, 3, 3, 4, 0, 2, 1]), 'layer_hidden2': array([0, 0, 1, 0, 0, 0, 0, 0, 1, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {2}
1 8 {0}
2 70 {5}
3 41 {8, 9}
4 12 {1}
5 19 {9}
6 88 {6, 7}
7 20 {2}
8 98 {5}
9 33 {0}
Round  96, Devices participated 10, Average loss 0.091, Central accuracy on global test data 75.938, Ensemble accuracy on global test data 76.299, Local accuracy on global train data 46.875, Local accuracy on local train data 98.917


{'layer_input': array([0, 1, 2, 2, 3, 4, 5, 6, 7, 8]), 'layer_hidden1': array([0, 1, 2, 2, 3, 4, 5, 6, 7, 3]), 'layer_hidden2': array([0, 1, 0, 0, 0, 0, 2, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 50 {8}
1 29 {3}
2 81 {0}
3 33 {0}
4 44 {2}
5 69 {5}
6 2 {4}
7 68 {7}
8 93 {9}
9 85 {2}
Round  97, Devices participated 10, Average loss 0.077, Central accuracy on global test data 80.273, Ensemble accuracy on global test data 80.459, Local accuracy on global train data 37.478, Local accuracy on local train data 99.133


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 5 {7}
1 84 {1}
2 70 {5}
3 73 {1, 2}
4 31 {4}
5 62 {0}
6 69 {5}
7 13 {4}
8 86 {0}
9 74 {4}
Round  98, Devices participated 10, Average loss 0.083, Central accuracy on global test data 75.488, Ensemble accuracy on global test data 75.781, Local accuracy on global train data 52.322, Local accuracy on local train data 99.083


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 0, 7]), 'layer_hidden1': array([0, 1, 2, 0, 0, 0, 1, 0, 0, 0]), 'layer_hidden2': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 69 {5}
1 9 {3}
2 41 {8, 9}
3 83 {7}
4 76 {0, 1}
5 63 {5}
6 34 {3}
7 67 {1}
8 36 {5}
9 87 {1}
Round  99, Devices participated 10, Average loss 0.089, Central accuracy on global test data 79.121, Ensemble accuracy on global test data 78.467, Local accuracy on global train data 43.970, Local accuracy on local train data 98.950


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 1, 2, 3, 1, 3, 2, 3, 4, 1]), 'layer_hidden2': array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {2}
1 77 {6}
2 95 {3}
3 52 {1}
4 25 {6}
5 38 {4, 5}
6 46 {3}
7 47 {5}
8 93 {9}
9 88 {6, 7}
Round 100, Devices participated 10, Average loss 0.093, Central accuracy on global test data 80.498, Ensemble accuracy on global test data 80.459, Local accuracy on global train data 44.883, Local accuracy on local train data 99.150


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {6}
1 22 {3}
2 87 {1}
3 6 {2}
4 15 {2}
5 36 {5}
6 37 {9}
7 44 {2}
8 48 {4}
9 38 {4, 5}
Round 101, Devices participated 10, Average loss 0.072, Central accuracy on global test data 79.092, Ensemble accuracy on global test data 79.570, Local accuracy on global train data 45.481, Local accuracy on local train data 99.233


{'layer_input': array([0, 0, 1, 2, 3, 0, 2, 0, 4, 3]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.3333333333333333, 'layer_input.bias': 0.3333333333333333, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {6}
1 66 {6}
2 74 {4}
3 62 {0}
4 49 {7}
5 45 {6}
6 18 {0}
7 79 {6}
8 15 {2}
9 68 {7}
Round 102, Devices participated 10, Average loss 0.062, Central accuracy on global test data 81.416, Ensemble accuracy on global test data 81.074, Local accuracy on global train data 56.111, Local accuracy on local train data 99.300


{'layer_input': array([0, 1, 2, 3, 4, 5, 2, 6, 7, 8]), 'layer_hidden1': array([0, 1, 1, 1, 2, 1, 1, 3, 4, 1]), 'layer_hidden2': array([0, 1, 1, 1, 2, 1, 1, 1, 1, 1])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {8}
1 32 {2}
2 77 {6}
3 87 {1}
4 27 {9}
5 13 {4}
6 14 {6}
7 47 {5}
8 22 {3}
9 73 {1, 2}
Round 103, Devices participated 10, Average loss 0.087, Central accuracy on global test data 82.607, Ensemble accuracy on global test data 83.252, Local accuracy on global train data 45.754, Local accuracy on local train data 99.083


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 4, 5]), 'layer_hidden1': array([0, 1, 2, 2, 2, 2, 3, 2, 2, 2]), 'layer_hidden2': array([0, 1, 2, 2, 2, 2, 2, 2, 2, 2])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {8}
1 31 {4}
2 44 {2}
3 3 {9}
4 84 {1}
5 25 {6}
6 98 {5}
7 73 {1, 2}
8 52 {1}
9 82 {6}
Round 104, Devices participated 10, Average loss 0.083, Central accuracy on global test data 82.637, Ensemble accuracy on global test data 82.773, Local accuracy on global train data 50.972, Local accuracy on local train data 99.100


{'layer_input': array([0, 1, 1, 2, 3, 4, 5, 6, 7, 8]), 'layer_hidden1': array([0, 0, 0, 0, 1, 0, 0, 2, 3, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.4528, device='cuda:0', grad_fn=<MaxBackward1>)
0 66 {6}
1 38 {4, 5}
2 74 {4}
3 49 {7}
4 46 {3}
5 45 {6}
6 12 {1}
7 0 {8, 7}
8 81 {0}
9 23 {7}
Round 105, Devices participated 10, Average loss 0.117, Central accuracy on global test data 82.109, Ensemble accuracy on global test data 83.242, Local accuracy on global train data 55.581, Local accuracy on local train data 99.000


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 1, 1, 1, 2, 3, 1, 1, 1, 2]), 'layer_hidden2': array([0, 1, 1, 1, 2, 1, 1, 1, 1, 2])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^[^[^[^[^[^[^[^[^[^[^[^[^[^[^[^[^[^[                                    ^B0   0 6 {2}
1 14 {6}
2 56 {0}
3 8 {0}
4 37 {9}
5 71 {3, 4}
6 68 {7}
7 53 {1}
8 88 {6, 7}
9 65 {9}
Round 106, Devices participated 10, Average loss 0.102, Central accuracy on global test data 79.873, Ensemble accuracy on global test data 80.645, Local accuracy on global train data 49.753, Local accuracy on local train data 99.017


{'layer_input': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'layer_hidden1': array([0, 1, 0, 0, 0, 2, 0, 0, 3, 4]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])}
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^[[A^[[B^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 366, in DFAN_regavg
    t_logit += teacher[k](fake).detach()
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/deepedge/models/Nets.py", line 31, in forward
    x = self.layer_input(x)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1370, in linear
    ret = torch.addmm(bias, input, weight.t())
KeyboardInterrupt
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input': array([0, 1, 2, 0, 3, 3, 4, 4, 2, 5]), 'layer_hidden1': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 1, 2, 0, 0, 0, 3, 3, 2, 0])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 49 {7}
1 29 {3}
2 56 {0}
3 42 {7}
4 35 {8}
5 17 {8}
6 45 {6}
7 88 {6, 7}
8 62 {0}
9 0 {8, 7}
Round   0, Devices participated 10, Average loss 0.257, Central accuracy on global test data 17.617, Ensemble accuracy on global test data 19.756, Local accuracy on global train data 12.100, Local accuracy on local train data 96.250


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 1, 2, 3, 4, 5, 4, 6, 7, 1])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 53 {1}
1 34 {3}
2 14 {6}
3 50 {8}
4 37 {9}
5 48 {4}
6 89 {9}
7 86 {0}
8 15 {2}
9 55 {3}
Round   1, Devices participated 10, Average loss 0.135, Central accuracy on global test data 33.516, Ensemble accuracy on global test data 32.197, Local accuracy on global train data 9.946, Local accuracy on local train data 96.517


{'layer_input': array([0, 0, 1, 0, 0, 0, 0, 2, 3, 4]), 'layer_hidden1': array([0, 0, 1, 2, 0, 0, 2, 3, 4, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])}
{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {7}
1 4 {8}
2 94 {2, 3}
3 27 {9}
4 42 {7}
5 23 {7}
6 19 {9}
7 33 {0}
8 70 {5}
9 59 {3}
Round   2, Devices participated 10, Average loss 0.152, Central accuracy on global test data 28.906, Ensemble accuracy on global test data 27.803, Local accuracy on global train data 12.021, Local accuracy on local train data 96.500


{'layer_input': array([0, 1, 2, 3, 1, 0, 1, 2, 3, 4]), 'layer_hidden1': array([0, 1, 2, 3, 1, 0, 1, 2, 3, 4]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {1}
1 46 {3}
2 3 {9}
3 1 {4}
4 55 {3}
5 39 {1}
6 8 {0}
7 93 {9}
8 31 {4}
9 47 {5}
Round   3, Devices participated 10, Average loss 0.115, Central accuracy on global test data 31.328, Ensemble accuracy on global test data 23.525, Local accuracy on global train data 11.606, Local accuracy on local train data 97.600


{'layer_input': array([0, 0, 1, 2, 3, 4, 5, 0, 6, 0]), 'layer_hidden1': array([0, 0, 0, 1, 2, 0, 3, 0, 2, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.3333333333333333, 'layer_input.bias': 0.3333333333333333, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 84 {1}
2 20 {2}
3 10 {7}
4 95 {3}
5 73 {1, 2}
6 17 {8}
7 67 {1}
8 94 {2, 3}
9 3 {9}
Round   4, Devices participated 10, Average loss 0.064, Central accuracy on global test data 42.773, Ensemble accuracy on global test data 38.691, Local accuracy on global train data 14.312, Local accuracy on local train data 98.250


{'layer_input': array([0, 0, 0, 0, 1, 0, 2, 0, 3, 0]), 'layer_hidden1': array([0, 0, 0, 0, 1, 0, 2, 0, 3, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])}
{'layer_input.weight': 0.6666666666666666, 'layer_input.bias': 0.6666666666666666, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 29 {3}
1 80 {1}
2 55 {3}
3 64 {7}
4 19 {9}
5 10 {7}
6 58 {8}
7 67 {1}
8 63 {5}
9 83 {7}
Round   5, Devices participated 10, Average loss 0.073, Central accuracy on global test data 50.518, Ensemble accuracy on global test data 46.367, Local accuracy on global train data 13.694, Local accuracy on local train data 98.467


{'layer_input': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {7}
1 40 {2}
2 99 {5}
3 23 {7}
4 60 {8}
5 26 {1}
6 27 {9}
7 41 {8, 9}
8 79 {6}
9 28 {6}
Round   6, Devices participated 10, Average loss 0.064, Central accuracy on global test data 55.283, Ensemble accuracy on global test data 52.637, Local accuracy on global train data 15.916, Local accuracy on local train data 98.300


{'layer_input': array([0, 0, 0, 0, 0, 1, 2, 2, 2, 2]), 'layer_hidden1': array([0, 0, 0, 0, 0, 1, 2, 2, 2, 2]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1])}
{'layer_input.weight': 0.4444444444444444, 'layer_input.bias': 0.4444444444444444, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {3}
1 79 {6}
2 30 {8}
3 34 {3}
4 84 {1}
5 2 {4}
6 90 {5}
7 99 {5}
8 69 {5}
9 98 {5}
Round   7, Devices participated 10, Average loss 0.103, Central accuracy on global test data 51.650, Ensemble accuracy on global test data 41.592, Local accuracy on global train data 12.456, Local accuracy on local train data 97.683


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {9}
1 81 {0}
2 96 {0}
3 35 {8}
4 56 {0}
5 24 {2}
6 41 {8, 9}
7 17 {8}
8 70 {5}
9 22 {3}
Round   8, Devices participated 10, Average loss 0.040, Central accuracy on global test data 42.812, Ensemble accuracy on global test data 37.949, Local accuracy on global train data 11.375, Local accuracy on local train data 98.683


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 51 {5, 6}
1 91 {1}
2 83 {7}
3 28 {6}
4 87 {1}
5 16 {7}
6 59 {3}
7 55 {3}
8 52 {1}
9 92 {8}
Round   9, Devices participated 10, Average loss 0.042, Central accuracy on global test data 47.314, Ensemble accuracy on global test data 49.297, Local accuracy on global train data 16.162, Local accuracy on local train data 98.683


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8}
1 86 {0}
2 66 {6}
3 56 {0}
4 91 {1}
5 50 {8}
6 77 {6}
7 96 {0}
8 48 {4}
9 81 {0}
Round  10, Devices participated 10, Average loss 0.026, Central accuracy on global test data 50.537, Ensemble accuracy on global test data 48.525, Local accuracy on global train data 20.225, Local accuracy on local train data 99.283


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {5}
1 43 {0}
2 38 {4, 5}
3 64 {7}
4 39 {1}
5 36 {5}
6 90 {5}
7 28 {6}
8 74 {4}
9 34 {3}
Round  11, Devices participated 10, Average loss 0.043, Central accuracy on global test data 52.275, Ensemble accuracy on global test data 49.678, Local accuracy on global train data 15.750, Local accuracy on local train data 98.983


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {5}
1 47 {5}
2 62 {0}
3 28 {6}
4 77 {6}
5 37 {9}
6 14 {6}
7 36 {5}
8 88 {6, 7}
9 71 {3, 4}
Round  12, Devices participated 10, Average loss 0.032, Central accuracy on global test data 45.078, Ensemble accuracy on global test data 42.383, Local accuracy on global train data 15.886, Local accuracy on local train data 99.283


{'layer_input': array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 48 {4}
1 24 {2}
2 40 {2}
3 33 {0}
4 91 {1}
5 70 {5}
6 30 {8}
7 77 {6}
8 69 {5}
9 63 {5}
Round  13, Devices participated 10, Average loss 0.038, Central accuracy on global test data 48.252, Ensemble accuracy on global test data 46.631, Local accuracy on global train data 13.052, Local accuracy on local train data 99.067


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {9}
1 93 {9}
2 67 {1}
3 11 {4}
4 75 {4}
5 56 {0}
6 52 {1}
7 48 {4}
8 18 {0}
9 64 {7}
Round  14, Devices participated 10, Average loss 0.022, Central accuracy on global test data 60.586, Ensemble accuracy on global test data 52.676, Local accuracy on global train data 16.492, Local accuracy on local train data 99.250


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 17 {8}
1 56 {0}
2 6 {2}
3 55 {3}
4 57 {9}
5 4 {8}
6 11 {4}
7 44 {2}
8 74 {4}
9 27 {9}
Round  15, Devices participated 10, Average loss 0.023, Central accuracy on global test data 55.205, Ensemble accuracy on global test data 52.764, Local accuracy on global train data 13.926, Local accuracy on local train data 99.167


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 29 {3}
1 25 {6}
2 28 {6}
3 22 {3}
4 72 {5}
5 5 {7}
6 33 {0}
7 93 {9}
8 70 {5}
9 84 {1}
Round  16, Devices participated 10, Average loss 0.026, Central accuracy on global test data 69.756, Ensemble accuracy on global test data 67.080, Local accuracy on global train data 15.210, Local accuracy on local train data 99.183


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {2}
1 10 {7}
2 22 {3}
3 5 {7}
4 1 {4}
5 76 {0, 1}
6 61 {1}
7 96 {0}
8 94 {2, 3}
9 38 {4, 5}
Round  17, Devices participated 10, Average loss 0.031, Central accuracy on global test data 66.045, Ensemble accuracy on global test data 66.191, Local accuracy on global train data 21.948, Local accuracy on local train data 99.083


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 50 {8}
1 66 {6}
2 76 {0, 1}
3 47 {5}
4 84 {1}
5 54 {7}
6 91 {1}
7 56 {0}
8 42 {7}
9 96 {0}
Round  18, Devices participated 10, Average loss 0.017, Central accuracy on global test data 64.297, Ensemble accuracy on global test data 63.232, Local accuracy on global train data 27.429, Local accuracy on local train data 99.617


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 54 {7}
2 42 {7}
3 2 {4}
4 60 {8}
5 56 {0}
6 49 {7}
7 4 {8}
8 88 {6, 7}
9 66 {6}
Round  19, Devices participated 10, Average loss 0.014, Central accuracy on global test data 58.887, Ensemble accuracy on global test data 59.424, Local accuracy on global train data 24.758, Local accuracy on local train data 99.617


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {6}
1 63 {5}
2 20 {2}
3 82 {6}
4 33 {0}
5 71 {3, 4}
6 86 {0}
7 75 {4}
8 38 {4, 5}
9 81 {0}
Round  20, Devices participated 10, Average loss 0.026, Central accuracy on global test data 66.201, Ensemble accuracy on global test data 64.277, Local accuracy on global train data 21.782, Local accuracy on local train data 99.300


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {8}
1 27 {9}
2 45 {6}
3 43 {0}
4 35 {8}
5 86 {0}
6 24 {2}
7 33 {0}
8 73 {1, 2}
9 88 {6, 7}
Round  21, Devices participated 10, Average loss 0.028, Central accuracy on global test data 74.121, Ensemble accuracy on global test data 69.160, Local accuracy on global train data 24.934, Local accuracy on local train data 99.167


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 14 {6}
2 98 {5}
3 54 {7}
4 97 {3}
5 8 {0}
6 16 {7}
7 31 {4}
8 40 {2}
9 83 {7}
Round  22, Devices participated 10, Average loss 0.020, Central accuracy on global test data 73.320, Ensemble accuracy on global test data 73.027, Local accuracy on global train data 20.603, Local accuracy on local train data 99.467


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {1}
1 87 {1}
2 31 {4}
3 70 {5}
4 36 {5}
5 99 {5}
6 83 {7}
7 11 {4}
8 45 {6}
9 47 {5}
Round  23, Devices participated 10, Average loss 0.020, Central accuracy on global test data 54.385, Ensemble accuracy on global test data 51.143, Local accuracy on global train data 21.536, Local accuracy on local train data 99.633


{'layer_input': array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {6}
1 18 {0}
2 27 {9}
3 93 {9}
4 87 {1}
5 70 {5}
6 14 {6}
7 88 {6, 7}
8 24 {2}
9 91 {1}
Round  24, Devices participated 10, Average loss 0.023, Central accuracy on global test data 60.664, Ensemble accuracy on global test data 59.531, Local accuracy on global train data 20.081, Local accuracy on local train data 99.167


{'layer_input': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {0}
1 46 {3}
2 65 {9}
3 5 {7}
4 25 {6}
5 63 {5}
6 35 {8}
7 32 {2}
8 36 {5}
9 33 {0}
Round  25, Devices participated 10, Average loss 0.021, Central accuracy on global test data 73.760, Ensemble accuracy on global test data 72.744, Local accuracy on global train data 20.188, Local accuracy on local train data 99.433


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 94 {2, 3}
1 6 {2}
2 34 {3}
3 55 {3}
4 87 {1}
5 23 {7}
6 82 {6}
7 41 {8, 9}
8 63 {5}
9 93 {9}
Round  26, Devices participated 10, Average loss 0.030, Central accuracy on global test data 61.211, Ensemble accuracy on global test data 66.748, Local accuracy on global train data 22.021, Local accuracy on local train data 99.167


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 43 {0}
1 96 {0}
2 57 {9}
3 69 {5}
4 91 {1}
5 17 {8}
6 33 {0}
7 39 {1}
8 93 {9}
9 36 {5}
Round  27, Devices participated 10, Average loss 0.016, Central accuracy on global test data 63.965, Ensemble accuracy on global test data 61.758, Local accuracy on global train data 25.168, Local accuracy on local train data 99.517


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(9.4111, device='cuda:0', grad_fn=<MaxBackward1>)
0 22 {3}
1 47 {5}
2 61 {1}
3 27 {9}
4 50 {8}
5 96 {0}
6 89 {9}
7 97 {3}
8 85 {2}
9 30 {8}
Round  28, Devices participated 10, Average loss 0.021, Central accuracy on global test data 53.789, Ensemble accuracy on global test data 67.080, Local accuracy on global train data 19.492, Local accuracy on local train data 99.467


{'layer_input': array([0, 1, 1, 2, 2, 0, 1, 1, 1, 1]), 'layer_hidden1': array([0, 1, 1, 2, 2, 0, 1, 1, 1, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
q 0 44 {2}
1 70 {5}
2 23 {7}
3 38 {4, 5}
4 74 {4}
5 21 {2}
6 82 {6}
7 4 {8}
8 88 {6, 7}
9 98 {5}
Round  29, Devices participated 10, Average loss 0.045, Central accuracy on global test data 59.990, Ensemble accuracy on global test data 64.189, Local accuracy on global train data 17.842, Local accuracy on local train data 98.767


{'layer_input': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {5}
1 72 {5}
2 65 {9}
3 50 {8}
4 85 {2}
5 56 {0}
6 4 {8}
7 80 {1}
8 31 {4}
9 17 {8}
Round  30, Devices participated 10, Average loss 0.038, Central accuracy on global test data 68.926, Ensemble accuracy on global test data 70.918, Local accuracy on global train data 18.140, Local accuracy on local train data 99.183


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 40 {2}
2 41 {8, 9}
3 34 {3}
4 97 {3}
5 51 {5, 6}
6 50 {8}
7 13 {4}
8 22 {3}
9 8 {0}
Round  31, Devices participated 10, Average loss 0.035, Central accuracy on global test data 69.287, Ensemble accuracy on global test data 71.963, Local accuracy on global train data 25.452, Local accuracy on local train data 98.900


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {1}
1 54 {7}
2 63 {5}
3 83 {7}
4 96 {0}
5 26 {1}
6 0 {8, 7}
7 59 {3}
8 69 {5}
9 14 {6}
Round  32, Devices participated 10, Average loss 0.023, Central accuracy on global test data 74.844, Ensemble accuracy on global test data 74.795, Local accuracy on global train data 30.425, Local accuracy on local train data 99.350


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {8}
1 47 {5}
2 83 {7}
3 36 {5}
4 8 {0}
5 2 {4}
6 56 {0}
7 49 {7}
8 10 {7}
9 38 {4, 5}
Round  33, Devices participated 10, Average loss 0.016, Central accuracy on global test data 65.527, Ensemble accuracy on global test data 66.748, Local accuracy on global train data 30.198, Local accuracy on local train data 99.567


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 90 {5}
2 81 {0}
3 54 {7}
4 59 {3}
5 4 {8}
6 53 {1}
7 47 {5}
8 52 {1}
9 19 {9}
Round  34, Devices participated 10, Average loss 0.020, Central accuracy on global test data 74.834, Ensemble accuracy on global test data 73.252, Local accuracy on global train data 23.936, Local accuracy on local train data 99.400


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 24 {2}
1 13 {4}
2 81 {0}
3 73 {1, 2}
4 19 {9}
5 55 {3}
6 40 {2}
7 76 {0, 1}
8 57 {9}
9 96 {0}
Round  35, Devices participated 10, Average loss 0.025, Central accuracy on global test data 70.615, Ensemble accuracy on global test data 70.557, Local accuracy on global train data 26.851, Local accuracy on local train data 99.150


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {4}
1 75 {4}
2 94 {2, 3}
3 20 {2}
4 87 {1}
5 26 {1}
6 29 {3}
7 70 {5}
8 2 {4}
9 50 {8}
Round  36, Devices participated 10, Average loss 0.022, Central accuracy on global test data 72.363, Ensemble accuracy on global test data 68.389, Local accuracy on global train data 23.386, Local accuracy on local train data 99.217


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {8, 7}
1 9 {3}
2 46 {3}
3 37 {9}
4 72 {5}
5 83 {7}
6 73 {1, 2}
7 16 {7}
8 55 {3}
9 27 {9}
Round  37, Devices participated 10, Average loss 0.033, Central accuracy on global test data 67.070, Ensemble accuracy on global test data 68.662, Local accuracy on global train data 19.348, Local accuracy on local train data 99.050


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {1}
1 29 {3}
2 9 {3}
3 50 {8}
4 26 {1}
5 81 {0}
6 48 {4}
7 82 {6}
8 37 {9}
9 15 {2}
Round  38, Devices participated 10, Average loss 0.017, Central accuracy on global test data 79.746, Ensemble accuracy on global test data 79.180, Local accuracy on global train data 23.345, Local accuracy on local train data 99.533


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 82 {6}
1 6 {2}
2 87 {1}
3 74 {4}
4 29 {3}
5 50 {8}
6 62 {0}
7 67 {1}
8 12 {1}
9 83 {7}
Round  39, Devices participated 10, Average loss 0.012, Central accuracy on global test data 74.873, Ensemble accuracy on global test data 80.850, Local accuracy on global train data 33.547, Local accuracy on local train data 99.667


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {5}
1 75 {4}
2 89 {9}
3 43 {0}
4 12 {1}
5 21 {2}
6 95 {3}
7 72 {5}
8 24 {2}
9 63 {5}
Round  40, Devices participated 10, Average loss 0.023, Central accuracy on global test data 69.131, Ensemble accuracy on global test data 72.148, Local accuracy on global train data 22.197, Local accuracy on local train data 99.217


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3}
1 15 {2}
2 31 {4}
3 94 {2, 3}
4 46 {3}
5 70 {5}
6 19 {9}
7 80 {1}
8 6 {2}
9 88 {6, 7}
Round  41, Devices participated 10, Average loss 0.028, Central accuracy on global test data 79.971, Ensemble accuracy on global test data 75.762, Local accuracy on global train data 22.646, Local accuracy on local train data 99.133


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {5}
1 92 {8}
2 72 {5}
3 77 {6}
4 23 {7}
5 53 {1}
6 1 {4}
7 12 {1}
8 97 {3}
9 93 {9}
Round  42, Devices participated 10, Average loss 0.014, Central accuracy on global test data 72.744, Ensemble accuracy on global test data 79.502, Local accuracy on global train data 28.984, Local accuracy on local train data 99.617


{'layer_input': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden1': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'layer_hidden2': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:348: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:368: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 355, in DFAN_regavg
    loss_S2 = diff_L2
  File "/opt/conda/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --noniid_hard --alpha                   alpha_sczle   ale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 345, in DFAN_regavg
    t_logit = torch.zeros_like(teacher[0](fake).detach())
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/deepedge/models/Nets.py", line 32, in forward
    x = self.relu(x)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 94, in forward
    return F.relu(input, inplace=self.inplace)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 914, in relu
    result = torch.relu(input)
KeyboardInterrupt
^C
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {4}
1 82 {2}
2 28 {1}
3 64 {1}
4 79 {1}
5 13 {7}
6 72 {5}
7 17 {6}
8 74 {1}
9 51 {1}
Round   0, Devices participated 10, Average loss 0.484, Central accuracy on global test data 11.914, Ensemble accuracy on global test data 11.895, Local accuracy on global train data 10.449, Local accuracy on local train data 95.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {4}
1 43 {3}
2 69 {9}
3 56 {9}
4 94 {7}
5 28 {1}
6 61 {6}
7 86 {0}
8 14 {2}
9 11 {7}
Round   1, Devices participated 10, Average loss 0.384, Central accuracy on global test data 40.547, Ensemble accuracy on global test data 37.539, Local accuracy on global train data 11.797, Local accuracy on local train data 95.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.8570, device='cuda:0', grad_fn=<MaxBackward1>)
0 81 {9}
1 43 {3}
2 17 {6}
3 80 {8}
4 96 {9}
5 45 {2}
6 87 {3}
7 57 {2}
8 44 {0}
9 4 {0}
Round   2, Devices participated 10, Average loss 0.468, Central accuracy on global test data 41.660, Ensemble accuracy on global test data 41.689, Local accuracy on global train data 19.563, Local accuracy on local train data 95.767


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 76 {9}
1 85 {7}
2 26 {6}
3 13 {7}
4 53 {1}
5 90 {6}
6 15 {0}
7 51 {1}
8 23 {2}
9 47 {3}
Round   3, Devices participated 10, Average loss 0.138, Central accuracy on global test data 42.207, Ensemble accuracy on global test data 44.355, Local accuracy on global train data 25.603, Local accuracy on local train data 97.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {2}
1 86 {0}
2 4 {0}
3 27 {8}
4 29 {3}
5 85 {7}
6 22 {0}
7 28 {1}
8 0 {2}
9 96 {9}
Round   4, Devices participated 10, Average loss 0.223, Central accuracy on global test data 43.184, Ensemble accuracy on global test data 40.908, Local accuracy on global train data 23.391, Local accuracy on local train data 97.433


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 97 {0}
1 38 {6, 7}
2 30 {7}
3 73 {4}
4 55 {0}
5 82 {2}
6 31 {5}
7 32 {2}
8 56 {9}
9 23 {2}
Round   5, Devices participated 10, Average loss 0.132, Central accuracy on global test data 51.455, Ensemble accuracy on global test data 48.281, Local accuracy on global train data 22.104, Local accuracy on local train data 97.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {9}
1 55 {0}
2 64 {1}
3 37 {1}
4 14 {2}
5 6 {7}
6 93 {7}
7 41 {3}
8 44 {0}
9 8 {7}
Round   6, Devices participated 10, Average loss 0.113, Central accuracy on global test data 49.395, Ensemble accuracy on global test data 49.658, Local accuracy on global train data 30.146, Local accuracy on local train data 97.800


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {2}
1 79 {1}
2 2 {4}
3 0 {2}
4 34 {1}
5 27 {8}
6 76 {9}
7 60 {3}
8 3 {4}
9 96 {9}
Round   7, Devices participated 10, Average loss 0.197, Central accuracy on global test data 44.521, Ensemble accuracy on global test data 42.559, Local accuracy on global train data 24.153, Local accuracy on local train data 97.033


^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 142, in <module>
    acc_l, _ = test_img(local_user[user_idx].net, dataset_train, args, stop_at_batch=16, shuffle=True)
  File "/workspace/deepedge/models/test.py", line 21, in test_img
    for idx, (data, target) in enumerate(data_loader):
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py", line 97, in __getitem__
    img = self.transform(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 70, in __call__
    img = t(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 207, in __call__
    return F.resize(img, self.size, self.interpolation)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py", line 256, in resize
    return img.resize(size[::-1], interpolation)
  File "/opt/conda/lib/python3.7/site-packages/PIL/Image.py", line 1843, in resize
    box = (0, 0) + self.size
  File "/opt/conda/lib/python3.7/site-packages/PIL/Image.py", line 522, in size
    @property
KeyboardInterrupt
^C
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 27.344, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.289, Central accuracy on global test data 34.570, Ensemble accuracy on global test data 32.393, Local accuracy on global train data 23.535, Local accuracy on local train data 93.467


^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 142, in <module>
    acc_l, _ = test_img(local_user[user_idx].net, dataset_train, args, stop_at_batch=16, shuffle=True)
  File "/workspace/deepedge/models/test.py", line 21, in test_img
    for idx, (data, target) in enumerate(data_loader):
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py", line 97, in __getitem__
    img = self.transform(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 70, in __call__
    img = t(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 101, in __call__
    return F.to_tensor(pic)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py", line 95, in to_tensor
    img = img.view(pic.size[1], pic.size[0], nchannel)
KeyboardInterrupt
^C
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 27.344, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.289, Central accuracy on global test data 34.570, Ensemble accuracy on global test data 32.393, Local accuracy on global train data 23.535, Local accuracy on local train data 93.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.4972, device='cuda:0', grad_fn=<MaxBackward1>)
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.378, Central accuracy on global test data 48.662, Ensemble accuracy on global test data 48.867, Local accuracy on global train data 32.808, Local accuracy on local train data 95.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.380, Central accuracy on global test data 63.252, Ensemble accuracy on global test data 62.852, Local accuracy on global train data 37.227, Local accuracy on local train data 94.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.248, Central accuracy on global test data 73.369, Ensemble accuracy on global test data 73.564, Local accuracy on global train data 45.723, Local accuracy on local train data 96.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.241, Central accuracy on global test data 72.891, Ensemble accuracy on global test data 73.408, Local accuracy on global train data 50.691, Local accuracy on local train data 95.950


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.212, Central accuracy on global test data 72.285, Ensemble accuracy on global test data 72.500, Local accuracy on global train data 48.350, Local accuracy on local train data 95.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.228, Central accuracy on global test data 76.709, Ensemble accuracy on global test data 78.086, Local accuracy on global train data 47.383, Local accuracy on local train data 95.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.173, Central accuracy on global test data 76.055, Ensemble accuracy on global test data 76.338, Local accuracy on global train data 47.747, Local accuracy on local train data 95.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.152, Central accuracy on global test data 74.541, Ensemble accuracy on global test data 74.746, Local accuracy on global train data 48.411, Local accuracy on local train data 96.267


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.170, Central accuracy on global test data 79.512, Ensemble accuracy on global test data 79.688, Local accuracy on global train data 45.996, Local accuracy on local train data 95.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.112, Central accuracy on global test data 77.207, Ensemble accuracy on global test data 76.445, Local accuracy on global train data 54.839, Local accuracy on local train data 96.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.181, Central accuracy on global test data 77.949, Ensemble accuracy on global test data 77.754, Local accuracy on global train data 50.444, Local accuracy on local train data 95.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.170, Central accuracy on global test data 80.713, Ensemble accuracy on global test data 80.781, Local accuracy on global train data 46.892, Local accuracy on local train data 95.867


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.187, Central accuracy on global test data 81.387, Ensemble accuracy on global test data 80.957, Local accuracy on global train data 53.525, Local accuracy on local train data 94.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.126, Central accuracy on global test data 82.529, Ensemble accuracy on global test data 83.066, Local accuracy on global train data 51.907, Local accuracy on local train data 96.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 83 {9, 2}
2 22 {0, 5}
3 69 {2, 3}
4 1 {8, 4}
5 54 {1, 6}
6 62 {0, 7}
7 36 {1, 3}
8 58 {9, 4}
9 63 {4, 5}
Round  16, Devices participated 10, Average loss 0.205, Central accuracy on global test data 81.318, Ensemble accuracy on global test data 82.021, Local accuracy on global train data 50.596, Local accuracy on local train data 95.233


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 68 {9, 6}
2 4 {8, 7}
3 10 {3, 7}
4 33 {5, 6}
5 96 {1, 3}
6 15 {5, 7}
7 64 {1, 2}
8 89 {0, 9}
9 54 {1, 6}
Round  17, Devices participated 10, Average loss 0.133, Central accuracy on global test data 82.227, Ensemble accuracy on global test data 82.598, Local accuracy on global train data 47.947, Local accuracy on local train data 96.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {1, 3}
1 22 {0, 5}
2 1 {8, 4}
3 40 {3, 5}
4 41 {0, 3}
5 52 {8, 7}
6 23 {1, 5}
7 75 {8, 3}
8 62 {0, 7}
9 11 {4, 5}
Round  18, Devices participated 10, Average loss 0.214, Central accuracy on global test data 78.896, Ensemble accuracy on global test data 78.848, Local accuracy on global train data 51.340, Local accuracy on local train data 95.200


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 27 {2, 5}
2 8 {8, 7}
3 34 {3, 7}
4 98 {4}
5 74 {5}
6 36 {1, 3}
7 78 {2, 3, 4}
8 85 {1, 4}
9 88 {0}
Round  19, Devices participated 10, Average loss 0.147, Central accuracy on global test data 80.215, Ensemble accuracy on global test data 80.811, Local accuracy on global train data 48.479, Local accuracy on local train data 97.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 2 {9}
2 83 {9, 2}
3 32 {0, 9}
4 78 {2, 3, 4}
5 39 {8, 0}
6 25 {8, 2}
7 56 {9, 2}
8 76 {1, 3}
9 35 {8, 2}
Round  20, Devices participated 10, Average loss 0.226, Central accuracy on global test data 70.596, Ensemble accuracy on global test data 72.236, Local accuracy on global train data 43.445, Local accuracy on local train data 95.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 2 {9}
2 40 {3, 5}
3 64 {1, 2}
4 12 {8, 9, 3}
5 94 {0, 1}
6 60 {2}
7 24 {4, 5}
8 61 {6}
9 29 {9}
Round  21, Devices participated 10, Average loss 0.170, Central accuracy on global test data 76.416, Ensemble accuracy on global test data 77.334, Local accuracy on global train data 44.307, Local accuracy on local train data 96.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 26 {9, 7}
2 80 {6}
3 38 {2, 7}
4 85 {1, 4}
5 86 {1, 2}
6 11 {4, 5}
7 5 {0, 6, 7}
8 98 {4}
9 96 {1, 3}
Round  22, Devices participated 10, Average loss 0.154, Central accuracy on global test data 80.566, Ensemble accuracy on global test data 80.420, Local accuracy on global train data 50.564, Local accuracy on local train data 96.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 41 {0, 3}
1 36 {1, 3}
2 92 {1, 3}
3 48 {3, 4}
4 96 {1, 3}
5 76 {1, 3}
6 77 {8, 1}
7 57 {4, 6}
8 78 {2, 3, 4}
9 28 {2, 7}
Round  23, Devices participated 10, Average loss 0.223, Central accuracy on global test data 72.070, Ensemble accuracy on global test data 69.238, Local accuracy on global train data 54.888, Local accuracy on local train data 96.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 65 {3, 4}
2 87 {8, 9}
3 31 {0, 1}
4 89 {0, 9}
5 10 {3, 7}
6 70 {8, 5}
7 44 {6, 7}
8 28 {2, 7}
9 41 {0, 3}
Round  24, Devices participated 10, Average loss 0.214, Central accuracy on global test data 80.215, Ensemble accuracy on global test data 80.498, Local accuracy on global train data 46.201, Local accuracy on local train data 95.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 18 {0, 6}
2 59 {0, 7}
3 53 {0, 7}
4 99 {8, 9}
5 72 {1, 9}
6 90 {8, 6, 7}
7 14 {9, 5}
8 13 {8, 5, 6}
9 80 {6}
Round  25, Devices participated 10, Average loss 0.346, Central accuracy on global test data 77.646, Ensemble accuracy on global test data 78.740, Local accuracy on global train data 45.374, Local accuracy on local train data 96.200


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 61 {6}
2 33 {5, 6}
3 12 {8, 9, 3}
4 87 {8, 9}
5 20 {2, 7}
6 3 {8, 6}
7 34 {3, 7}
8 9 {1}
9 63 {4, 5}
Round  26, Devices participated 10, Average loss 0.377, Central accuracy on global test data 79.209, Ensemble accuracy on global test data 80.527, Local accuracy on global train data 50.881, Local accuracy on local train data 95.767


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 31 {0, 1}
2 28 {2, 7}
3 90 {8, 6, 7}
4 19 {5, 6}
5 26 {9, 7}
6 73 {0, 6}
7 23 {1, 5}
8 11 {4, 5}
9 96 {1, 3}
Round  27, Devices participated 10, Average loss 0.278, Central accuracy on global test data 78.213, Ensemble accuracy on global test data 78.643, Local accuracy on global train data 54.006, Local accuracy on local train data 96.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))

tensor(8.1315, device='cuda:0', grad_fn=<MaxBackward1>)
0 3 {8, 6}
1 55 {4, 7}
2 1 {8, 4}
3 5 {0, 6, 7}
4 93 {9, 6}
5 53 {0, 7}
6 7 {0, 1}
7 96 {1, 3}
8 57 {4, 6}
9 64 {1, 2}
Round  28, Devices participated 10, Average loss 0.229, Central accuracy on global test data 82.002, Ensemble accuracy on global test data 81.875, Local accuracy on global train data 54.551, Local accuracy on local train data 96.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 46 {9, 6}
2 99 {8, 9}
3 78 {2, 3, 4}
4 33 {5, 6}
5 79 {0, 8}
6 63 {4, 5}
7 13 {8, 5, 6}
8 26 {9, 7}
9 10 {3, 7}
Round  29, Devices participated 10, Average loss 0.311, Central accuracy on global test data 82.480, Ensemble accuracy on global test data 83.477, Local accuracy on global train data 51.045, Local accuracy on local train data 94.767


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 21 {2, 5}
2 40 {3, 5}
3 74 {5}
4 49 {2, 7}
5 36 {1, 3}
6 1 {8, 4}
7 24 {4, 5}
8 71 {1, 5}
9 79 {0, 8}
Round  30, Devices participated 10, Average loss 0.223, Central accuracy on global test data 74.990, Ensemble accuracy on global test data 74.541, Local accuracy on global train data 42.986, Local accuracy on local train data 96.450


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 68 {9, 6}
2 43 {8, 3}
3 85 {1, 4}
4 10 {3, 7}
5 2 {9}
6 83 {9, 2}
7 94 {0, 1}
8 0 {1, 2}
9 66 {1, 3}
Round  31, Devices participated 10, Average loss 0.199, Central accuracy on global test data 77.393, Ensemble accuracy on global test data 76.387, Local accuracy on global train data 49.575, Local accuracy on local train data 96.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 16 {0, 3}
2 32 {0, 9}
3 76 {1, 3}
4 4 {8, 7}
5 51 {4, 7}
6 92 {1, 3}
7 33 {5, 6}
8 27 {2, 5}
9 86 {1, 2}
Round  32, Devices participated 10, Average loss 0.159, Central accuracy on global test data 80.918, Ensemble accuracy on global test data 81.777, Local accuracy on global train data 53.198, Local accuracy on local train data 97.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 40 {3, 5}
2 15 {5, 7}
3 16 {0, 3}
4 66 {1, 3}
5 10 {3, 7}
6 87 {8, 9}
7 1 {8, 4}
8 2 {9}
9 41 {0, 3}
Round  33, Devices participated 10, Average loss 0.200, Central accuracy on global test data 79.375, Ensemble accuracy on global test data 79.463, Local accuracy on global train data 44.568, Local accuracy on local train data 96.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 15 {5, 7}
2 38 {2, 7}
3 44 {6, 7}
4 88 {0}
5 56 {9, 2}
6 19 {5, 6}
7 71 {1, 5}
8 29 {9}
9 61 {6}
Round  34, Devices participated 10, Average loss 0.111, Central accuracy on global test data 80.205, Ensemble accuracy on global test data 79.209, Local accuracy on global train data 44.844, Local accuracy on local train data 97.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 95 {3, 4}
1 67 {8, 1}
2 46 {9, 6}
3 57 {4, 6}
4 1 {8, 4}
5 27 {2, 5}
6 48 {3, 4}
7 44 {6, 7}
8 16 {0, 3}
9 87 {8, 9}
Round  35, Devices participated 10, Average loss 0.161, Central accuracy on global test data 80.439, Ensemble accuracy on global test data 80.840, Local accuracy on global train data 47.036, Local accuracy on local train data 96.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 0 {1, 2}
2 40 {3, 5}
3 32 {0, 9}
4 39 {8, 0}
5 76 {1, 3}
6 73 {0, 6}
7 79 {0, 8}
8 42 {8, 1}
9 24 {4, 5}
Round  36, Devices participated 10, Average loss 0.169, Central accuracy on global test data 80.508, Ensemble accuracy on global test data 80.361, Local accuracy on global train data 50.769, Local accuracy on local train data 96.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 91 {2, 6}
2 93 {9, 6}
3 36 {1, 3}
4 92 {1, 3}
5 58 {9, 4}
6 73 {0, 6}
7 77 {8, 1}
8 13 {8, 5, 6}
9 35 {8, 2}
Round  37, Devices participated 10, Average loss 0.190, Central accuracy on global test data 79.561, Ensemble accuracy on global test data 79.463, Local accuracy on global train data 50.891, Local accuracy on local train data 94.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 10 {3, 7}
2 35 {8, 2}
3 76 {1, 3}
4 90 {8, 6, 7}
5 41 {0, 3}
6 60 {2}
7 97 {4, 7}
8 19 {5, 6}
9 92 {1, 3}
Round  38, Devices participated 10, Average loss 0.142, Central accuracy on global test data 80.713, Ensemble accuracy on global test data 80.557, Local accuracy on global train data 48.904, Local accuracy on local train data 96.417


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 56 {9, 2}
2 50 {2, 3}
3 98 {4}
4 88 {0}
5 2 {9}
6 39 {8, 0}
7 22 {0, 5}
8 30 {0, 1, 2}
9 23 {1, 5}
Round  39, Devices participated 10, Average loss 0.113, Central accuracy on global test data 80.830, Ensemble accuracy on global test data 79.639, Local accuracy on global train data 48.386, Local accuracy on local train data 96.950


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 21 {2, 5}
2 87 {8, 9}
3 35 {8, 2}
4 34 {3, 7}
5 5 {0, 6, 7}
6 89 {0, 9}
7 44 {6, 7}
8 17 {3, 4}
9 65 {3, 4}
Round  40, Devices participated 10, Average loss 0.142, Central accuracy on global test data 81.484, Ensemble accuracy on global test data 82.002, Local accuracy on global train data 47.126, Local accuracy on local train data 96.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 42 {8, 1}
2 5 {0, 6, 7}
3 15 {5, 7}
4 71 {1, 5}
5 28 {2, 7}
6 12 {8, 9, 3}
7 22 {0, 5}
8 35 {8, 2}
9 14 {9, 5}
Round  41, Devices participated 10, Average loss 0.139, Central accuracy on global test data 81.279, Ensemble accuracy on global test data 80.254, Local accuracy on global train data 49.365, Local accuracy on local train data 95.883


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 45 {4, 5}
2 55 {4, 7}
3 25 {8, 2}
4 38 {2, 7}
5 91 {2, 6}
6 57 {4, 6}
7 61 {6}
8 83 {9, 2}
9 53 {0, 7}
Round  42, Devices participated 10, Average loss 0.132, Central accuracy on global test data 80.410, Ensemble accuracy on global test data 80.742, Local accuracy on global train data 47.747, Local accuracy on local train data 96.633


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:329: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:349: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 341, in DFAN_regavg
    loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/deepedge/network/gan.py", line 44, in forward
    img = self.conv_blocks1(img)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 98, in forward
    self.num_batches_tracked = self.num_batches_tracked + 1
KeyboardInterrupt
^C
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 314, in DFAN_regavg
    mlpa, mlpt = fusion_layer_MLP(teacher, student, 1024, 200, 10)
  File "/workspace/deepedge/models/DFAN.py", line 401, in fusion_layer_MLP
    return MLP_agg_layer(dim_in, dim_hidden, dim_out, models, cluster_labels), MLP_trimmed_layer(dim_in, dim_hidden, dim_out, models, cluster_labels)
  File "/workspace/deepedge/models/DFAN.py", line 573, in __init__
    for ln in layers:
NameError: name 'layers' is not defined
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 314, in DFAN_regavg
    mlpa, mlpt = fusion_layer_MLP(teacher, student, 1024, 200, 10)
  File "/workspace/deepedge/models/DFAN.py", line 401, in fusion_layer_MLP
    return MLP_agg_layer(dim_in, dim_hidden, dim_out, models, cluster_labels), MLP_trimmed_layer(dim_in, dim_hidden, dim_out, models, cluster_labels)
  File "/workspace/deepedge/models/DFAN.py", line 577, in __init__
    self.alpha[ln+'.weight'] = float((cluster_labels[ln] == max_cluster_label).sum()-1) / (num_models-1)
NameError: name 'num_models' is not defined
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 314, in DFAN_regavg
    mlpa, mlpt = fusion_layer_MLP(teacher, student, 1024, 200, 10)
  File "/workspace/deepedge/models/DFAN.py", line 401, in fusion_layer_MLP
    return MLP_agg_layer(dim_in, dim_hidden, dim_out, models, cluster_labels), MLP_trimmed_layer(dim_in, dim_hidden, dim_out, models, cluster_labels)
  File "/workspace/deepedge/models/DFAN.py", line 578, in __init__
    self.alpha[ln+'.bias'] = alpha[ln+'.weight']
NameError: name 'alpha' is not defined
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 23.008, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.469, Central accuracy on global test data 39.336, Ensemble accuracy on global test data 32.139, Local accuracy on global train data 22.712, Local accuracy on local train data 90.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.665, Central accuracy on global test data 46.465, Ensemble accuracy on global test data 43.193, Local accuracy on global train data 29.536, Local accuracy on local train data 93.283


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.509, Central accuracy on global test data 60.811, Ensemble accuracy on global test data 59.102, Local accuracy on global train data 35.625, Local accuracy on local train data 92.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.291, Central accuracy on global test data 67.686, Ensemble accuracy on global test data 66.270, Local accuracy on global train data 39.834, Local accuracy on local train data 95.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.392, Central accuracy on global test data 70.654, Ensemble accuracy on global test data 70.410, Local accuracy on global train data 46.577, Local accuracy on local train data 94.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.331, Central accuracy on global test data 72.334, Ensemble accuracy on global test data 70.977, Local accuracy on global train data 45.208, Local accuracy on local train data 94.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.233, Central accuracy on global test data 72.500, Ensemble accuracy on global test data 72.725, Local accuracy on global train data 47.898, Local accuracy on local train data 94.383


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.227, Central accuracy on global test data 75.020, Ensemble accuracy on global test data 76.465, Local accuracy on global train data 51.479, Local accuracy on local train data 96.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.192, Central accuracy on global test data 75.605, Ensemble accuracy on global test data 74.463, Local accuracy on global train data 48.511, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.267, Central accuracy on global test data 81.074, Ensemble accuracy on global test data 81.357, Local accuracy on global train data 50.984, Local accuracy on local train data 95.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.177, Central accuracy on global test data 77.500, Ensemble accuracy on global test data 78.350, Local accuracy on global train data 55.486, Local accuracy on local train data 96.967


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.270, Central accuracy on global test data 75.791, Ensemble accuracy on global test data 76.406, Local accuracy on global train data 48.928, Local accuracy on local train data 95.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.258, Central accuracy on global test data 79.014, Ensemble accuracy on global test data 79.922, Local accuracy on global train data 45.447, Local accuracy on local train data 95.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.249, Central accuracy on global test data 75.352, Ensemble accuracy on global test data 76.689, Local accuracy on global train data 50.681, Local accuracy on local train data 95.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.204, Central accuracy on global test data 77.402, Ensemble accuracy on global test data 78.408, Local accuracy on global train data 52.075, Local accuracy on local train data 95.733


^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 140, in <module>
    w, loss, acc_ll = local_user[user_idx].train()
  File "/workspace/deepedge/models/Update.py", line 52, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/workspace/deepedge/models/Update.py", line 20, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py", line 97, in __getitem__
    img = self.transform(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 70, in __call__
    img = t(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 101, in __call__
    return F.to_tensor(pic)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py", line 87, in to_tensor
    img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))
KeyboardInterrupt
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 23.584, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.428, Central accuracy on global test data 33.818, Ensemble accuracy on global test data 30.371, Local accuracy on global train data 22.358, Local accuracy on local train data 92.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.4930, device='cuda:0', grad_fn=<MaxBackward1>)
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.343, Central accuracy on global test data 46.533, Ensemble accuracy on global test data 47.285, Local accuracy on global train data 34.282, Local accuracy on local train data 93.983


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.261, Central accuracy on global test data 64.258, Ensemble accuracy on global test data 65.166, Local accuracy on global train data 36.228, Local accuracy on local train data 94.483


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.161, Central accuracy on global test data 55.928, Ensemble accuracy on global test data 68.086, Local accuracy on global train data 40.361, Local accuracy on local train data 96.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.197, Central accuracy on global test data 65.713, Ensemble accuracy on global test data 65.049, Local accuracy on global train data 42.917, Local accuracy on local train data 95.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.162, Central accuracy on global test data 70.186, Ensemble accuracy on global test data 69.902, Local accuracy on global train data 42.815, Local accuracy on local train data 95.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.196, Central accuracy on global test data 74.189, Ensemble accuracy on global test data 75.078, Local accuracy on global train data 39.893, Local accuracy on local train data 94.133


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.159, Central accuracy on global test data 69.482, Ensemble accuracy on global test data 73.027, Local accuracy on global train data 42.556, Local accuracy on local train data 95.617


{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.136, Central accuracy on global test data 70.420, Ensemble accuracy on global test data 72.822, Local accuracy on global train data 42.930, Local accuracy on local train data 95.600


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.150, Central accuracy on global test data 75.859, Ensemble accuracy on global test data 78.564, Local accuracy on global train data 42.744, Local accuracy on local train data 95.433


{'layer_input.weight': 0.4444444444444444, 'layer_input.bias': 0.4444444444444444, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.099, Central accuracy on global test data 75.566, Ensemble accuracy on global test data 76.357, Local accuracy on global train data 49.067, Local accuracy on local train data 96.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.157, Central accuracy on global test data 74.170, Ensemble accuracy on global test data 72.451, Local accuracy on global train data 44.199, Local accuracy on local train data 94.917


{'layer_input.weight': 0.3333333333333333, 'layer_input.bias': 0.3333333333333333, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.159, Central accuracy on global test data 75.088, Ensemble accuracy on global test data 77.529, Local accuracy on global train data 39.768, Local accuracy on local train data 95.583


{'layer_input.weight': 0.4444444444444444, 'layer_input.bias': 0.4444444444444444, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.166, Central accuracy on global test data 76.416, Ensemble accuracy on global test data 78.828, Local accuracy on global train data 48.433, Local accuracy on local train data 94.533


{'layer_input.weight': 0.6666666666666666, 'layer_input.bias': 0.6666666666666666, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.118, Central accuracy on global test data 78.281, Ensemble accuracy on global test data 80.684, Local accuracy on global train data 47.986, Local accuracy on local train data 96.133


{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 83 {9, 2}
2 22 {0, 5}
3 69 {2, 3}
4 1 {8, 4}
5 54 {1, 6}
6 62 {0, 7}
7 36 {1, 3}
8 58 {9, 4}
9 63 {4, 5}
Round  16, Devices participated 10, Average loss 0.141, Central accuracy on global test data 78.906, Ensemble accuracy on global test data 80.078, Local accuracy on global train data 47.668, Local accuracy on local train data 95.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 68 {9, 6}
2 4 {8, 7}
3 10 {3, 7}
4 33 {5, 6}
5 96 {1, 3}
6 15 {5, 7}
7 64 {1, 2}
8 89 {0, 9}
9 54 {1, 6}
Round  17, Devices participated 10, Average loss 0.105, Central accuracy on global test data 78.555, Ensemble accuracy on global test data 81.133, Local accuracy on global train data 45.720, Local accuracy on local train data 96.750


{'layer_input.weight': 0.3333333333333333, 'layer_input.bias': 0.3333333333333333, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {1, 3}
1 22 {0, 5}
2 1 {8, 4}
3 40 {3, 5}
4 41 {0, 3}
5 52 {8, 7}
6 23 {1, 5}
7 75 {8, 3}
8 62 {0, 7}
9 11 {4, 5}
Round  18, Devices participated 10, Average loss 0.185, Central accuracy on global test data 75.869, Ensemble accuracy on global test data 77.979, Local accuracy on global train data 47.698, Local accuracy on local train data 95.050


{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 27 {2, 5}
2 8 {8, 7}
3 34 {3, 7}
4 98 {4}
5 74 {5}
6 36 {1, 3}
7 78 {2, 3, 4}
8 85 {1, 4}
9 88 {0}
Round  19, Devices participated 10, Average loss 0.098, Central accuracy on global test data 72.119, Ensemble accuracy on global test data 79.561, Local accuracy on global train data 47.739, Local accuracy on local train data 97.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.3176, device='cuda:0', grad_fn=<MaxBackward1>)
0 54 {1, 6}
1 2 {9}
2 83 {9, 2}
3 32 {0, 9}
4 78 {2, 3, 4}
5 39 {8, 0}
6 25 {8, 2}
7 56 {9, 2}
8 76 {1, 3}
9 35 {8, 2}
Round  20, Devices participated 10, Average loss 0.578, Central accuracy on global test data 69.014, Ensemble accuracy on global test data 64.902, Local accuracy on global train data 40.710, Local accuracy on local train data 94.733


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 2 {9}
2 40 {3, 5}
3 64 {1, 2}
4 12 {8, 9, 3}
5 94 {0, 1}
6 60 {2}
7 24 {4, 5}
8 61 {6}
9 29 {9}
Round  21, Devices participated 10, Average loss 0.308, Central accuracy on global test data 74.365, Ensemble accuracy on global test data 72.773, Local accuracy on global train data 46.206, Local accuracy on local train data 96.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 26 {9, 7}
2 80 {6}
3 38 {2, 7}
4 85 {1, 4}
5 86 {1, 2}
6 11 {4, 5}
7 5 {0, 6, 7}
8 98 {4}
9 96 {1, 3}
Round  22, Devices participated 10, Average loss 0.286, Central accuracy on global test data 77.520, Ensemble accuracy on global test data 76.973, Local accuracy on global train data 49.409, Local accuracy on local train data 96.567


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 41 {0, 3}
1 36 {1, 3}
2 92 {1, 3}
3 48 {3, 4}
4 96 {1, 3}
5 76 {1, 3}
6 77 {8, 1}
7 57 {4, 6}
8 78 {2, 3, 4}
9 28 {2, 7}
Round  23, Devices participated 10, Average loss 0.366, Central accuracy on global test data 70.723, Ensemble accuracy on global test data 66.719, Local accuracy on global train data 51.521, Local accuracy on local train data 96.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^0 3 {8, 6}
1 65 {3, 4}
2 87 {8, 9}
3 31 {0, 1}
4 89 {0, 9}
5 10 {3, 7}
6 70 {8, 5}
7 44 {6, 7}
8 28 {2, 7}
9 41 {0, 3}
Round  24, Devices participated 10, Average loss 0.328, Central accuracy on global test data 78.027, Ensemble accuracy on global test data 78.545, Local accuracy on global train data 46.218, Local accuracy on local train data 95.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 18 {0, 6}
2 59 {0, 7}
3 53 {0, 7}
4 99 {8, 9}
5 72 {1, 9}
6 90 {8, 6, 7}
7 14 {9, 5}
8 13 {8, 5, 6}
9 80 {6}
Round  25, Devices participated 10, Average loss 0.283, Central accuracy on global test data 77.139, Ensemble accuracy on global test data 78.135, Local accuracy on global train data 51.204, Local accuracy on local train data 96.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 61 {6}
2 33 {5, 6}
3 12 {8, 9, 3}
4 87 {8, 9}
5 20 {2, 7}
6 3 {8, 6}
7 34 {3, 7}
8 9 {1}
9 63 {4, 5}
Round  26, Devices participated 10, Average loss 0.285, Central accuracy on global test data 82.764, Ensemble accuracy on global test data 82.305, Local accuracy on global train data 54.956, Local accuracy on local train data 95.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 31 {0, 1}
2 28 {2, 7}
3 90 {8, 6, 7}
4 19 {5, 6}
5 26 {9, 7}
6 73 {0, 6}
7 23 {1, 5}
8 11 {4, 5}
9 96 {1, 3}
Round  27, Devices participated 10, Average loss 0.238, Central accuracy on global test data 83.672, Ensemble accuracy on global test data 82.578, Local accuracy on global train data 57.085, Local accuracy on local train data 96.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 55 {4, 7}
2 1 {8, 4}
3 5 {0, 6, 7}
4 93 {9, 6}
5 53 {0, 7}
6 7 {0, 1}
7 96 {1, 3}
8 57 {4, 6}
9 64 {1, 2}
Round  28, Devices participated 10, Average loss 0.150, Central accuracy on global test data 83.906, Ensemble accuracy on global test data 83.174, Local accuracy on global train data 56.404, Local accuracy on local train data 97.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 46 {9, 6}
2 99 {8, 9}
3 78 {2, 3, 4}
4 33 {5, 6}
5 79 {0, 8}
6 63 {4, 5}
7 13 {8, 5, 6}
8 26 {9, 7}
9 10 {3, 7}
Round  29, Devices participated 10, Average loss 0.269, Central accuracy on global test data 84.648, Ensemble accuracy on global test data 84.854, Local accuracy on global train data 53.821, Local accuracy on local train data 95.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 21 {2, 5}
2 40 {3, 5}
3 74 {5}
4 49 {2, 7}
5 36 {1, 3}
6 1 {8, 4}
7 24 {4, 5}
8 71 {1, 5}
9 79 {0, 8}
Round  30, Devices participated 10, Average loss 0.185, Central accuracy on global test data 83.887, Ensemble accuracy on global test data 77.891, Local accuracy on global train data 49.187, Local accuracy on local train data 96.283


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 68 {9, 6}
2 43 {8, 3}
3 85 {1, 4}
4 10 {3, 7}
5 2 {9}
6 83 {9, 2}
7 94 {0, 1}
8 0 {1, 2}
9 66 {1, 3}
Round  31, Devices participated 10, Average loss 0.148, Central accuracy on global test data 80.986, Ensemble accuracy on global test data 80.957, Local accuracy on global train data 57.173, Local accuracy on local train data 96.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 16 {0, 3}
2 32 {0, 9}
3 76 {1, 3}
4 4 {8, 7}
5 51 {4, 7}
6 92 {1, 3}
7 33 {5, 6}
8 27 {2, 5}
9 86 {1, 2}
Round  32, Devices participated 10, Average loss 0.153, Central accuracy on global test data 84.336, Ensemble accuracy on global test data 84.658, Local accuracy on global train data 55.085, Local accuracy on local train data 97.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 40 {3, 5}
2 15 {5, 7}
3 16 {0, 3}
4 66 {1, 3}
5 10 {3, 7}
6 87 {8, 9}
7 1 {8, 4}
8 2 {9}
9 41 {0, 3}
Round  33, Devices participated 10, Average loss 0.194, Central accuracy on global test data 78.828, Ensemble accuracy on global test data 78.594, Local accuracy on global train data 51.084, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 15 {5, 7}
2 38 {2, 7}
3 44 {6, 7}
4 88 {0}
5 56 {9, 2}
6 19 {5, 6}
7 71 {1, 5}
8 29 {9}
9 61 {6}
Round  34, Devices participated 10, Average loss 0.127, Central accuracy on global test data 82.617, Ensemble accuracy on global test data 81.904, Local accuracy on global train data 52.361, Local accuracy on local train data 97.633


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 95 {3, 4}
1 67 {8, 1}
2 46 {9, 6}
3 57 {4, 6}
4 1 {8, 4}
5 27 {2, 5}
6 48 {3, 4}
7 44 {6, 7}
8 16 {0, 3}
9 87 {8, 9}
Round  35, Devices participated 10, Average loss 0.162, Central accuracy on global test data 83.877, Ensemble accuracy on global test data 82.383, Local accuracy on global train data 51.672, Local accuracy on local train data 96.717


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 0 {1, 2}
2 40 {3, 5}
3 32 {0, 9}
4 39 {8, 0}
5 76 {1, 3}
6 73 {0, 6}
7 79 {0, 8}
8 42 {8, 1}
9 24 {4, 5}
Round  36, Devices participated 10, Average loss 0.164, Central accuracy on global test data 85.146, Ensemble accuracy on global test data 85.898, Local accuracy on global train data 56.604, Local accuracy on local train data 96.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 91 {2, 6}
2 93 {9, 6}
3 36 {1, 3}
4 92 {1, 3}
5 58 {9, 4}
6 73 {0, 6}
7 77 {8, 1}
8 13 {8, 5, 6}
9 35 {8, 2}
Round  37, Devices participated 10, Average loss 0.229, Central accuracy on global test data 80.479, Ensemble accuracy on global test data 82.500, Local accuracy on global train data 58.079, Local accuracy on local train data 95.383


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 10 {3, 7}
2 35 {8, 2}
3 76 {1, 3}
4 90 {8, 6, 7}
5 41 {0, 3}
6 60 {2}
7 97 {4, 7}
8 19 {5, 6}
9 92 {1, 3}
Round  38, Devices participated 10, Average loss 0.162, Central accuracy on global test data 81.621, Ensemble accuracy on global test data 84.297, Local accuracy on global train data 52.634, Local accuracy on local train data 96.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 56 {9, 2}
2 50 {2, 3}
3 98 {4}
4 88 {0}
5 2 {9}
6 39 {8, 0}
7 22 {0, 5}
8 30 {0, 1, 2}
9 23 {1, 5}
Round  39, Devices participated 10, Average loss 0.180, Central accuracy on global test data 74.150, Ensemble accuracy on global test data 81.201, Local accuracy on global train data 44.412, Local accuracy on local train data 96.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 21 {2, 5}
2 87 {8, 9}
3 35 {8, 2}
4 34 {3, 7}
5 5 {0, 6, 7}
6 89 {0, 9}
7 44 {6, 7}
8 17 {3, 4}
9 65 {3, 4}
Round  40, Devices participated 10, Average loss 0.228, Central accuracy on global test data 82.559, Ensemble accuracy on global test data 83.428, Local accuracy on global train data 48.948, Local accuracy on local train data 96.433


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 42 {8, 1}
2 5 {0, 6, 7}
3 15 {5, 7}
4 71 {1, 5}
5 28 {2, 7}
6 12 {8, 9, 3}
7 22 {0, 5}
8 35 {8, 2}
9 14 {9, 5}
Round  41, Devices participated 10, Average loss 0.250, Central accuracy on global test data 79.189, Ensemble accuracy on global test data 79.551, Local accuracy on global train data 52.153, Local accuracy on local train data 95.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 45 {4, 5}
2 55 {4, 7}
3 25 {8, 2}
4 38 {2, 7}
5 91 {2, 6}
6 57 {4, 6}
7 61 {6}
8 83 {9, 2}
9 53 {0, 7}
Round  42, Devices participated 10, Average loss 0.202, Central accuracy on global test data 79.600, Ensemble accuracy on global test data 78.428, Local accuracy on global train data 52.546, Local accuracy on local train data 96.517


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 42 {8, 1}
2 89 {0, 9}
3 35 {8, 2}
4 69 {2, 3}
5 36 {1, 3}
6 74 {5}
7 53 {0, 7}
8 11 {4, 5}
9 68 {9, 6}
Round  43, Devices participated 10, Average loss 0.219, Central accuracy on global test data 82.471, Ensemble accuracy on global test data 82.227, Local accuracy on global train data 45.750, Local accuracy on local train data 96.533


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 75 {8, 3}
2 26 {9, 7}
3 31 {0, 1}
4 47 {9, 5}
5 13 {8, 5, 6}
6 36 {1, 3}
7 79 {0, 8}
8 60 {2}
9 12 {8, 9, 3}
Round  44, Devices participated 10, Average loss 0.291, Central accuracy on global test data 77.373, Ensemble accuracy on global test data 82.754, Local accuracy on global train data 53.940, Local accuracy on local train data 95.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 91 {2, 6}
2 12 {8, 9, 3}
3 7 {0, 1}
4 9 {1}
5 23 {1, 5}
6 51 {4, 7}
7 6 {1, 6}
8 43 {8, 3}
9 44 {6, 7}
Round  45, Devices participated 10, Average loss 0.202, Central accuracy on global test data 70.293, Ensemble accuracy on global test data 73.730, Local accuracy on global train data 45.117, Local accuracy on local train data 96.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 78 {2, 3, 4}
2 82 {8, 4}
3 68 {9, 6}
4 92 {1, 3}
5 30 {0, 1, 2}
6 12 {8, 9, 3}
7 58 {9, 4}
8 89 {0, 9}
9 44 {6, 7}
Round  46, Devices participated 10, Average loss 0.248, Central accuracy on global test data 79.121, Ensemble accuracy on global test data 78.809, Local accuracy on global train data 55.388, Local accuracy on local train data 95.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 65 {3, 4}
1 80 {6}
2 62 {0, 7}
3 19 {5, 6}
4 51 {4, 7}
5 92 {1, 3}
6 71 {1, 5}
7 48 {3, 4}
8 6 {1, 6}
9 60 {2}
Round  47, Devices participated 10, Average loss 0.162, Central accuracy on global test data 80.352, Ensemble accuracy on global test data 78.721, Local accuracy on global train data 52.407, Local accuracy on local train data 97.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 26 {9, 7}
2 5 {0, 6, 7}
3 88 {0}
4 83 {9, 2}
5 62 {0, 7}
6 75 {8, 3}
7 80 {6}
8 87 {8, 9}
9 36 {1, 3}
Round  48, Devices participated 10, Average loss 0.184, Central accuracy on global test data 79.971, Ensemble accuracy on global test data 81.855, Local accuracy on global train data 52.764, Local accuracy on local train data 95.950


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^C^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 358, in DFAN_regavg
    diff_L2 = torch.FloatTensor([0.]).to(args.device)
KeyboardInterrupt
^C^C
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [K
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# 
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# 
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# 
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# python main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 28.389, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.306, Central accuracy on global test data 37.100, Ensemble accuracy on global test data 33.525, Local accuracy on global train data 23.330, Local accuracy on local train data 93.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.246, Central accuracy on global test data 49.229, Ensemble accuracy on global test data 51.465, Local accuracy on global train data 34.099, Local accuracy on local train data 94.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.284, Central accuracy on global test data 61.289, Ensemble accuracy on global test data 61.758, Local accuracy on global train data 36.328, Local accuracy on local train data 94.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.394, Central accuracy on global test data 67.041, Ensemble accuracy on global test data 69.766, Local accuracy on global train data 40.237, Local accuracy on local train data 96.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.308, Central accuracy on global test data 70.713, Ensemble accuracy on global test data 70.674, Local accuracy on global train data 45.488, Local accuracy on local train data 95.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.411, Central accuracy on global test data 72.988, Ensemble accuracy on global test data 72.852, Local accuracy on global train data 41.692, Local accuracy on local train data 95.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.392, Central accuracy on global test data 76.738, Ensemble accuracy on global test data 76.807, Local accuracy on global train data 39.148, Local accuracy on local train data 94.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.290, Central accuracy on global test data 77.285, Ensemble accuracy on global test data 78.057, Local accuracy on global train data 47.275, Local accuracy on local train data 95.717


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.240, Central accuracy on global test data 74.346, Ensemble accuracy on global test data 75.801, Local accuracy on global train data 46.963, Local accuracy on local train data 95.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.334, Central accuracy on global test data 81.279, Ensemble accuracy on global test data 81.943, Local accuracy on global train data 44.429, Local accuracy on local train data 95.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.243, Central accuracy on global test data 79.277, Ensemble accuracy on global test data 80.693, Local accuracy on global train data 53.054, Local accuracy on local train data 96.567


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.4632, device='cuda:0', grad_fn=<MaxBackward1>)
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.307, Central accuracy on global test data 69.844, Ensemble accuracy on global test data 81.338, Local accuracy on global train data 51.704, Local accuracy on local train data 95.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.247, Central accuracy on global test data 69.375, Ensemble accuracy on global test data 81.934, Local accuracy on global train data 45.120, Local accuracy on local train data 95.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.214, Central accuracy on global test data 70.596, Ensemble accuracy on global test data 75.156, Local accuracy on global train data 44.473, Local accuracy on local train data 94.767


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.150, Central accuracy on global test data 64.775, Ensemble accuracy on global test data 77.285, Local accuracy on global train data 36.001, Local accuracy on local train data 96.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 83 {9, 2}
2 22 {0, 5}
3 69 {2, 3}
4 1 {8, 4}
5 54 {1, 6}
6 62 {0, 7}
7 36 {1, 3}
8 58 {9, 4}
9 63 {4, 5}
Round  16, Devices participated 10, Average loss 0.195, Central accuracy on global test data 61.182, Ensemble accuracy on global test data 71.992, Local accuracy on global train data 29.673, Local accuracy on local train data 94.617


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 68 {9, 6}
2 4 {8, 7}
3 10 {3, 7}
4 33 {5, 6}
5 96 {1, 3}
6 15 {5, 7}
7 64 {1, 2}
8 89 {0, 9}
9 54 {1, 6}
Round  17, Devices participated 10, Average loss 0.127, Central accuracy on global test data 54.111, Ensemble accuracy on global test data 66.963, Local accuracy on global train data 23.611, Local accuracy on local train data 96.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {1, 3}
1 22 {0, 5}
2 1 {8, 4}
3 40 {3, 5}
4 41 {0, 3}
5 52 {8, 7}
6 23 {1, 5}
7 75 {8, 3}
8 62 {0, 7}
9 11 {4, 5}
Round  18, Devices participated 10, Average loss 0.208, Central accuracy on global test data 46.963, Ensemble accuracy on global test data 54.756, Local accuracy on global train data 24.158, Local accuracy on local train data 94.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 27 {2, 5}
2 8 {8, 7}
3 34 {3, 7}
4 98 {4}
5 74 {5}
6 36 {1, 3}
7 78 {2, 3, 4}
8 85 {1, 4}
9 88 {0}
Round  19, Devices participated 10, Average loss 0.123, Central accuracy on global test data 42.090, Ensemble accuracy on global test data 50.547, Local accuracy on global train data 19.570, Local accuracy on local train data 96.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 2 {9}
2 83 {9, 2}
3 32 {0, 9}
4 78 {2, 3, 4}
5 39 {8, 0}
6 25 {8, 2}
7 56 {9, 2}
8 76 {1, 3}
9 35 {8, 2}
Round  20, Devices participated 10, Average loss 0.176, Central accuracy on global test data 37.217, Ensemble accuracy on global test data 35.908, Local accuracy on global train data 21.733, Local accuracy on local train data 94.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 2 {9}
2 40 {3, 5}
3 64 {1, 2}
4 12 {8, 9, 3}
5 94 {0, 1}
6 60 {2}
7 24 {4, 5}
8 61 {6}
9 29 {9}
Round  21, Devices participated 10, Average loss 0.133, Central accuracy on global test data 40.771, Ensemble accuracy on global test data 55.918, Local accuracy on global train data 18.806, Local accuracy on local train data 95.633


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 26 {9, 7}
2 80 {6}
3 38 {2, 7}
4 85 {1, 4}
5 86 {1, 2}
6 11 {4, 5}
7 5 {0, 6, 7}
8 98 {4}
9 96 {1, 3}
Round  22, Devices participated 10, Average loss 0.169, Central accuracy on global test data 38.740, Ensemble accuracy on global test data 53.633, Local accuracy on global train data 20.225, Local accuracy on local train data 95.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 41 {0, 3}
1 36 {1, 3}
2 92 {1, 3}
3 48 {3, 4}
4 96 {1, 3}
5 76 {1, 3}
6 77 {8, 1}
7 57 {4, 6}
8 78 {2, 3, 4}
9 28 {2, 7}
Round  23, Devices participated 10, Average loss 0.175, Central accuracy on global test data 39.502, Ensemble accuracy on global test data 30.127, Local accuracy on global train data 22.690, Local accuracy on local train data 94.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/deepedge/models/DFAN.py", line 345, in DFAN_regavg
    z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
KeyboardInterrupt
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 23.584, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.428, Central accuracy on global test data 32.373, Ensemble accuracy on global test data 30.371, Local accuracy on global train data 22.358, Local accuracy on local train data 92.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.340, Central accuracy on global test data 51.982, Ensemble accuracy on global test data 48.730, Local accuracy on global train data 34.348, Local accuracy on local train data 94.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.266, Central accuracy on global test data 67.695, Ensemble accuracy on global test data 68.184, Local accuracy on global train data 38.191, Local accuracy on local train data 94.383


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.165, Central accuracy on global test data 72.959, Ensemble accuracy on global test data 72.578, Local accuracy on global train data 41.140, Local accuracy on local train data 96.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.227, Central accuracy on global test data 73.203, Ensemble accuracy on global test data 72.334, Local accuracy on global train data 47.241, Local accuracy on local train data 95.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.399, Central accuracy on global test data 73.447, Ensemble accuracy on global test data 71.992, Local accuracy on global train data 40.366, Local accuracy on local train data 95.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.448, Central accuracy on global test data 73.291, Ensemble accuracy on global test data 76.836, Local accuracy on global train data 43.147, Local accuracy on local train data 94.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.478, Central accuracy on global test data 75.049, Ensemble accuracy on global test data 75.566, Local accuracy on global train data 43.640, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.326, Central accuracy on global test data 76.162, Ensemble accuracy on global test data 76.465, Local accuracy on global train data 45.669, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.384, Central accuracy on global test data 80.098, Ensemble accuracy on global test data 80.352, Local accuracy on global train data 44.661, Local accuracy on local train data 95.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.218, Central accuracy on global test data 81.982, Ensemble accuracy on global test data 80.762, Local accuracy on global train data 53.325, Local accuracy on local train data 96.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.367, Central accuracy on global test data 81.465, Ensemble accuracy on global test data 81.045, Local accuracy on global train data 50.232, Local accuracy on local train data 95.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.327, Central accuracy on global test data 83.389, Ensemble accuracy on global test data 83.359, Local accuracy on global train data 44.736, Local accuracy on local train data 95.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.366, Central accuracy on global test data 82.012, Ensemble accuracy on global test data 82.197, Local accuracy on global train data 56.067, Local accuracy on local train data 94.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.224, Central accuracy on global test data 82.480, Ensemble accuracy on global test data 81.152, Local accuracy on global train data 54.285, Local accuracy on local train data 96.233


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.7509, device='cuda:0', grad_fn=<MaxBackward1>)
0 79 {0, 8}
1 83 {9, 2}
2 22 {0, 5}
3 69 {2, 3}
4 1 {8, 4}
5 54 {1, 6}
6 62 {0, 7}
7 36 {1, 3}
8 58 {9, 4}
9 63 {4, 5}
Round  16, Devices participated 10, Average loss 0.287, Central accuracy on global test data 83.027, Ensemble accuracy on global test data 84.795, Local accuracy on global train data 50.479, Local accuracy on local train data 95.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 68 {9, 6}
2 4 {8, 7}
3 10 {3, 7}
4 33 {5, 6}
5 96 {1, 3}
6 15 {5, 7}
7 64 {1, 2}
8 89 {0, 9}
9 54 {1, 6}
Round  17, Devices participated 10, Average loss 0.180, Central accuracy on global test data 80.674, Ensemble accuracy on global test data 84.238, Local accuracy on global train data 55.215, Local accuracy on local train data 96.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {1, 3}
1 22 {0, 5}
2 1 {8, 4}
3 40 {3, 5}
4 41 {0, 3}
5 52 {8, 7}
6 23 {1, 5}
7 75 {8, 3}
8 62 {0, 7}
9 11 {4, 5}
Round  18, Devices participated 10, Average loss 0.261, Central accuracy on global test data 77.852, Ensemble accuracy on global test data 79.248, Local accuracy on global train data 55.652, Local accuracy on local train data 94.983


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 27 {2, 5}
2 8 {8, 7}
3 34 {3, 7}
4 98 {4}
5 74 {5}
6 36 {1, 3}
7 78 {2, 3, 4}
8 85 {1, 4}
9 88 {0}
Round  19, Devices participated 10, Average loss 0.155, Central accuracy on global test data 74.824, Ensemble accuracy on global test data 78.828, Local accuracy on global train data 50.474, Local accuracy on local train data 97.100


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 2 {9}
2 83 {9, 2}
3 32 {0, 9}
4 78 {2, 3, 4}
5 39 {8, 0}
6 25 {8, 2}
7 56 {9, 2}
8 76 {1, 3}
9 35 {8, 2}
Round  20, Devices participated 10, Average loss 0.202, Central accuracy on global test data 66.230, Ensemble accuracy on global test data 68.291, Local accuracy on global train data 44.590, Local accuracy on local train data 95.767


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 2 {9}
2 40 {3, 5}
3 64 {1, 2}
4 12 {8, 9, 3}
5 94 {0, 1}
6 60 {2}
7 24 {4, 5}
8 61 {6}
9 29 {9}
Round  21, Devices participated 10, Average loss 0.160, Central accuracy on global test data 68.652, Ensemble accuracy on global test data 72.090, Local accuracy on global train data 40.142, Local accuracy on local train data 96.433


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 26 {9, 7}
2 80 {6}
3 38 {2, 7}
4 85 {1, 4}
5 86 {1, 2}
6 11 {4, 5}
7 5 {0, 6, 7}
8 98 {4}
9 96 {1, 3}
Round  22, Devices participated 10, Average loss 0.145, Central accuracy on global test data 67.910, Ensemble accuracy on global test data 73.350, Local accuracy on global train data 46.560, Local accuracy on local train data 96.200


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 41 {0, 3}
1 36 {1, 3}
2 92 {1, 3}
3 48 {3, 4}
4 96 {1, 3}
5 76 {1, 3}
6 77 {8, 1}
7 57 {4, 6}
8 78 {2, 3, 4}
9 28 {2, 7}
Round  23, Devices participated 10, Average loss 0.211, Central accuracy on global test data 56.221, Ensemble accuracy on global test data 59.043, Local accuracy on global train data 42.454, Local accuracy on local train data 95.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 65 {3, 4}
2 87 {8, 9}
3 31 {0, 1}
4 89 {0, 9}
5 10 {3, 7}
6 70 {8, 5}
7 44 {6, 7}
8 28 {2, 7}
9 41 {0, 3}
Round  24, Devices participated 10, Average loss 0.186, Central accuracy on global test data 65.342, Ensemble accuracy on global test data 69.541, Local accuracy on global train data 35.996, Local accuracy on local train data 95.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 18 {0, 6}
2 59 {0, 7}
3 53 {0, 7}
4 99 {8, 9}
5 72 {1, 9}
6 90 {8, 6, 7}
7 14 {9, 5}
8 13 {8, 5, 6}
9 80 {6}
Round  25, Devices participated 10, Average loss 0.162, Central accuracy on global test data 60.850, Ensemble accuracy on global test data 69.346, Local accuracy on global train data 36.875, Local accuracy on local train data 96.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 61 {6}
2 33 {5, 6}
3 12 {8, 9, 3}
4 87 {8, 9}
5 20 {2, 7}
6 3 {8, 6}
7 34 {3, 7}
8 9 {1}
9 63 {4, 5}
Round  26, Devices participated 10, Average loss 0.169, Central accuracy on global test data 61.514, Ensemble accuracy on global test data 68.789, Local accuracy on global train data 34.878, Local accuracy on local train data 95.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 31 {0, 1}
2 28 {2, 7}
3 90 {8, 6, 7}
4 19 {5, 6}
5 26 {9, 7}
6 73 {0, 6}
7 23 {1, 5}
8 11 {4, 5}
9 96 {1, 3}
Round  27, Devices participated 10, Average loss 0.172, Central accuracy on global test data 61.504, Ensemble accuracy on global test data 70.068, Local accuracy on global train data 38.162, Local accuracy on local train data 95.400


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 55 {4, 7}
2 1 {8, 4}
3 5 {0, 6, 7}
4 93 {9, 6}
5 53 {0, 7}
6 7 {0, 1}
7 96 {1, 3}
8 57 {4, 6}
9 64 {1, 2}
Round  28, Devices participated 10, Average loss 0.134, Central accuracy on global test data 64.365, Ensemble accuracy on global test data 72.588, Local accuracy on global train data 38.743, Local accuracy on local train data 96.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 46 {9, 6}
2 99 {8, 9}
3 78 {2, 3, 4}
4 33 {5, 6}
5 79 {0, 8}
6 63 {4, 5}
7 13 {8, 5, 6}
8 26 {9, 7}
9 10 {3, 7}
Round  29, Devices participated 10, Average loss 0.216, Central accuracy on global test data 65.811, Ensemble accuracy on global test data 66.592, Local accuracy on global train data 33.628, Local accuracy on local train data 94.017


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 21 {2, 5}
2 40 {3, 5}
3 74 {5}
4 49 {2, 7}
5 36 {1, 3}
6 1 {8, 4}
7 24 {4, 5}
8 71 {1, 5}
9 79 {0, 8}
Round  30, Devices participated 10, Average loss 0.146, Central accuracy on global test data 59.150, Ensemble accuracy on global test data 61.064, Local accuracy on global train data 29.441, Local accuracy on local train data 95.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 68 {9, 6}
2 43 {8, 3}
3 85 {1, 4}
4 10 {3, 7}
5 2 {9}
6 83 {9, 2}
7 94 {0, 1}
8 0 {1, 2}
9 66 {1, 3}
Round  31, Devices participated 10, Average loss 0.150, Central accuracy on global test data 56.514, Ensemble accuracy on global test data 58.789, Local accuracy on global train data 38.049, Local accuracy on local train data 95.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 16 {0, 3}
2 32 {0, 9}
3 76 {1, 3}
4 4 {8, 7}
5 51 {4, 7}
6 92 {1, 3}
7 33 {5, 6}
8 27 {2, 5}
9 86 {1, 2}
Round  32, Devices participated 10, Average loss 0.129, Central accuracy on global test data 60.537, Ensemble accuracy on global test data 64.658, Local accuracy on global train data 33.179, Local accuracy on local train data 96.333


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 40 {3, 5}
2 15 {5, 7}
3 16 {0, 3}
4 66 {1, 3}
5 10 {3, 7}
6 87 {8, 9}
7 1 {8, 4}
8 2 {9}
9 41 {0, 3}
Round  33, Devices participated 10, Average loss 0.155, Central accuracy on global test data 47.607, Ensemble accuracy on global test data 48.369, Local accuracy on global train data 29.116, Local accuracy on local train data 95.600


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 15 {5, 7}
2 38 {2, 7}
3 44 {6, 7}
4 88 {0}
5 56 {9, 2}
6 19 {5, 6}
7 71 {1, 5}
8 29 {9}
9 61 {6}
Round  34, Devices participated 10, Average loss 0.097, Central accuracy on global test data 60.850, Ensemble accuracy on global test data 67.822, Local accuracy on global train data 31.738, Local accuracy on local train data 97.100


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 95 {3, 4}
1 67 {8, 1}
2 46 {9, 6}
3 57 {4, 6}
4 1 {8, 4}
5 27 {2, 5}
6 48 {3, 4}
7 44 {6, 7}
8 16 {0, 3}
9 87 {8, 9}
Round  35, Devices participated 10, Average loss 0.139, Central accuracy on global test data 62.021, Ensemble accuracy on global test data 62.119, Local accuracy on global train data 30.967, Local accuracy on local train data 95.950


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 0 {1, 2}
2 40 {3, 5}
3 32 {0, 9}
4 39 {8, 0}
5 76 {1, 3}
6 73 {0, 6}
7 79 {0, 8}
8 42 {8, 1}
9 24 {4, 5}
Round  36, Devices participated 10, Average loss 0.143, Central accuracy on global test data 68.145, Ensemble accuracy on global test data 67.754, Local accuracy on global train data 35.955, Local accuracy on local train data 95.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 91 {2, 6}
2 93 {9, 6}
3 36 {1, 3}
4 92 {1, 3}
5 58 {9, 4}
6 73 {0, 6}
7 77 {8, 1}
8 13 {8, 5, 6}
9 35 {8, 2}
Round  37, Devices participated 10, Average loss 0.172, Central accuracy on global test data 64.053, Ensemble accuracy on global test data 68.584, Local accuracy on global train data 38.484, Local accuracy on local train data 94.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 10 {3, 7}
2 35 {8, 2}
3 76 {1, 3}
4 90 {8, 6, 7}
5 41 {0, 3}
6 60 {2}
7 97 {4, 7}
8 19 {5, 6}
9 92 {1, 3}
Round  38, Devices participated 10, Average loss 0.136, Central accuracy on global test data 44.033, Ensemble accuracy on global test data 66.133, Local accuracy on global train data 30.723, Local accuracy on local train data 95.467


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 56 {9, 2}
2 50 {2, 3}
3 98 {4}
4 88 {0}
5 2 {9}
6 39 {8, 0}
7 22 {0, 5}
8 30 {0, 1, 2}
9 23 {1, 5}
Round  39, Devices participated 10, Average loss 0.122, Central accuracy on global test data 56.768, Ensemble accuracy on global test data 54.473, Local accuracy on global train data 25.178, Local accuracy on local train data 96.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 21 {2, 5}
2 87 {8, 9}
3 35 {8, 2}
4 34 {3, 7}
5 5 {0, 6, 7}
6 89 {0, 9}
7 44 {6, 7}
8 17 {3, 4}
9 65 {3, 4}
Round  40, Devices participated 10, Average loss 0.147, Central accuracy on global test data 39.521, Ensemble accuracy on global test data 63.301, Local accuracy on global train data 28.472, Local accuracy on local train data 95.567


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 42 {8, 1}
2 5 {0, 6, 7}
3 15 {5, 7}
4 71 {1, 5}
5 28 {2, 7}
6 12 {8, 9, 3}
7 22 {0, 5}
8 35 {8, 2}
9 14 {9, 5}
Round  41, Devices participated 10, Average loss 0.153, Central accuracy on global test data 38.340, Ensemble accuracy on global test data 53.271, Local accuracy on global train data 25.786, Local accuracy on local train data 95.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 45 {4, 5}
2 55 {4, 7}
3 25 {8, 2}
4 38 {2, 7}
5 91 {2, 6}
6 57 {4, 6}
7 61 {6}
8 83 {9, 2}
9 53 {0, 7}
Round  42, Devices participated 10, Average loss 0.145, Central accuracy on global test data 32.939, Ensemble accuracy on global test data 44.043, Local accuracy on global train data 19.539, Local accuracy on local train data 95.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 42 {8, 1}
2 89 {0, 9}
3 35 {8, 2}
4 69 {2, 3}
5 36 {1, 3}
6 74 {5}
7 53 {0, 7}
8 11 {4, 5}
9 68 {9, 6}
Round  43, Devices participated 10, Average loss 0.128, Central accuracy on global test data 34.014, Ensemble accuracy on global test data 49.541, Local accuracy on global train data 18.860, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 75 {8, 3}
2 26 {9, 7}
3 31 {0, 1}
4 47 {9, 5}
5 13 {8, 5, 6}
6 36 {1, 3}
7 79 {0, 8}
8 60 {2}
9 12 {8, 9, 3}
Round  44, Devices participated 10, Average loss 0.212, Central accuracy on global test data 33.418, Ensemble accuracy on global test data 51.367, Local accuracy on global train data 21.191, Local accuracy on local train data 93.233


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 91 {2, 6}
2 12 {8, 9, 3}
3 7 {0, 1}
4 9 {1}
5 23 {1, 5}
6 51 {4, 7}
7 6 {1, 6}
8 43 {8, 3}
9 44 {6, 7}
Round  45, Devices participated 10, Average loss 0.189, Central accuracy on global test data 35.996, Ensemble accuracy on global test data 34.033, Local accuracy on global train data 21.316, Local accuracy on local train data 94.733


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 78 {2, 3, 4}
2 82 {8, 4}
3 68 {9, 6}
4 92 {1, 3}
5 30 {0, 1, 2}
6 12 {8, 9, 3}
7 58 {9, 4}
8 89 {0, 9}
9 44 {6, 7}
Round  46, Devices participated 10, Average loss 0.220, Central accuracy on global test data 52.197, Ensemble accuracy on global test data 57.666, Local accuracy on global train data 26.155, Local accuracy on local train data 93.450


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 65 {3, 4}
1 80 {6}
2 62 {0, 7}
3 19 {5, 6}
4 51 {4, 7}
5 92 {1, 3}
6 71 {1, 5}
7 48 {3, 4}
8 6 {1, 6}
9 60 {2}
Round  47, Devices participated 10, Average loss 0.125, Central accuracy on global test data 52.783, Ensemble accuracy on global test data 58.330, Local accuracy on global train data 25.039, Local accuracy on local train data 96.633


{'layer_input.weight': 0.4444444444444444, 'layer_input.bias': 0.4444444444444444, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 26 {9, 7}
2 5 {0, 6, 7}
3 88 {0}
4 83 {9, 2}
5 62 {0, 7}
6 75 {8, 3}
7 80 {6}
8 87 {8, 9}
9 36 {1, 3}
Round  48, Devices participated 10, Average loss 0.137, Central accuracy on global test data 69.062, Ensemble accuracy on global test data 72.158, Local accuracy on global train data 27.231, Local accuracy on local train data 95.517


{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 16 {0, 3}
1 93 {9, 6}
2 17 {3, 4}
3 77 {8, 1}
4 88 {0}
5 91 {2, 6}
6 73 {0, 6}
7 76 {1, 3}
8 31 {0, 1}
9 47 {9, 5}
Round  49, Devices participated 10, Average loss 0.090, Central accuracy on global test data 61.709, Ensemble accuracy on global test data 68.779, Local accuracy on global train data 29.692, Local accuracy on local train data 97.083


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 5 {0, 6, 7}
2 24 {4, 5}
3 47 {9, 5}
4 98 {4}
5 15 {5, 7}
6 55 {4, 7}
7 58 {9, 4}
8 95 {3, 4}
9 32 {0, 9}
Round  50, Devices participated 10, Average loss 0.101, Central accuracy on global test data 62.451, Ensemble accuracy on global test data 64.854, Local accuracy on global train data 31.934, Local accuracy on local train data 96.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 11 {4, 5}
1 80 {6}
2 87 {8, 9}
3 70 {8, 5}
4 6 {1, 6}
5 91 {2, 6}
6 51 {4, 7}
7 89 {0, 9}
8 42 {8, 1}
9 62 {0, 7}
Round  51, Devices participated 10, Average loss 0.109, Central accuracy on global test data 64.736, Ensemble accuracy on global test data 75.449, Local accuracy on global train data 33.086, Local accuracy on local train data 96.283


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 11 {4, 5}
2 16 {0, 3}
3 81 {2, 3, 7}
4 13 {8, 5, 6}
5 25 {8, 2}
6 33 {5, 6}
7 92 {1, 3}
8 41 {0, 3}
9 48 {3, 4}
Round  52, Devices participated 10, Average loss 0.140, Central accuracy on global test data 53.398, Ensemble accuracy on global test data 64.385, Local accuracy on global train data 27.585, Local accuracy on local train data 95.600


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 99 {8, 9}
2 22 {0, 5}
3 33 {5, 6}
4 7 {0, 1}
5 8 {8, 7}
6 49 {2, 7}
7 23 {1, 5}
8 79 {0, 8}
9 15 {5, 7}
Round  53, Devices participated 10, Average loss 0.116, Central accuracy on global test data 58.984, Ensemble accuracy on global test data 58.174, Local accuracy on global train data 29.036, Local accuracy on local train data 96.483


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 46 {9, 6}
2 23 {1, 5}
3 84 {0, 9}
4 27 {2, 5}
5 4 {8, 7}
6 32 {0, 9}
7 64 {1, 2}
8 44 {6, 7}
9 79 {0, 8}
Round  54, Devices participated 10, Average loss 0.097, Central accuracy on global test data 64.746, Ensemble accuracy on global test data 63.574, Local accuracy on global train data 32.078, Local accuracy on local train data 97.167


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 55 {4, 7}
2 23 {1, 5}
3 56 {9, 2}
4 21 {2, 5}
5 52 {8, 7}
6 17 {3, 4}
7 57 {4, 6}
8 99 {8, 9}
9 71 {1, 5}
Round  55, Devices participated 10, Average loss 0.107, Central accuracy on global test data 76.455, Ensemble accuracy on global test data 76.299, Local accuracy on global train data 33.376, Local accuracy on local train data 96.417


^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 140, in <module>
    w, loss, acc_ll = local_user[user_idx].train()
  File "/workspace/deepedge/models/Update.py", line 55, in train
    nn_outputs = self.net(images)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/workspace/deepedge/models/Nets.py", line 33, in forward
    x = self.layer_hidden1(x)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1370, in linear
    ret = torch.addmm(bias, input, weight.t())
KeyboardInterrupt
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5[A[A[K

[K

[K[A[Aroot@da8c0c2f5553:/workspace/deepedge# mkdir data a/models/testtemp
testtemp/       testtemp_async/ testtemp_hard/  
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# mkdir data/models/testtemp_gen2
]0;root@da8c0c2f5553: /workspace/deepedgeroot@da8c0c2f5553:/workspace/deepedge# mkdir data/models/testtemp_gen2python main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp --alpha_scale 0.5[1@_[1@g[1@e[1@n[1@2
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 22.910, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(13.7080, device='cuda:0', grad_fn=<MaxBackward1>)
tensor(8.8958, device='cuda:0', grad_fn=<MaxBackward1>)
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.363, Central accuracy on global test data 36.855, Ensemble accuracy on global test data 32.412, Local accuracy on global train data 22.612, Local accuracy on local train data 93.383


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.322, Central accuracy on global test data 49.355, Ensemble accuracy on global test data 48.564, Local accuracy on global train data 34.824, Local accuracy on local train data 94.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.401, Central accuracy on global test data 71.924, Ensemble accuracy on global test data 71.387, Local accuracy on global train data 35.190, Local accuracy on local train data 94.533


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.293, Central accuracy on global test data 72.568, Ensemble accuracy on global test data 72.129, Local accuracy on global train data 41.514, Local accuracy on local train data 96.450


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.1564, device='cuda:0', grad_fn=<MaxBackward1>)
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.379, Central accuracy on global test data 74.912, Ensemble accuracy on global test data 75.039, Local accuracy on global train data 45.059, Local accuracy on local train data 95.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.311, Central accuracy on global test data 75.469, Ensemble accuracy on global test data 75.293, Local accuracy on global train data 44.893, Local accuracy on local train data 95.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.370, Central accuracy on global test data 78.906, Ensemble accuracy on global test data 79.150, Local accuracy on global train data 45.901, Local accuracy on local train data 94.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.270, Central accuracy on global test data 79.961, Ensemble accuracy on global test data 79.639, Local accuracy on global train data 46.731, Local accuracy on local train data 96.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.282, Central accuracy on global test data 80.635, Ensemble accuracy on global test data 80.469, Local accuracy on global train data 51.277, Local accuracy on local train data 95.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.330, Central accuracy on global test data 84.209, Ensemble accuracy on global test data 84.102, Local accuracy on global train data 48.022, Local accuracy on local train data 95.950


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.176, Central accuracy on global test data 81.992, Ensemble accuracy on global test data 81.133, Local accuracy on global train data 57.820, Local accuracy on local train data 97.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.321, Central accuracy on global test data 81.523, Ensemble accuracy on global test data 81.582, Local accuracy on global train data 53.340, Local accuracy on local train data 95.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.323, Central accuracy on global test data 83.613, Ensemble accuracy on global test data 83.535, Local accuracy on global train data 50.256, Local accuracy on local train data 95.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.3478, device='cuda:0', grad_fn=<MaxBackward1>)
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.317, Central accuracy on global test data 83.838, Ensemble accuracy on global test data 84.424, Local accuracy on global train data 59.172, Local accuracy on local train data 95.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.245, Central accuracy on global test data 82.295, Ensemble accuracy on global test data 81.807, Local accuracy on global train data 55.105, Local accuracy on local train data 96.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 83 {9, 2}
2 22 {0, 5}
3 69 {2, 3}
4 1 {8, 4}
5 54 {1, 6}
6 62 {0, 7}
7 36 {1, 3}
8 58 {9, 4}
9 63 {4, 5}
Round  16, Devices participated 10, Average loss 0.305, Central accuracy on global test data 85.420, Ensemble accuracy on global test data 85.791, Local accuracy on global train data 52.727, Local accuracy on local train data 95.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 68 {9, 6}
2 4 {8, 7}
3 10 {3, 7}
4 33 {5, 6}
5 96 {1, 3}
6 15 {5, 7}
7 64 {1, 2}
8 89 {0, 9}
9 54 {1, 6}
Round  17, Devices participated 10, Average loss 0.165, Central accuracy on global test data 84.922, Ensemble accuracy on global test data 85.078, Local accuracy on global train data 51.914, Local accuracy on local train data 97.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {1, 3}
1 22 {0, 5}
2 1 {8, 4}
3 40 {3, 5}
4 41 {0, 3}
5 52 {8, 7}
6 23 {1, 5}
7 75 {8, 3}
8 62 {0, 7}
9 11 {4, 5}
Round  18, Devices participated 10, Average loss 0.225, Central accuracy on global test data 81.924, Ensemble accuracy on global test data 80.918, Local accuracy on global train data 56.628, Local accuracy on local train data 95.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 27 {2, 5}
2 8 {8, 7}
3 34 {3, 7}
4 98 {4}
5 74 {5}
6 36 {1, 3}
7 78 {2, 3, 4}
8 85 {1, 4}
9 88 {0}
Round  19, Devices participated 10, Average loss 0.135, Central accuracy on global test data 84.092, Ensemble accuracy on global test data 84.014, Local accuracy on global train data 52.974, Local accuracy on local train data 97.600


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 2 {9}
2 83 {9, 2}
3 32 {0, 9}
4 78 {2, 3, 4}
5 39 {8, 0}
6 25 {8, 2}
7 56 {9, 2}
8 76 {1, 3}
9 35 {8, 2}
Round  20, Devices participated 10, Average loss 0.201, Central accuracy on global test data 78.799, Ensemble accuracy on global test data 78.506, Local accuracy on global train data 52.578, Local accuracy on local train data 96.500


{'layer_input.weight': 0.3333333333333333, 'layer_input.bias': 0.3333333333333333, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 2 {9}
2 40 {3, 5}
3 64 {1, 2}
4 12 {8, 9, 3}
5 94 {0, 1}
6 60 {2}
7 24 {4, 5}
8 61 {6}
9 29 {9}
Round  21, Devices participated 10, Average loss 0.135, Central accuracy on global test data 78.867, Ensemble accuracy on global test data 78.652, Local accuracy on global train data 55.112, Local accuracy on local train data 97.200


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 26 {9, 7}
2 80 {6}
3 38 {2, 7}
4 85 {1, 4}
5 86 {1, 2}
6 11 {4, 5}
7 5 {0, 6, 7}
8 98 {4}
9 96 {1, 3}
Round  22, Devices participated 10, Average loss 0.136, Central accuracy on global test data 84.502, Ensemble accuracy on global test data 84.375, Local accuracy on global train data 60.769, Local accuracy on local train data 97.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 41 {0, 3}
1 36 {1, 3}
2 92 {1, 3}
3 48 {3, 4}
4 96 {1, 3}
5 76 {1, 3}
6 77 {8, 1}
7 57 {4, 6}
8 78 {2, 3, 4}
9 28 {2, 7}
Round  23, Devices participated 10, Average loss 0.187, Central accuracy on global test data 70.254, Ensemble accuracy on global test data 70.713, Local accuracy on global train data 58.418, Local accuracy on local train data 96.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 65 {3, 4}
2 87 {8, 9}
3 31 {0, 1}
4 89 {0, 9}
5 10 {3, 7}
6 70 {8, 5}
7 44 {6, 7}
8 28 {2, 7}
9 41 {0, 3}
Round  24, Devices participated 10, Average loss 0.244, Central accuracy on global test data 81.914, Ensemble accuracy on global test data 82.139, Local accuracy on global train data 49.641, Local accuracy on local train data 96.200


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 18 {0, 6}
2 59 {0, 7}
3 53 {0, 7}
4 99 {8, 9}
5 72 {1, 9}
6 90 {8, 6, 7}
7 14 {9, 5}
8 13 {8, 5, 6}
9 80 {6}
Round  25, Devices participated 10, Average loss 0.264, Central accuracy on global test data 82.383, Ensemble accuracy on global test data 82.754, Local accuracy on global train data 54.290, Local accuracy on local train data 96.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 61 {6}
2 33 {5, 6}
3 12 {8, 9, 3}
4 87 {8, 9}
5 20 {2, 7}
6 3 {8, 6}
7 34 {3, 7}
8 9 {1}
9 63 {4, 5}
Round  26, Devices participated 10, Average loss 0.274, Central accuracy on global test data 85.459, Ensemble accuracy on global test data 85.137, Local accuracy on global train data 57.112, Local accuracy on local train data 96.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 31 {0, 1}
2 28 {2, 7}
3 90 {8, 6, 7}
4 19 {5, 6}
5 26 {9, 7}
6 73 {0, 6}
7 23 {1, 5}
8 11 {4, 5}
9 96 {1, 3}
Round  27, Devices participated 10, Average loss 0.282, Central accuracy on global test data 83.037, Ensemble accuracy on global test data 82.490, Local accuracy on global train data 55.798, Local accuracy on local train data 96.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 55 {4, 7}
2 1 {8, 4}
3 5 {0, 6, 7}
4 93 {9, 6}
5 53 {0, 7}
6 7 {0, 1}
7 96 {1, 3}
8 57 {4, 6}
9 64 {1, 2}
Round  28, Devices participated 10, Average loss 0.193, Central accuracy on global test data 81.953, Ensemble accuracy on global test data 85.342, Local accuracy on global train data 59.966, Local accuracy on local train data 97.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 46 {9, 6}
2 99 {8, 9}
3 78 {2, 3, 4}
4 33 {5, 6}
5 79 {0, 8}
6 63 {4, 5}
7 13 {8, 5, 6}
8 26 {9, 7}
9 10 {3, 7}
Round  29, Devices participated 10, Average loss 0.332, Central accuracy on global test data 86.348, Ensemble accuracy on global test data 86.621, Local accuracy on global train data 51.326, Local accuracy on local train data 95.100


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.3936, device='cuda:0', grad_fn=<MaxBackward1>)
0 80 {6}
1 21 {2, 5}
2 40 {3, 5}
3 74 {5}
4 49 {2, 7}
5 36 {1, 3}
6 1 {8, 4}
7 24 {4, 5}
8 71 {1, 5}
9 79 {0, 8}
Round  30, Devices participated 10, Average loss 0.229, Central accuracy on global test data 82.617, Ensemble accuracy on global test data 80.908, Local accuracy on global train data 45.149, Local accuracy on local train data 96.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 68 {9, 6}
2 43 {8, 3}
3 85 {1, 4}
4 10 {3, 7}
5 2 {9}
6 83 {9, 2}
7 94 {0, 1}
8 0 {1, 2}
9 66 {1, 3}
Round  31, Devices participated 10, Average loss 0.193, Central accuracy on global test data 80.869, Ensemble accuracy on global test data 81.094, Local accuracy on global train data 56.807, Local accuracy on local train data 96.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 16 {0, 3}
2 32 {0, 9}
3 76 {1, 3}
4 4 {8, 7}
5 51 {4, 7}
6 92 {1, 3}
7 33 {5, 6}
8 27 {2, 5}
9 86 {1, 2}
Round  32, Devices participated 10, Average loss 0.202, Central accuracy on global test data 84.512, Ensemble accuracy on global test data 85.010, Local accuracy on global train data 58.401, Local accuracy on local train data 97.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 40 {3, 5}
2 15 {5, 7}
3 16 {0, 3}
4 66 {1, 3}
5 10 {3, 7}
6 87 {8, 9}
7 1 {8, 4}
8 2 {9}
9 41 {0, 3}
Round  33, Devices participated 10, Average loss 0.203, Central accuracy on global test data 81.787, Ensemble accuracy on global test data 82.051, Local accuracy on global train data 56.382, Local accuracy on local train data 96.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 15 {5, 7}
2 38 {2, 7}
3 44 {6, 7}
4 88 {0}
5 56 {9, 2}
6 19 {5, 6}
7 71 {1, 5}
8 29 {9}
9 61 {6}
Round  34, Devices participated 10, Average loss 0.132, Central accuracy on global test data 83.066, Ensemble accuracy on global test data 82.559, Local accuracy on global train data 53.525, Local accuracy on local train data 98.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 95 {3, 4}
1 67 {8, 1}
2 46 {9, 6}
3 57 {4, 6}
4 1 {8, 4}
5 27 {2, 5}
6 48 {3, 4}
7 44 {6, 7}
8 16 {0, 3}
9 87 {8, 9}
Round  35, Devices participated 10, Average loss 0.211, Central accuracy on global test data 83.564, Ensemble accuracy on global test data 82.744, Local accuracy on global train data 53.833, Local accuracy on local train data 96.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 0 {1, 2}
2 40 {3, 5}
3 32 {0, 9}
4 39 {8, 0}
5 76 {1, 3}
6 73 {0, 6}
7 79 {0, 8}
8 42 {8, 1}
9 24 {4, 5}
Round  36, Devices participated 10, Average loss 0.188, Central accuracy on global test data 85.986, Ensemble accuracy on global test data 86.074, Local accuracy on global train data 60.757, Local accuracy on local train data 96.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 91 {2, 6}
2 93 {9, 6}
3 36 {1, 3}
4 92 {1, 3}
5 58 {9, 4}
6 73 {0, 6}
7 77 {8, 1}
8 13 {8, 5, 6}
9 35 {8, 2}
Round  37, Devices participated 10, Average loss 0.252, Central accuracy on global test data 83.125, Ensemble accuracy on global test data 83.721, Local accuracy on global train data 59.600, Local accuracy on local train data 95.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 10 {3, 7}
2 35 {8, 2}
3 76 {1, 3}
4 90 {8, 6, 7}
5 41 {0, 3}
6 60 {2}
7 97 {4, 7}
8 19 {5, 6}
9 92 {1, 3}
Round  38, Devices participated 10, Average loss 0.177, Central accuracy on global test data 83.096, Ensemble accuracy on global test data 82.705, Local accuracy on global train data 53.730, Local accuracy on local train data 97.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 56 {9, 2}
2 50 {2, 3}
3 98 {4}
4 88 {0}
5 2 {9}
6 39 {8, 0}
7 22 {0, 5}
8 30 {0, 1, 2}
9 23 {1, 5}
Round  39, Devices participated 10, Average loss 0.150, Central accuracy on global test data 86.318, Ensemble accuracy on global test data 85.859, Local accuracy on global train data 57.793, Local accuracy on local train data 97.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 21 {2, 5}
2 87 {8, 9}
3 35 {8, 2}
4 34 {3, 7}
5 5 {0, 6, 7}
6 89 {0, 9}
7 44 {6, 7}
8 17 {3, 4}
9 65 {3, 4}
Round  40, Devices participated 10, Average loss 0.198, Central accuracy on global test data 87.344, Ensemble accuracy on global test data 87.256, Local accuracy on global train data 55.105, Local accuracy on local train data 96.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 42 {8, 1}
2 5 {0, 6, 7}
3 15 {5, 7}
4 71 {1, 5}
5 28 {2, 7}
6 12 {8, 9, 3}
7 22 {0, 5}
8 35 {8, 2}
9 14 {9, 5}
Round  41, Devices participated 10, Average loss 0.183, Central accuracy on global test data 85.566, Ensemble accuracy on global test data 85.078, Local accuracy on global train data 60.574, Local accuracy on local train data 96.617


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 45 {4, 5}
2 55 {4, 7}
3 25 {8, 2}
4 38 {2, 7}
5 91 {2, 6}
6 57 {4, 6}
7 61 {6}
8 83 {9, 2}
9 53 {0, 7}
Round  42, Devices participated 10, Average loss 0.212, Central accuracy on global test data 86.084, Ensemble accuracy on global test data 85.742, Local accuracy on global train data 58.411, Local accuracy on local train data 96.733


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 42 {8, 1}
2 89 {0, 9}
3 35 {8, 2}
4 69 {2, 3}
5 36 {1, 3}
6 74 {5}
7 53 {0, 7}
8 11 {4, 5}
9 68 {9, 6}
Round  43, Devices participated 10, Average loss 0.143, Central accuracy on global test data 86.816, Ensemble accuracy on global test data 86.514, Local accuracy on global train data 50.842, Local accuracy on local train data 97.283


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0647, device='cuda:0', grad_fn=<MaxBackward1>)
0 52 {8, 7}
1 75 {8, 3}
2 26 {9, 7}
3 31 {0, 1}
4 47 {9, 5}
5 13 {8, 5, 6}
6 36 {1, 3}
7 79 {0, 8}
8 60 {2}
9 12 {8, 9, 3}
Round  44, Devices participated 10, Average loss 0.231, Central accuracy on global test data 84.375, Ensemble accuracy on global test data 84.385, Local accuracy on global train data 57.046, Local accuracy on local train data 95.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 91 {2, 6}
2 12 {8, 9, 3}
3 7 {0, 1}
4 9 {1}
5 23 {1, 5}
6 51 {4, 7}
7 6 {1, 6}
8 43 {8, 3}
9 44 {6, 7}
Round  45, Devices participated 10, Average loss 0.156, Central accuracy on global test data 85.391, Ensemble accuracy on global test data 85.371, Local accuracy on global train data 62.910, Local accuracy on local train data 96.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 78 {2, 3, 4}
2 82 {8, 4}
3 68 {9, 6}
4 92 {1, 3}
5 30 {0, 1, 2}
6 12 {8, 9, 3}
7 58 {9, 4}
8 89 {0, 9}
9 44 {6, 7}
Round  46, Devices participated 10, Average loss 0.197, Central accuracy on global test data 85.098, Ensemble accuracy on global test data 85.459, Local accuracy on global train data 63.286, Local accuracy on local train data 96.067


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 65 {3, 4}
1 80 {6}
2 62 {0, 7}
3 19 {5, 6}
4 51 {4, 7}
5 92 {1, 3}
6 71 {1, 5}
7 48 {3, 4}
8 6 {1, 6}
9 60 {2}
Round  47, Devices participated 10, Average loss 0.127, Central accuracy on global test data 84.424, Ensemble accuracy on global test data 84.053, Local accuracy on global train data 57.126, Local accuracy on local train data 97.900


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 26 {9, 7}
2 5 {0, 6, 7}
3 88 {0}
4 83 {9, 2}
5 62 {0, 7}
6 75 {8, 3}
7 80 {6}
8 87 {8, 9}
9 36 {1, 3}
Round  48, Devices participated 10, Average loss 0.179, Central accuracy on global test data 85.254, Ensemble accuracy on global test data 85.342, Local accuracy on global train data 63.503, Local accuracy on local train data 96.150


{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 16 {0, 3}
1 93 {9, 6}
2 17 {3, 4}
3 77 {8, 1}
4 88 {0}
5 91 {2, 6}
6 73 {0, 6}
7 76 {1, 3}
8 31 {0, 1}
9 47 {9, 5}
Round  49, Devices participated 10, Average loss 0.119, Central accuracy on global test data 81.758, Ensemble accuracy on global test data 81.475, Local accuracy on global train data 61.826, Local accuracy on local train data 97.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 5 {0, 6, 7}
2 24 {4, 5}
3 47 {9, 5}
4 98 {4}
5 15 {5, 7}
6 55 {4, 7}
7 58 {9, 4}
8 95 {3, 4}
9 32 {0, 9}
Round  50, Devices participated 10, Average loss 0.143, Central accuracy on global test data 84.482, Ensemble accuracy on global test data 84.326, Local accuracy on global train data 61.660, Local accuracy on local train data 96.767


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 11 {4, 5}
1 80 {6}
2 87 {8, 9}
3 70 {8, 5}
4 6 {1, 6}
5 91 {2, 6}
6 51 {4, 7}
7 89 {0, 9}
8 42 {8, 1}
9 62 {0, 7}
Round  51, Devices participated 10, Average loss 0.130, Central accuracy on global test data 87.178, Ensemble accuracy on global test data 87.402, Local accuracy on global train data 62.422, Local accuracy on local train data 97.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 11 {4, 5}
2 16 {0, 3}
3 81 {2, 3, 7}
4 13 {8, 5, 6}
5 25 {8, 2}
6 33 {5, 6}
7 92 {1, 3}
8 41 {0, 3}
9 48 {3, 4}
Round  52, Devices participated 10, Average loss 0.162, Central accuracy on global test data 84.092, Ensemble accuracy on global test data 83.750, Local accuracy on global train data 57.734, Local accuracy on local train data 96.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 99 {8, 9}
2 22 {0, 5}
3 33 {5, 6}
4 7 {0, 1}
5 8 {8, 7}
6 49 {2, 7}
7 23 {1, 5}
8 79 {0, 8}
9 15 {5, 7}
Round  53, Devices participated 10, Average loss 0.138, Central accuracy on global test data 83.066, Ensemble accuracy on global test data 82.754, Local accuracy on global train data 55.198, Local accuracy on local train data 97.233


{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 46 {9, 6}
2 23 {1, 5}
3 84 {0, 9}
4 27 {2, 5}
5 4 {8, 7}
6 32 {0, 9}
7 64 {1, 2}
8 44 {6, 7}
9 79 {0, 8}
Round  54, Devices participated 10, Average loss 0.095, Central accuracy on global test data 83.301, Ensemble accuracy on global test data 83.271, Local accuracy on global train data 55.283, Local accuracy on local train data 98.017


{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.1536, device='cuda:0', grad_fn=<MaxBackward1>)
0 84 {0, 9}
1 55 {4, 7}
2 23 {1, 5}
3 56 {9, 2}
4 21 {2, 5}
5 52 {8, 7}
6 17 {3, 4}
7 57 {4, 6}
8 99 {8, 9}
9 71 {1, 5}
Round  55, Devices participated 10, Average loss 0.110, Central accuracy on global test data 86.699, Ensemble accuracy on global test data 86.348, Local accuracy on global train data 57.896, Local accuracy on local train data 97.483


{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 43 {8, 3}
2 85 {1, 4}
3 22 {0, 5}
4 84 {0, 9}
5 0 {1, 2}
6 78 {2, 3, 4}
7 24 {4, 5}
8 65 {3, 4}
9 44 {6, 7}
Round  56, Devices participated 10, Average loss 0.134, Central accuracy on global test data 86.592, Ensemble accuracy on global test data 86.641, Local accuracy on global train data 63.052, Local accuracy on local train data 96.917


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 49 {2, 7}
2 0 {1, 2}
3 98 {4}
4 24 {4, 5}
5 42 {8, 1}
6 31 {0, 1}
7 57 {4, 6}
8 67 {8, 1}
9 19 {5, 6}
Round  57, Devices participated 10, Average loss 0.097, Central accuracy on global test data 84.434, Ensemble accuracy on global test data 84.727, Local accuracy on global train data 59.651, Local accuracy on local train data 97.917


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 50 {2, 3}
2 82 {8, 4}
3 54 {1, 6}
4 60 {2}
5 74 {5}
6 75 {8, 3}
7 26 {9, 7}
8 7 {0, 1}
9 1 {8, 4}
Round  58, Devices participated 10, Average loss 0.107, Central accuracy on global test data 86.152, Ensemble accuracy on global test data 85.977, Local accuracy on global train data 58.521, Local accuracy on local train data 96.817


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 79 {0, 8}
2 92 {1, 3}
3 43 {8, 3}
4 17 {3, 4}
5 66 {1, 3}
6 19 {5, 6}
7 51 {4, 7}
8 46 {9, 6}
9 32 {0, 9}
Round  59, Devices participated 10, Average loss 0.113, Central accuracy on global test data 84.248, Ensemble accuracy on global test data 84.316, Local accuracy on global train data 62.976, Local accuracy on local train data 97.233


{'layer_input.weight': 0.6666666666666666, 'layer_input.bias': 0.6666666666666666, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 19 {5, 6}
2 42 {8, 1}
3 21 {2, 5}
4 15 {5, 7}
5 50 {2, 3}
6 35 {8, 2}
7 81 {2, 3, 7}
8 65 {3, 4}
9 95 {3, 4}
Round  60, Devices participated 10, Average loss 0.122, Central accuracy on global test data 86.045, Ensemble accuracy on global test data 85.928, Local accuracy on global train data 57.869, Local accuracy on local train data 96.917


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 29 {9}
2 19 {5, 6}
3 60 {2}
4 35 {8, 2}
5 25 {8, 2}
6 93 {9, 6}
7 24 {4, 5}
8 18 {0, 6}
9 85 {1, 4}
Round  61, Devices participated 10, Average loss 0.085, Central accuracy on global test data 85.840, Ensemble accuracy on global test data 85.859, Local accuracy on global train data 62.097, Local accuracy on local train data 97.900


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 20 {2, 7}
1 78 {2, 3, 4}
2 96 {1, 3}
3 8 {8, 7}
4 39 {8, 0}
5 71 {1, 5}
6 28 {2, 7}
7 9 {1}
8 32 {0, 9}
9 29 {9}
Round  62, Devices participated 10, Average loss 0.093, Central accuracy on global test data 86.582, Ensemble accuracy on global test data 87.168, Local accuracy on global train data 56.033, Local accuracy on local train data 97.433


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 82 {8, 4}
1 17 {3, 4}
2 75 {8, 3}
3 68 {9, 6}
4 15 {5, 7}
5 33 {5, 6}
6 22 {0, 5}
7 23 {1, 5}
8 38 {2, 7}
9 90 {8, 6, 7}
Round  63, Devices participated 10, Average loss 0.083, Central accuracy on global test data 85.811, Ensemble accuracy on global test data 85.400, Local accuracy on global train data 59.734, Local accuracy on local train data 97.517


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {8, 9}
1 59 {0, 7}
2 39 {8, 0}
3 9 {1}
4 35 {8, 2}
5 73 {0, 6}
6 30 {0, 1, 2}
7 75 {8, 3}
8 60 {2}
9 4 {8, 7}
Round  64, Devices participated 10, Average loss 0.087, Central accuracy on global test data 86.318, Ensemble accuracy on global test data 85.273, Local accuracy on global train data 60.657, Local accuracy on local train data 97.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {3, 5}
1 97 {4, 7}
2 6 {1, 6}
3 12 {8, 9, 3}
4 13 {8, 5, 6}
5 98 {4}
6 70 {8, 5}
7 9 {1}
8 5 {0, 6, 7}
9 77 {8, 1}
Round  65, Devices participated 10, Average loss 0.098, Central accuracy on global test data 86.982, Ensemble accuracy on global test data 85.273, Local accuracy on global train data 68.076, Local accuracy on local train data 97.200


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(9.5748, device='cuda:0', grad_fn=<MaxBackward1>)
0 30 {0, 1, 2}
1 93 {9, 6}
2 88 {0}
3 16 {0, 3}
4 90 {8, 6, 7}
5 26 {9, 7}
6 49 {2, 7}
7 44 {6, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round  66, Devices participated 10, Average loss 0.076, Central accuracy on global test data 86.348, Ensemble accuracy on global test data 86.162, Local accuracy on global train data 61.824, Local accuracy on local train data 97.583


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {2, 3, 4}
1 11 {4, 5}
2 52 {8, 7}
3 82 {8, 4}
4 7 {0, 1}
5 62 {0, 7}
6 38 {2, 7}
7 41 {0, 3}
8 46 {9, 6}
9 13 {8, 5, 6}
Round  67, Devices participated 10, Average loss 0.061, Central accuracy on global test data 86.465, Ensemble accuracy on global test data 85.781, Local accuracy on global train data 59.248, Local accuracy on local train data 98.233


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {9, 2}
1 85 {1, 4}
2 24 {4, 5}
3 89 {0, 9}
4 44 {6, 7}
5 18 {0, 6}
6 97 {4, 7}
7 15 {5, 7}
8 6 {1, 6}
9 3 {8, 6}
Round  68, Devices participated 10, Average loss 0.046, Central accuracy on global test data 87.100, Ensemble accuracy on global test data 87.148, Local accuracy on global train data 64.583, Local accuracy on local train data 98.817


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {9, 5}
1 19 {5, 6}
2 62 {0, 7}
3 61 {6}
4 60 {2}
5 2 {9}
6 87 {8, 9}
7 16 {0, 3}
8 7 {0, 1}
9 89 {0, 9}
Round  69, Devices participated 10, Average loss 0.062, Central accuracy on global test data 83.877, Ensemble accuracy on global test data 83.301, Local accuracy on global train data 59.084, Local accuracy on local train data 98.200


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 70 {8, 5}
3 26 {9, 7}
4 48 {3, 4}
5 76 {1, 3}
6 97 {4, 7}
7 71 {1, 5}
8 59 {0, 7}
9 30 {0, 1, 2}
Round  70, Devices participated 10, Average loss 0.081, Central accuracy on global test data 87.871, Ensemble accuracy on global test data 87.939, Local accuracy on global train data 60.332, Local accuracy on local train data 97.483


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 49 {2, 7}
1 2 {9}
2 29 {9}
3 96 {1, 3}
4 71 {1, 5}
5 18 {0, 6}
6 88 {0}
7 40 {3, 5}
8 59 {0, 7}
9 65 {3, 4}
Round  71, Devices participated 10, Average loss 0.055, Central accuracy on global test data 86.211, Ensemble accuracy on global test data 85.410, Local accuracy on global train data 59.836, Local accuracy on local train data 98.133


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 31 {0, 1}
2 50 {2, 3}
3 38 {2, 7}
4 33 {5, 6}
5 51 {4, 7}
6 91 {2, 6}
7 68 {9, 6}
8 81 {2, 3, 7}
9 30 {0, 1, 2}
Round  72, Devices participated 10, Average loss 0.068, Central accuracy on global test data 86.680, Ensemble accuracy on global test data 86.484, Local accuracy on global train data 61.111, Local accuracy on local train data 97.967


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 18 {0, 6}
2 65 {3, 4}
3 32 {0, 9}
4 7 {0, 1}
5 92 {1, 3}
6 24 {4, 5}
7 86 {1, 2}
8 53 {0, 7}
9 67 {8, 1}
Round  73, Devices participated 10, Average loss 0.051, Central accuracy on global test data 88.223, Ensemble accuracy on global test data 87.803, Local accuracy on global train data 61.072, Local accuracy on local train data 98.500


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 86 {1, 2}
2 64 {1, 2}
3 57 {4, 6}
4 67 {8, 1}
5 94 {0, 1}
6 3 {8, 6}
7 19 {5, 6}
8 8 {8, 7}
9 69 {2, 3}
Round  74, Devices participated 10, Average loss 0.076, Central accuracy on global test data 88.115, Ensemble accuracy on global test data 87.822, Local accuracy on global train data 56.511, Local accuracy on local train data 97.617


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 26 {9, 7}
2 37 {4, 5, 7}
3 90 {8, 6, 7}
4 47 {9, 5}
5 89 {0, 9}
6 16 {0, 3}
7 65 {3, 4}
8 36 {1, 3}
9 61 {6}
Round  75, Devices participated 10, Average loss 0.076, Central accuracy on global test data 87.227, Ensemble accuracy on global test data 87.373, Local accuracy on global train data 62.705, Local accuracy on local train data 97.833


^[[A^[[B{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
 /workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
       0 27 {2, 5}
1 63 {4, 5}
2 77 {8, 1}
3 16 {0, 3}
4 36 {1, 3}
5 28 {2, 7}
6 59 {0, 7}
7 12 {8, 9, 3}
8 97 {4, 7}
9 55 {4, 7}
Round  76, Devices participated 10, Average loss 0.076, Central accuracy on global test data 86.514, Ensemble accuracy on global test data 85.557, Local accuracy on global train data 59.832, Local accuracy on local train data 97.533


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 55 {4, 7}
2 14 {9, 5}
3 80 {6}
4 74 {5}
5 25 {8, 2}
6 26 {9, 7}
7 57 {4, 6}
8 41 {0, 3}
9 22 {0, 5}
Round  77, Devices participated 10, Average loss 0.064, Central accuracy on global test data 87.480, Ensemble accuracy on global test data 87.520, Local accuracy on global train data 60.371, Local accuracy on local train data 97.833


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 79 {0, 8}
2 86 {1, 2}
3 13 {8, 5, 6}
4 21 {2, 5}
5 32 {0, 9}
6 95 {3, 4}
7 52 {8, 7}
8 27 {2, 5}
9 47 {9, 5}
Round  78, Devices participated 10, Average loss 0.054, Central accuracy on global test data 87.256, Ensemble accuracy on global test data 86.719, Local accuracy on global train data 55.630, Local accuracy on local train data 98.383


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 42 {8, 1}
2 0 {1, 2}
3 39 {8, 0}
4 54 {1, 6}
5 16 {0, 3}
6 33 {5, 6}
7 65 {3, 4}
8 83 {9, 2}
9 5 {0, 6, 7}
Round  79, Devices participated 10, Average loss 0.067, Central accuracy on global test data 88.867, Ensemble accuracy on global test data 88.975, Local accuracy on global train data 59.224, Local accuracy on local train data 97.800


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 19 {5, 6}
2 70 {8, 5}
3 91 {2, 6}
4 10 {3, 7}
5 86 {1, 2}
6 78 {2, 3, 4}
7 81 {2, 3, 7}
8 97 {4, 7}
9 22 {0, 5}
Round  80, Devices participated 10, Average loss 0.076, Central accuracy on global test data 87.656, Ensemble accuracy on global test data 87.383, Local accuracy on global train data 59.348, Local accuracy on local train data 97.483


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 12 {8, 9, 3}
1 79 {0, 8}
2 66 {1, 3}
3 20 {2, 7}
4 2 {9}
5 13 {8, 5, 6}
6 6 {1, 6}
7 19 {5, 6}
8 91 {2, 6}
9 29 {9}
Round  81, Devices participated 10, Average loss 0.065, Central accuracy on global test data 86.436, Ensemble accuracy on global test data 86.406, Local accuracy on global train data 59.565, Local accuracy on local train data 98.133


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 10 {3, 7}
2 21 {2, 5}
3 83 {9, 2}
4 59 {0, 7}
5 3 {8, 6}
6 46 {9, 6}
7 0 {1, 2}
8 16 {0, 3}
9 94 {0, 1}
Round  82, Devices participated 10, Average loss 0.050, Central accuracy on global test data 85.215, Ensemble accuracy on global test data 85.039, Local accuracy on global train data 55.959, Local accuracy on local train data 98.567


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {2, 5}
1 96 {1, 3}
2 36 {1, 3}
3 17 {3, 4}
4 74 {5}
5 46 {9, 6}
6 73 {0, 6}
7 80 {6}
8 20 {2, 7}
9 40 {3, 5}
Round  83, Devices participated 10, Average loss 0.065, Central accuracy on global test data 84.961, Ensemble accuracy on global test data 84.893, Local accuracy on global train data 57.505, Local accuracy on local train data 98.033


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 81 {2, 3, 7}
2 10 {3, 7}
3 91 {2, 6}
4 31 {0, 1}
5 4 {8, 7}
6 42 {8, 1}
7 26 {9, 7}
8 77 {8, 1}
9 99 {8, 9}
Round  84, Devices participated 10, Average loss 0.068, Central accuracy on global test data 86.943, Ensemble accuracy on global test data 86.934, Local accuracy on global train data 60.935, Local accuracy on local train data 97.750


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 78 {2, 3, 4}
2 93 {9, 6}
3 75 {8, 3}
4 31 {0, 1}
5 86 {1, 2}
6 7 {0, 1}
7 17 {3, 4}
8 77 {8, 1}
9 28 {2, 7}
Round  85, Devices participated 10, Average loss 0.062, Central accuracy on global test data 88.164, Ensemble accuracy on global test data 87.939, Local accuracy on global train data 60.911, Local accuracy on local train data 98.050


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 22 {0, 5}
2 44 {6, 7}
3 18 {0, 6}
4 68 {9, 6}
5 34 {3, 7}
6 91 {2, 6}
7 98 {4}
8 84 {0, 9}
9 83 {9, 2}
Round  86, Devices participated 10, Average loss 0.044, Central accuracy on global test data 85.801, Ensemble accuracy on global test data 86.885, Local accuracy on global train data 63.721, Local accuracy on local train data 98.617


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 59 {0, 7}
2 51 {4, 7}
3 42 {8, 1}
4 49 {2, 7}
5 69 {2, 3}
6 52 {8, 7}
7 37 {4, 5, 7}
8 2 {9}
9 4 {8, 7}
Round  87, Devices participated 10, Average loss 0.061, Central accuracy on global test data 85.742, Ensemble accuracy on global test data 86.943, Local accuracy on global train data 57.356, Local accuracy on local train data 97.883


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 10 {3, 7}
2 27 {2, 5}
3 34 {3, 7}
4 7 {0, 1}
5 76 {1, 3}
6 74 {5}
7 3 {8, 6}
8 59 {0, 7}
9 52 {8, 7}
Round  88, Devices participated 10, Average loss 0.042, Central accuracy on global test data 87.607, Ensemble accuracy on global test data 87.197, Local accuracy on global train data 59.119, Local accuracy on local train data 98.767


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {6, 7}
1 9 {1}
2 73 {0, 6}
3 3 {8, 6}
4 55 {4, 7}
5 81 {2, 3, 7}
6 17 {3, 4}
7 59 {0, 7}
8 97 {4, 7}
9 63 {4, 5}
Round  89, Devices participated 10, Average loss 0.047, Central accuracy on global test data 82.568, Ensemble accuracy on global test data 82.178, Local accuracy on global train data 66.084, Local accuracy on local train data 98.367


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 63 {4, 5}
1 0 {1, 2}
2 33 {5, 6}
3 88 {0}
4 93 {9, 6}
5 87 {8, 9}
6 72 {1, 9}
7 84 {0, 9}
8 86 {1, 2}
9 20 {2, 7}
Round  90, Devices participated 10, Average loss 0.056, Central accuracy on global test data 88.115, Ensemble accuracy on global test data 87.939, Local accuracy on global train data 56.440, Local accuracy on local train data 98.167


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 19 {5, 6}
1 95 {3, 4}
2 33 {5, 6}
3 92 {1, 3}
4 65 {3, 4}
5 69 {2, 3}
6 81 {2, 3, 7}
7 26 {9, 7}
8 14 {9, 5}
9 52 {8, 7}
Round  91, Devices participated 10, Average loss 0.092, Central accuracy on global test data 88.350, Ensemble accuracy on global test data 88.008, Local accuracy on global train data 59.441, Local accuracy on local train data 97.117


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 22 {0, 5}
1 62 {0, 7}
2 3 {8, 6}
3 14 {9, 5}
4 64 {1, 2}
5 96 {1, 3}
6 33 {5, 6}
7 97 {4, 7}
8 31 {0, 1}
9 42 {8, 1}
Round  92, Devices participated 10, Average loss 0.051, Central accuracy on global test data 89.727, Ensemble accuracy on global test data 89.648, Local accuracy on global train data 60.486, Local accuracy on local train data 98.300


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 37 {4, 5, 7}
1 10 {3, 7}
2 88 {0}
3 63 {4, 5}
4 53 {0, 7}
5 68 {9, 6}
6 2 {9}
7 52 {8, 7}
8 26 {9, 7}
9 59 {0, 7}
Round  93, Devices participated 10, Average loss 0.049, Central accuracy on global test data 86.992, Ensemble accuracy on global test data 87.090, Local accuracy on global train data 64.131, Local accuracy on local train data 98.533


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 71 {1, 5}
2 85 {1, 4}
3 49 {2, 7}
4 20 {2, 7}
5 10 {3, 7}
6 3 {8, 6}
7 86 {1, 2}
8 88 {0}
9 33 {5, 6}
Round  94, Devices participated 10, Average loss 0.045, Central accuracy on global test data 87.373, Ensemble accuracy on global test data 86.816, Local accuracy on global train data 60.942, Local accuracy on local train data 98.550


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {1}
1 0 {1, 2}
2 18 {0, 6}
3 66 {1, 3}
4 44 {6, 7}
5 17 {3, 4}
6 92 {1, 3}
7 95 {3, 4}
8 64 {1, 2}
9 96 {1, 3}
Round  95, Devices participated 10, Average loss 0.045, Central accuracy on global test data 82.461, Ensemble accuracy on global test data 81.826, Local accuracy on global train data 66.511, Local accuracy on local train data 98.767


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {8, 5}
1 58 {9, 4}
2 24 {4, 5}
3 57 {4, 6}
4 32 {0, 9}
5 19 {5, 6}
6 43 {8, 3}
7 65 {3, 4}
8 5 {0, 6, 7}
9 88 {0}
Round  96, Devices participated 10, Average loss 0.089, Central accuracy on global test data 85.928, Ensemble accuracy on global test data 85.771, Local accuracy on global train data 65.225, Local accuracy on local train data 96.867


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {6}
1 75 {8, 3}
2 68 {9, 6}
3 2 {9}
4 6 {1, 6}
5 26 {9, 7}
6 1 {8, 4}
7 56 {9, 2}
8 78 {2, 3, 4}
9 74 {5}
Round  97, Devices participated 10, Average loss 0.064, Central accuracy on global test data 87.061, Ensemble accuracy on global test data 87.930, Local accuracy on global train data 59.780, Local accuracy on local train data 97.900


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 57 {4, 6}
2 22 {0, 5}
3 90 {8, 6, 7}
4 46 {9, 6}
5 9 {1}
6 56 {9, 2}
7 99 {8, 9}
8 89 {0, 9}
9 4 {8, 7}
Round  98, Devices participated 10, Average loss 0.050, Central accuracy on global test data 84.150, Ensemble accuracy on global test data 84.473, Local accuracy on global train data 60.881, Local accuracy on local train data 98.417


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {9, 2}
1 41 {0, 3}
2 69 {2, 3}
3 7 {0, 1}
4 63 {4, 5}
5 51 {4, 7}
6 78 {2, 3, 4}
7 89 {0, 9}
8 43 {8, 3}
9 81 {2, 3, 7}
Round  99, Devices participated 10, Average loss 0.090, Central accuracy on global test data 87.900, Ensemble accuracy on global test data 88.086, Local accuracy on global train data 64.319, Local accuracy on local train data 97.117


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {5}
1 76 {1, 3}
2 14 {9, 5}
3 98 {4}
4 73 {0, 6}
5 37 {4, 5, 7}
6 94 {0, 1}
7 68 {9, 6}
8 39 {8, 0}
9 11 {4, 5}
Round 100, Devices participated 10, Average loss 0.044, Central accuracy on global test data 88.672, Ensemble accuracy on global test data 88.340, Local accuracy on global train data 62.529, Local accuracy on local train data 98.750


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 38 {2, 7}
1 78 {2, 3, 4}
2 17 {3, 4}
3 56 {9, 2}
4 90 {8, 6, 7}
5 34 {3, 7}
6 42 {8, 1}
7 0 {1, 2}
8 1 {8, 4}
9 12 {8, 9, 3}
Round 101, Devices participated 10, Average loss 0.065, Central accuracy on global test data 89.414, Ensemble accuracy on global test data 89.453, Local accuracy on global train data 58.943, Local accuracy on local train data 97.917


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 36 {1, 3}
2 95 {3, 4}
3 8 {8, 7}
4 99 {8, 9}
5 75 {8, 3}
6 93 {9, 6}
7 4 {8, 7}
8 24 {4, 5}
9 58 {9, 4}
Round 102, Devices participated 10, Average loss 0.078, Central accuracy on global test data 86.592, Ensemble accuracy on global test data 87.139, Local accuracy on global train data 60.149, Local accuracy on local train data 97.583


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 66 {1, 3}
1 46 {9, 6}
2 89 {0, 9}
3 23 {1, 5}
4 4 {8, 7}
5 97 {4, 7}
6 78 {2, 3, 4}
7 26 {9, 7}
8 79 {0, 8}
9 52 {8, 7}
Round 103, Devices participated 10, Average loss 0.061, Central accuracy on global test data 89.414, Ensemble accuracy on global test data 89.648, Local accuracy on global train data 62.356, Local accuracy on local train data 98.300


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 79 {0, 8}
2 23 {1, 5}
3 7 {0, 1}
4 97 {4, 7}
5 86 {1, 2}
6 72 {1, 9}
7 57 {4, 6}
8 70 {8, 5}
9 64 {1, 2}
Round 104, Devices participated 10, Average loss 0.047, Central accuracy on global test data 88.789, Ensemble accuracy on global test data 88.340, Local accuracy on global train data 65.107, Local accuracy on local train data 98.483


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 55 {4, 7}
1 14 {9, 5}
2 21 {2, 5}
3 43 {8, 3}
4 5 {0, 6, 7}
5 30 {0, 1, 2}
6 35 {8, 2}
7 15 {5, 7}
8 28 {2, 7}
9 34 {3, 7}
Round 105, Devices participated 10, Average loss 0.064, Central accuracy on global test data 89.141, Ensemble accuracy on global test data 88.340, Local accuracy on global train data 60.649, Local accuracy on local train data 97.767


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 51 {4, 7}
2 22 {0, 5}
3 85 {1, 4}
4 73 {0, 6}
5 90 {8, 6, 7}
6 4 {8, 7}
7 69 {2, 3}
8 75 {8, 3}
9 6 {1, 6}
Round 106, Devices participated 10, Average loss 0.060, Central accuracy on global test data 87.979, Ensemble accuracy on global test data 87.988, Local accuracy on global train data 68.884, Local accuracy on local train data 98.167


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {2, 3, 4}
1 98 {4}
2 48 {3, 4}
3 28 {2, 7}
4 76 {1, 3}
5 96 {1, 3}
6 97 {4, 7}
7 69 {2, 3}
8 66 {1, 3}
9 65 {3, 4}
Round 107, Devices participated 10, Average loss 0.069, Central accuracy on global test data 76.191, Ensemble accuracy on global test data 75.400, Local accuracy on global train data 62.197, Local accuracy on local train data 97.983


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 54 {1, 6}
2 70 {8, 5}
3 39 {8, 0}
4 89 {0, 9}
5 14 {9, 5}
6 77 {8, 1}
7 74 {5}
8 15 {5, 7}
9 88 {0}
Round 108, Devices participated 10, Average loss 0.062, Central accuracy on global test data 87.900, Ensemble accuracy on global test data 87.549, Local accuracy on global train data 60.583, Local accuracy on local train data 98.050


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 12 {8, 9, 3}
2 87 {8, 9}
3 84 {0, 9}
4 91 {2, 6}
5 62 {0, 7}
6 82 {8, 4}
7 57 {4, 6}
8 6 {1, 6}
9 21 {2, 5}
Round 109, Devices participated 10, Average loss 0.073, Central accuracy on global test data 89.727, Ensemble accuracy on global test data 89.785, Local accuracy on global train data 66.116, Local accuracy on local train data 97.600


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 99 {8, 9}
2 54 {1, 6}
3 55 {4, 7}
4 14 {9, 5}
5 63 {4, 5}
6 7 {0, 1}
7 88 {0}
8 91 {2, 6}
9 52 {8, 7}
Round 110, Devices participated 10, Average loss 0.043, Central accuracy on global test data 89.189, Ensemble accuracy on global test data 89.453, Local accuracy on global train data 68.611, Local accuracy on local train data 98.900


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {6, 7}
1 73 {0, 6}
2 39 {8, 0}
3 28 {2, 7}
4 18 {0, 6}
5 92 {1, 3}
6 52 {8, 7}
7 79 {0, 8}
8 54 {1, 6}
9 77 {8, 1}
Round 111, Devices participated 10, Average loss 0.050, Central accuracy on global test data 87.314, Ensemble accuracy on global test data 87.295, Local accuracy on global train data 66.772, Local accuracy on local train data 98.483


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 70 {8, 5}
2 54 {1, 6}
3 27 {2, 5}
4 89 {0, 9}
5 21 {2, 5}
6 42 {8, 1}
7 25 {8, 2}
8 60 {2}
9 49 {2, 7}
Round 112, Devices participated 10, Average loss 0.048, Central accuracy on global test data 87.207, Ensemble accuracy on global test data 86.973, Local accuracy on global train data 60.488, Local accuracy on local train data 98.733


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 56 {9, 2}
2 59 {0, 7}
3 80 {6}
4 62 {0, 7}
5 19 {5, 6}
6 42 {8, 1}
7 33 {5, 6}
8 72 {1, 9}
9 35 {8, 2}
Round 113, Devices participated 10, Average loss 0.045, Central accuracy on global test data 86.240, Ensemble accuracy on global test data 85.645, Local accuracy on global train data 61.924, Local accuracy on local train data 98.700


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 53 {0, 7}
1 17 {3, 4}
2 11 {4, 5}
3 78 {2, 3, 4}
4 66 {1, 3}
5 39 {8, 0}
6 94 {0, 1}
7 4 {8, 7}
8 47 {9, 5}
9 90 {8, 6, 7}
Round 114, Devices participated 10, Average loss 0.056, Central accuracy on global test data 90.195, Ensemble accuracy on global test data 90.439, Local accuracy on global train data 62.324, Local accuracy on local train data 98.483


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 48 {3, 4}
1 51 {4, 7}
2 34 {3, 7}
3 78 {2, 3, 4}
4 92 {1, 3}
5 54 {1, 6}
6 96 {1, 3}
7 84 {0, 9}
8 88 {0}
9 74 {5}
Round 115, Devices participated 10, Average loss 0.048, Central accuracy on global test data 85.498, Ensemble accuracy on global test data 84.990, Local accuracy on global train data 67.449, Local accuracy on local train data 98.600


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 15 {5, 7}
1 25 {8, 2}
2 50 {2, 3}
3 89 {0, 9}
4 72 {1, 9}
5 20 {2, 7}
6 85 {1, 4}
7 47 {9, 5}
8 1 {8, 4}
9 3 {8, 6}
Round 116, Devices participated 10, Average loss 0.050, Central accuracy on global test data 90.605, Ensemble accuracy on global test data 90.820, Local accuracy on global train data 58.914, Local accuracy on local train data 98.433


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 91 {2, 6}
1 39 {8, 0}
2 11 {4, 5}
3 12 {8, 9, 3}
4 93 {9, 6}
5 64 {1, 2}
6 30 {0, 1, 2}
7 79 {0, 8}
8 29 {9}
9 49 {2, 7}
Round 117, Devices participated 10, Average loss 0.046, Central accuracy on global test data 87.773, Ensemble accuracy on global test data 89.023, Local accuracy on global train data 66.670, Local accuracy on local train data 98.583


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 88 {0}
2 95 {3, 4}
3 65 {3, 4}
4 60 {2}
5 8 {8, 7}
6 46 {9, 6}
7 31 {0, 1}
8 63 {4, 5}
9 72 {1, 9}
Round 118, Devices participated 10, Average loss 0.038, Central accuracy on global test data 90.059, Ensemble accuracy on global test data 90.557, Local accuracy on global train data 58.982, Local accuracy on local train data 98.917


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 63 {4, 5}
2 58 {9, 4}
3 77 {8, 1}
4 95 {3, 4}
5 23 {1, 5}
6 61 {6}
7 29 {9}
8 91 {2, 6}
9 59 {0, 7}
Round 119, Devices participated 10, Average loss 0.058, Central accuracy on global test data 90.059, Ensemble accuracy on global test data 90.322, Local accuracy on global train data 67.952, Local accuracy on local train data 98.183


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 69 {2, 3}
2 44 {6, 7}
3 89 {0, 9}
4 49 {2, 7}
5 36 {1, 3}
6 77 {8, 1}
7 19 {5, 6}
8 92 {1, 3}
9 72 {1, 9}
Round 120, Devices participated 10, Average loss 0.061, Central accuracy on global test data 90.547, Ensemble accuracy on global test data 89.922, Local accuracy on global train data 60.742, Local accuracy on local train data 98.117


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 49 {2, 7}
2 1 {8, 4}
3 80 {6}
4 27 {2, 5}
5 9 {1}
6 83 {9, 2}
7 72 {1, 9}
8 76 {1, 3}
9 92 {1, 3}
Round 121, Devices participated 10, Average loss 0.038, Central accuracy on global test data 89.199, Ensemble accuracy on global test data 89.688, Local accuracy on global train data 63.337, Local accuracy on local train data 98.967


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 57 {4, 6}
2 54 {1, 6}
3 1 {8, 4}
4 80 {6}
5 76 {1, 3}
6 56 {9, 2}
7 39 {8, 0}
8 14 {9, 5}
9 20 {2, 7}
Round 122, Devices participated 10, Average loss 0.043, Central accuracy on global test data 90.029, Ensemble accuracy on global test data 90.391, Local accuracy on global train data 63.491, Local accuracy on local train data 98.567


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 6 {1, 6}
1 62 {0, 7}
2 31 {0, 1}
3 90 {8, 6, 7}
4 3 {8, 6}
5 51 {4, 7}
6 24 {4, 5}
7 91 {2, 6}
8 23 {1, 5}
9 55 {4, 7}
Round 123, Devices participated 10, Average loss 0.036, Central accuracy on global test data 89.277, Ensemble accuracy on global test data 87.607, Local accuracy on global train data 70.190, Local accuracy on local train data 98.883


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 18 {0, 6}
2 78 {2, 3, 4}
3 89 {0, 9}
4 27 {2, 5}
5 80 {6}
6 33 {5, 6}
7 86 {1, 2}
8 88 {0}
9 15 {5, 7}
Round 124, Devices participated 10, Average loss 0.040, Central accuracy on global test data 89.082, Ensemble accuracy on global test data 88.975, Local accuracy on global train data 69.307, Local accuracy on local train data 98.733


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 12 {8, 9, 3}
2 59 {0, 7}
3 60 {2}
4 73 {0, 6}
5 80 {6}
6 0 {1, 2}
7 10 {3, 7}
8 23 {1, 5}
9 13 {8, 5, 6}
Round 125, Devices participated 10, Average loss 0.046, Central accuracy on global test data 90.195, Ensemble accuracy on global test data 89.736, Local accuracy on global train data 68.447, Local accuracy on local train data 98.633


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 46 {9, 6}
2 54 {1, 6}
3 87 {8, 9}
4 84 {0, 9}
5 51 {4, 7}
6 27 {2, 5}
7 95 {3, 4}
8 21 {2, 5}
9 61 {6}
Round 126, Devices participated 10, Average loss 0.042, Central accuracy on global test data 90.000, Ensemble accuracy on global test data 90.332, Local accuracy on global train data 65.537, Local accuracy on local train data 98.733


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 6 {1, 6}
2 96 {1, 3}
3 98 {4}
4 27 {2, 5}
5 71 {1, 5}
6 18 {0, 6}
7 3 {8, 6}
8 1 {8, 4}
9 95 {3, 4}
Round 127, Devices participated 10, Average loss 0.031, Central accuracy on global test data 88.945, Ensemble accuracy on global test data 88.613, Local accuracy on global train data 68.108, Local accuracy on local train data 99.250


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 88 {0}
1 90 {8, 6, 7}
2 96 {1, 3}
3 35 {8, 2}
4 69 {2, 3}
5 11 {4, 5}
6 60 {2}
7 48 {3, 4}
8 23 {1, 5}
9 42 {8, 1}
Round 128, Devices participated 10, Average loss 0.047, Central accuracy on global test data 87.705, Ensemble accuracy on global test data 87.822, Local accuracy on global train data 60.671, Local accuracy on local train data 98.650


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 88 {0}
2 43 {8, 3}
3 57 {4, 6}
4 48 {3, 4}
5 99 {8, 9}
6 63 {4, 5}
7 53 {0, 7}
8 94 {0, 1}
9 55 {4, 7}
Round 129, Devices participated 10, Average loss 0.054, Central accuracy on global test data 84.004, Ensemble accuracy on global test data 84.600, Local accuracy on global train data 67.246, Local accuracy on local train data 98.417


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 75 {8, 3}
2 74 {5}
3 76 {1, 3}
4 39 {8, 0}
5 80 {6}
6 34 {3, 7}
7 12 {8, 9, 3}
8 60 {2}
9 20 {2, 7}
Round 130, Devices participated 10, Average loss 0.060, Central accuracy on global test data 88.633, Ensemble accuracy on global test data 87.871, Local accuracy on global train data 63.159, Local accuracy on local train data 98.217


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 20 {2, 7}
1 47 {9, 5}
2 38 {2, 7}
3 59 {0, 7}
4 64 {1, 2}
5 82 {8, 4}
6 60 {2}
7 45 {4, 5}
8 63 {4, 5}
9 85 {1, 4}
Round 131, Devices participated 10, Average loss 0.037, Central accuracy on global test data 88.350, Ensemble accuracy on global test data 86.777, Local accuracy on global train data 60.427, Local accuracy on local train data 98.800


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 6 {1, 6}
2 51 {4, 7}
3 4 {8, 7}
4 8 {8, 7}
5 22 {0, 5}
6 72 {1, 9}
7 86 {1, 2}
8 55 {4, 7}
9 12 {8, 9, 3}
Round 132, Devices participated 10, Average loss 0.049, Central accuracy on global test data 91.143, Ensemble accuracy on global test data 91.074, Local accuracy on global train data 66.738, Local accuracy on local train data 98.517


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 71 {1, 5}
2 84 {0, 9}
3 65 {3, 4}
4 61 {6}
5 83 {9, 2}
6 87 {8, 9}
7 53 {0, 7}
8 77 {8, 1}
9 51 {4, 7}
Round 133, Devices participated 10, Average loss 0.044, Central accuracy on global test data 90.869, Ensemble accuracy on global test data 90.684, Local accuracy on global train data 66.123, Local accuracy on local train data 98.667


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 71 {1, 5}
2 56 {9, 2}
3 63 {4, 5}
4 49 {2, 7}
5 44 {6, 7}
6 85 {1, 4}
7 76 {1, 3}
8 50 {2, 3}
9 93 {9, 6}
Round 134, Devices participated 10, Average loss 0.041, Central accuracy on global test data 91.064, Ensemble accuracy on global test data 90.898, Local accuracy on global train data 64.094, Local accuracy on local train data 98.750


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 52 {8, 7}
2 95 {3, 4}
3 68 {9, 6}
4 64 {1, 2}
5 15 {5, 7}
6 29 {9}
7 54 {1, 6}
8 43 {8, 3}
9 93 {9, 6}
Round 135, Devices participated 10, Average loss 0.042, Central accuracy on global test data 90.732, Ensemble accuracy on global test data 90.771, Local accuracy on global train data 64.663, Local accuracy on local train data 98.667


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 70 {8, 5}
2 25 {8, 2}
3 56 {9, 2}
4 87 {8, 9}
5 27 {2, 5}
6 94 {0, 1}
7 1 {8, 4}
8 74 {5}
9 44 {6, 7}
Round 136, Devices participated 10, Average loss 0.060, Central accuracy on global test data 89.160, Ensemble accuracy on global test data 89.287, Local accuracy on global train data 60.317, Local accuracy on local train data 97.967


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0892, device='cuda:0', grad_fn=<MaxBackward1>)
0 2 {9}
1 7 {0, 1}
2 31 {0, 1}
3 45 {4, 5}
4 81 {2, 3, 7}
5 62 {0, 7}
6 23 {1, 5}
7 97 {4, 7}
8 18 {0, 6}
9 27 {2, 5}
Round 137, Devices participated 10, Average loss 0.031, Central accuracy on global test data 88.311, Ensemble accuracy on global test data 90.176, Local accuracy on global train data 66.379, Local accuracy on local train data 98.900


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 13 {8, 5, 6}
2 40 {3, 5}
3 64 {1, 2}
4 52 {8, 7}
5 44 {6, 7}
6 22 {0, 5}
7 48 {3, 4}
8 3 {8, 6}
9 46 {9, 6}
Round 138, Devices participated 10, Average loss 0.053, Central accuracy on global test data 91.172, Ensemble accuracy on global test data 90.420, Local accuracy on global train data 63.030, Local accuracy on local train data 98.267


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 9 {1}
2 72 {1, 9}
3 56 {9, 2}
4 13 {8, 5, 6}
5 98 {4}
6 21 {2, 5}
7 79 {0, 8}
8 71 {1, 5}
9 32 {0, 9}
Round 139, Devices participated 10, Average loss 0.025, Central accuracy on global test data 88.906, Ensemble accuracy on global test data 88.809, Local accuracy on global train data 66.382, Local accuracy on local train data 99.300


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 2 {9}
2 36 {1, 3}
3 91 {2, 6}
4 58 {9, 4}
5 81 {2, 3, 7}
6 87 {8, 9}
7 68 {9, 6}
8 25 {8, 2}
9 5 {0, 6, 7}
Round 140, Devices participated 10, Average loss 0.065, Central accuracy on global test data 83.047, Ensemble accuracy on global test data 86.475, Local accuracy on global train data 66.445, Local accuracy on local train data 97.883


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 41 {0, 3}
2 74 {5}
3 21 {2, 5}
4 97 {4, 7}
5 69 {2, 3}
6 4 {8, 7}
7 17 {3, 4}
8 3 {8, 6}
9 88 {0}
Round 141, Devices participated 10, Average loss 0.046, Central accuracy on global test data 89.736, Ensemble accuracy on global test data 91.396, Local accuracy on global train data 57.402, Local accuracy on local train data 98.417


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 19 {5, 6}
2 97 {4, 7}
3 75 {8, 3}
4 60 {2}
5 93 {9, 6}
6 78 {2, 3, 4}
7 40 {3, 5}
8 32 {0, 9}
9 87 {8, 9}
Round 142, Devices participated 10, Average loss 0.065, Central accuracy on global test data 89.502, Ensemble accuracy on global test data 90.449, Local accuracy on global train data 63.350, Local accuracy on local train data 97.833


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {8, 5}
1 59 {0, 7}
2 68 {9, 6}
3 16 {0, 3}
4 36 {1, 3}
5 72 {1, 9}
6 71 {1, 5}
7 45 {4, 5}
8 91 {2, 6}
9 56 {9, 2}
Round 143, Devices participated 10, Average loss 0.039, Central accuracy on global test data 89.199, Ensemble accuracy on global test data 89.668, Local accuracy on global train data 61.853, Local accuracy on local train data 98.917


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {9, 2}
1 81 {2, 3, 7}
2 9 {1}
3 39 {8, 0}
4 2 {9}
5 67 {8, 1}
6 66 {1, 3}
7 98 {4}
8 80 {6}
9 51 {4, 7}
Round 144, Devices participated 10, Average loss 0.039, Central accuracy on global test data 32.842, Ensemble accuracy on global test data 90.703, Local accuracy on global train data 65.879, Local accuracy on local train data 98.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 43 {8, 3}
1 64 {1, 2}
2 42 {8, 1}
3 4 {8, 7}
4 78 {2, 3, 4}
5 0 {1, 2}
6 39 {8, 0}
7 57 {4, 6}
8 37 {4, 5, 7}
9 98 {4}
Round 145, Devices participated 10, Average loss 0.117, Central accuracy on global test data 79.414, Ensemble accuracy on global test data 75.361, Local accuracy on global train data 49.043, Local accuracy on local train data 97.233


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 11 {4, 5}
1 6 {1, 6}
2 16 {0, 3}
3 48 {3, 4}
4 14 {9, 5}
5 19 {5, 6}
6 68 {9, 6}
7 82 {8, 4}
8 94 {0, 1}
9 39 {8, 0}
Round 146, Devices participated 10, Average loss 0.052, Central accuracy on global test data 88.477, Ensemble accuracy on global test data 88.262, Local accuracy on global train data 49.946, Local accuracy on local train data 98.633


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 7 {0, 1}
2 93 {9, 6}
3 77 {8, 1}
4 78 {2, 3, 4}
5 38 {2, 7}
6 46 {9, 6}
7 99 {8, 9}
8 42 {8, 1}
9 54 {1, 6}
Round 147, Devices participated 10, Average loss 0.049, Central accuracy on global test data 88.350, Ensemble accuracy on global test data 88.203, Local accuracy on global train data 58.413, Local accuracy on local train data 98.583


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 70 {8, 5}
2 71 {1, 5}
3 57 {4, 6}
4 63 {4, 5}
5 73 {0, 6}
6 69 {2, 3}
7 17 {3, 4}
8 48 {3, 4}
9 47 {9, 5}
Round 148, Devices participated 10, Average loss 0.069, Central accuracy on global test data 84.443, Ensemble accuracy on global test data 82.881, Local accuracy on global train data 55.381, Local accuracy on local train data 97.750


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 42 {8, 1}
2 60 {2}
3 43 {8, 3}
4 85 {1, 4}
5 48 {3, 4}
6 1 {8, 4}
7 91 {2, 6}
8 22 {0, 5}
9 26 {9, 7}
Round 149, Devices participated 10, Average loss 0.061, Central accuracy on global test data 88.066, Ensemble accuracy on global test data 87.471, Local accuracy on global train data 58.423, Local accuracy on local train data 97.967


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 3 {8, 6}
2 23 {1, 5}
3 96 {1, 3}
4 47 {9, 5}
5 32 {0, 9}
6 52 {8, 7}
7 50 {2, 3}
8 82 {8, 4}
9 28 {2, 7}
Round 150, Devices participated 10, Average loss 0.050, Central accuracy on global test data 90.361, Ensemble accuracy on global test data 91.348, Local accuracy on global train data 58.374, Local accuracy on local train data 98.700


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 55 {4, 7}
2 26 {9, 7}
3 94 {0, 1}
4 14 {9, 5}
5 72 {1, 9}
6 5 {0, 6, 7}
7 50 {2, 3}
8 91 {2, 6}
9 42 {8, 1}
Round 151, Devices participated 10, Average loss 0.058, Central accuracy on global test data 88.145, Ensemble accuracy on global test data 88.105, Local accuracy on global train data 63.357, Local accuracy on local train data 98.233


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 75 {8, 3}
1 69 {2, 3}
2 0 {1, 2}
3 7 {0, 1}
4 53 {0, 7}
5 77 {8, 1}
6 67 {8, 1}
7 64 {1, 2}
8 62 {0, 7}
9 73 {0, 6}
Round 152, Devices participated 10, Average loss 0.056, Central accuracy on global test data 89.346, Ensemble accuracy on global test data 89.443, Local accuracy on global train data 64.519, Local accuracy on local train data 98.267


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {9, 5}
1 27 {2, 5}
2 66 {1, 3}
3 5 {0, 6, 7}
4 15 {5, 7}
5 75 {8, 3}
6 91 {2, 6}
7 28 {2, 7}
8 63 {4, 5}
9 62 {0, 7}
Round 153, Devices participated 10, Average loss 0.046, Central accuracy on global test data 90.518, Ensemble accuracy on global test data 89.160, Local accuracy on global train data 58.779, Local accuracy on local train data 98.733


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {5}
1 39 {8, 0}
2 76 {1, 3}
3 51 {4, 7}
4 46 {9, 6}
5 12 {8, 9, 3}
6 95 {3, 4}
7 17 {3, 4}
8 68 {9, 6}
9 93 {9, 6}
Round 154, Devices participated 10, Average loss 0.050, Central accuracy on global test data 88.604, Ensemble accuracy on global test data 89.697, Local accuracy on global train data 62.346, Local accuracy on local train data 98.583


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 28 {2, 7}
2 88 {0}
3 96 {1, 3}
4 20 {2, 7}
5 23 {1, 5}
6 19 {5, 6}
7 15 {5, 7}
8 91 {2, 6}
9 11 {4, 5}
Round 155, Devices participated 10, Average loss 0.043, Central accuracy on global test data 89.307, Ensemble accuracy on global test data 88.057, Local accuracy on global train data 64.233, Local accuracy on local train data 98.667


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 18 {0, 6}
1 46 {9, 6}
2 87 {8, 9}
3 61 {6}
4 79 {0, 8}
5 50 {2, 3}
6 69 {2, 3}
7 97 {4, 7}
8 72 {1, 9}
9 37 {4, 5, 7}
Round 156, Devices participated 10, Average loss 0.053, Central accuracy on global test data 90.088, Ensemble accuracy on global test data 90.449, Local accuracy on global train data 63.567, Local accuracy on local train data 98.250


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 57 {4, 6}
2 53 {0, 7}
3 30 {0, 1, 2}
4 72 {1, 9}
5 9 {1}
6 37 {4, 5, 7}
7 19 {5, 6}
8 5 {0, 6, 7}
9 14 {9, 5}
Round 157, Devices participated 10, Average loss 0.046, Central accuracy on global test data 89.805, Ensemble accuracy on global test data 89.785, Local accuracy on global train data 71.050, Local accuracy on local train data 98.550


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 67 {8, 1}
2 61 {6}
3 35 {8, 2}
4 96 {1, 3}
5 86 {1, 2}
6 63 {4, 5}
7 9 {1}
8 8 {8, 7}
9 36 {1, 3}
Round 158, Devices participated 10, Average loss 0.044, Central accuracy on global test data 90.166, Ensemble accuracy on global test data 89.854, Local accuracy on global train data 63.542, Local accuracy on local train data 98.633


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {0, 9}
1 41 {0, 3}
2 20 {2, 7}
3 92 {1, 3}
4 67 {8, 1}
5 44 {6, 7}
6 61 {6}
7 77 {8, 1}
8 3 {8, 6}
9 9 {1}
Round 159, Devices participated 10, Average loss 0.035, Central accuracy on global test data 89.600, Ensemble accuracy on global test data 89.033, Local accuracy on global train data 67.625, Local accuracy on local train data 99.100


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {8, 9}
1 20 {2, 7}
2 97 {4, 7}
3 50 {2, 3}
4 35 {8, 2}
5 68 {9, 6}
6 33 {5, 6}
7 98 {4}
8 53 {0, 7}
9 96 {1, 3}
Round 160, Devices participated 10, Average loss 0.054, Central accuracy on global test data 91.006, Ensemble accuracy on global test data 90.596, Local accuracy on global train data 61.199, Local accuracy on local train data 98.317


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {0, 9}
1 57 {4, 6}
2 26 {9, 7}
3 69 {2, 3}
4 23 {1, 5}
5 54 {1, 6}
6 68 {9, 6}
7 92 {1, 3}
8 4 {8, 7}
9 82 {8, 4}
Round 161, Devices participated 10, Average loss 0.045, Central accuracy on global test data 90.947, Ensemble accuracy on global test data 91.113, Local accuracy on global train data 66.016, Local accuracy on local train data 98.733


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 31 {0, 1}
2 11 {4, 5}
3 28 {2, 7}
4 39 {8, 0}
5 71 {1, 5}
6 7 {0, 1}
7 91 {2, 6}
8 4 {8, 7}
9 93 {9, 6}
Round 162, Devices participated 10, Average loss 0.034, Central accuracy on global test data 89.775, Ensemble accuracy on global test data 90.742, Local accuracy on global train data 64.282, Local accuracy on local train data 98.867


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 85 {1, 4}
2 50 {2, 3}
3 23 {1, 5}
4 46 {9, 6}
5 82 {8, 4}
6 83 {9, 2}
7 20 {2, 7}
8 49 {2, 7}
9 6 {1, 6}
Round 163, Devices participated 10, Average loss 0.035, Central accuracy on global test data 91.025, Ensemble accuracy on global test data 90.811, Local accuracy on global train data 61.768, Local accuracy on local train data 98.800


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 42 {8, 1}
2 84 {0, 9}
3 62 {0, 7}
4 69 {2, 3}
5 25 {8, 2}
6 23 {1, 5}
7 76 {1, 3}
8 52 {8, 7}
9 75 {8, 3}
Round 164, Devices participated 10, Average loss 0.051, Central accuracy on global test data 90.205, Ensemble accuracy on global test data 89.219, Local accuracy on global train data 62.930, Local accuracy on local train data 98.367


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 95 {3, 4}
2 1 {8, 4}
3 21 {2, 5}
4 83 {9, 2}
5 23 {1, 5}
6 48 {3, 4}
7 78 {2, 3, 4}
8 42 {8, 1}
9 31 {0, 1}
Round 165, Devices participated 10, Average loss 0.041, Central accuracy on global test data 89.355, Ensemble accuracy on global test data 88.838, Local accuracy on global train data 62.991, Local accuracy on local train data 98.900


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 38 {2, 7}
1 6 {1, 6}
2 37 {4, 5, 7}
3 55 {4, 7}
4 34 {3, 7}
5 97 {4, 7}
6 98 {4}
7 74 {5}
8 56 {9, 2}
9 45 {4, 5}
Round 166, Devices participated 10, Average loss 0.033, Central accuracy on global test data 85.107, Ensemble accuracy on global test data 85.186, Local accuracy on global train data 64.324, Local accuracy on local train data 98.983


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 93 {9, 6}
2 74 {5}
3 85 {1, 4}
4 16 {0, 3}
5 98 {4}
6 50 {2, 3}
7 59 {0, 7}
8 70 {8, 5}
9 3 {8, 6}
Round 167, Devices participated 10, Average loss 0.044, Central accuracy on global test data 90.645, Ensemble accuracy on global test data 89.785, Local accuracy on global train data 66.699, Local accuracy on local train data 98.650


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 39 {8, 0}
1 20 {2, 7}
2 43 {8, 3}
3 1 {8, 4}
4 78 {2, 3, 4}
5 0 {1, 2}
6 60 {2}
7 31 {0, 1}
8 15 {5, 7}
9 6 {1, 6}
Round 168, Devices participated 10, Average loss 0.047, Central accuracy on global test data 90.957, Ensemble accuracy on global test data 90.518, Local accuracy on global train data 64.919, Local accuracy on local train data 98.450


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 76 {1, 3}
2 2 {9}
3 31 {0, 1}
4 19 {5, 6}
5 17 {3, 4}
6 83 {9, 2}
7 92 {1, 3}
8 44 {6, 7}
9 35 {8, 2}
Round 169, Devices participated 10, Average loss 0.038, Central accuracy on global test data 90.967, Ensemble accuracy on global test data 91.416, Local accuracy on global train data 65.703, Local accuracy on local train data 98.733


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {9, 2}
1 91 {2, 6}
2 92 {1, 3}
3 28 {2, 7}
4 38 {2, 7}
5 82 {8, 4}
6 32 {0, 9}
7 98 {4}
8 63 {4, 5}
9 85 {1, 4}
Round 170, Devices participated 10, Average loss 0.029, Central accuracy on global test data 90.684, Ensemble accuracy on global test data 90.107, Local accuracy on global train data 66.860, Local accuracy on local train data 99.133


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 5 {0, 6, 7}
1 84 {0, 9}
2 60 {2}
3 47 {9, 5}
4 3 {8, 6}
5 13 {8, 5, 6}
6 65 {3, 4}
7 92 {1, 3}
8 19 {5, 6}
9 74 {5}
Round 171, Devices participated 10, Average loss 0.040, Central accuracy on global test data 91.084, Ensemble accuracy on global test data 90.654, Local accuracy on global train data 63.499, Local accuracy on local train data 98.783


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 45 {4, 5}
2 39 {8, 0}
3 67 {8, 1}
4 6 {1, 6}
5 78 {2, 3, 4}
6 84 {0, 9}
7 80 {6}
8 22 {0, 5}
9 57 {4, 6}
Round 172, Devices participated 10, Average loss 0.033, Central accuracy on global test data 91.035, Ensemble accuracy on global test data 90.859, Local accuracy on global train data 73.308, Local accuracy on local train data 99.050


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 42 {8, 1}
2 43 {8, 3}
3 38 {2, 7}
4 94 {0, 1}
5 36 {1, 3}
6 68 {9, 6}
7 8 {8, 7}
8 11 {4, 5}
9 89 {0, 9}
Round 173, Devices participated 10, Average loss 0.040, Central accuracy on global test data 91.650, Ensemble accuracy on global test data 91.689, Local accuracy on global train data 64.641, Local accuracy on local train data 98.817


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 69 {2, 3}
2 89 {0, 9}
3 9 {1}
4 42 {8, 1}
5 74 {5}
6 62 {0, 7}
7 68 {9, 6}
8 57 {4, 6}
9 93 {9, 6}
Round 174, Devices participated 10, Average loss 0.033, Central accuracy on global test data 90.576, Ensemble accuracy on global test data 90.234, Local accuracy on global train data 70.986, Local accuracy on local train data 99.067


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 66 {1, 3}
2 93 {9, 6}
3 44 {6, 7}
4 48 {3, 4}
5 13 {8, 5, 6}
6 3 {8, 6}
7 21 {2, 5}
8 23 {1, 5}
9 39 {8, 0}
Round 175, Devices participated 10, Average loss 0.033, Central accuracy on global test data 91.025, Ensemble accuracy on global test data 90.986, Local accuracy on global train data 64.578, Local accuracy on local train data 98.900


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 94 {0, 1}
2 18 {0, 6}
3 68 {9, 6}
4 65 {3, 4}
5 53 {0, 7}
6 66 {1, 3}
7 75 {8, 3}
8 12 {8, 9, 3}
9 92 {1, 3}
Round 176, Devices participated 10, Average loss 0.048, Central accuracy on global test data 87.402, Ensemble accuracy on global test data 88.438, Local accuracy on global train data 68.564, Local accuracy on local train data 98.550


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 75 {8, 3}
2 39 {8, 0}
3 64 {1, 2}
4 49 {2, 7}
5 44 {6, 7}
6 66 {1, 3}
7 50 {2, 3}
8 60 {2}
9 98 {4}
Round 177, Devices participated 10, Average loss 0.043, Central accuracy on global test data 89.551, Ensemble accuracy on global test data 88.955, Local accuracy on global train data 61.379, Local accuracy on local train data 98.500


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {6}
1 27 {2, 5}
2 77 {8, 1}
3 23 {1, 5}
4 66 {1, 3}
5 33 {5, 6}
6 26 {9, 7}
7 42 {8, 1}
8 37 {4, 5, 7}
9 78 {2, 3, 4}
Round 178, Devices participated 10, Average loss 0.044, Central accuracy on global test data 91.416, Ensemble accuracy on global test data 91.406, Local accuracy on global train data 71.143, Local accuracy on local train data 98.533


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 56 {9, 2}
2 5 {0, 6, 7}
3 30 {0, 1, 2}
4 76 {1, 3}
5 95 {3, 4}
6 53 {0, 7}
7 73 {0, 6}
8 63 {4, 5}
9 88 {0}
Round 179, Devices participated 10, Average loss 0.037, Central accuracy on global test data 90.898, Ensemble accuracy on global test data 90.547, Local accuracy on global train data 71.921, Local accuracy on local train data 99.000


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 89 {0, 9}
2 68 {9, 6}
3 64 {1, 2}
4 78 {2, 3, 4}
5 62 {0, 7}
6 28 {2, 7}
7 30 {0, 1, 2}
8 49 {2, 7}
9 41 {0, 3}
Round 180, Devices participated 10, Average loss 0.031, Central accuracy on global test data 88.438, Ensemble accuracy on global test data 87.930, Local accuracy on global train data 69.375, Local accuracy on local train data 99.150


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 67 {8, 1}
1 45 {4, 5}
2 94 {0, 1}
3 35 {8, 2}
4 39 {8, 0}
5 72 {1, 9}
6 5 {0, 6, 7}
7 22 {0, 5}
8 7 {0, 1}
9 62 {0, 7}
Round 181, Devices participated 10, Average loss 0.033, Central accuracy on global test data 91.680, Ensemble accuracy on global test data 91.455, Local accuracy on global train data 69.939, Local accuracy on local train data 98.933


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 91 {2, 6}
2 9 {1}
3 58 {9, 4}
4 98 {4}
5 5 {0, 6, 7}
6 97 {4, 7}
7 92 {1, 3}
8 55 {4, 7}
9 12 {8, 9, 3}
Round 182, Devices participated 10, Average loss 0.059, Central accuracy on global test data 90.840, Ensemble accuracy on global test data 90.010, Local accuracy on global train data 76.321, Local accuracy on local train data 98.233


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 9 {1}
2 22 {0, 5}
3 95 {3, 4}
4 91 {2, 6}
5 55 {4, 7}
6 21 {2, 5}
7 15 {5, 7}
8 13 {8, 5, 6}
9 1 {8, 4}
Round 183, Devices participated 10, Average loss 0.030, Central accuracy on global test data 91.172, Ensemble accuracy on global test data 90.166, Local accuracy on global train data 69.995, Local accuracy on local train data 99.117


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 53 {0, 7}
2 10 {3, 7}
3 7 {0, 1}
4 6 {1, 6}
5 90 {8, 6, 7}
6 21 {2, 5}
7 92 {1, 3}
8 78 {2, 3, 4}
9 46 {9, 6}
Round 184, Devices participated 10, Average loss 0.037, Central accuracy on global test data 90.898, Ensemble accuracy on global test data 90.596, Local accuracy on global train data 66.672, Local accuracy on local train data 98.817


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 92 {1, 3}
2 66 {1, 3}
3 58 {9, 4}
4 24 {4, 5}
5 86 {1, 2}
6 82 {8, 4}
7 2 {9}
8 7 {0, 1}
9 64 {1, 2}
Round 185, Devices participated 10, Average loss 0.043, Central accuracy on global test data 89.482, Ensemble accuracy on global test data 89.189, Local accuracy on global train data 68.989, Local accuracy on local train data 98.350


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {3, 5}
1 50 {2, 3}
2 36 {1, 3}
3 99 {8, 9}
4 33 {5, 6}
5 79 {0, 8}
6 62 {0, 7}
7 74 {5}
8 16 {0, 3}
9 22 {0, 5}
Round 186, Devices participated 10, Average loss 0.067, Central accuracy on global test data 90.752, Ensemble accuracy on global test data 90.723, Local accuracy on global train data 64.338, Local accuracy on local train data 97.767


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 33 {5, 6}
2 10 {3, 7}
3 14 {9, 5}
4 87 {8, 9}
5 8 {8, 7}
6 51 {4, 7}
7 83 {9, 2}
8 69 {2, 3}
9 57 {4, 6}
Round 187, Devices participated 10, Average loss 0.057, Central accuracy on global test data 91.504, Ensemble accuracy on global test data 91.963, Local accuracy on global train data 61.494, Local accuracy on local train data 97.983


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 15 {5, 7}
2 38 {2, 7}
3 19 {5, 6}
4 52 {8, 7}
5 46 {9, 6}
6 71 {1, 5}
7 9 {1}
8 47 {9, 5}
9 75 {8, 3}
Round 188, Devices participated 10, Average loss 0.038, Central accuracy on global test data 91.211, Ensemble accuracy on global test data 91.045, Local accuracy on global train data 67.000, Local accuracy on local train data 98.717


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 86 {1, 2}
1 92 {1, 3}
2 14 {9, 5}
3 8 {8, 7}
4 45 {4, 5}
5 38 {2, 7}
6 60 {2}
7 63 {4, 5}
8 82 {8, 4}
9 46 {9, 6}
Round 189, Devices participated 10, Average loss 0.027, Central accuracy on global test data 92.334, Ensemble accuracy on global test data 91.982, Local accuracy on global train data 62.268, Local accuracy on local train data 99.083


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {0, 6}
1 57 {4, 6}
2 67 {8, 1}
3 14 {9, 5}
4 40 {3, 5}
5 24 {4, 5}
6 63 {4, 5}
7 35 {8, 2}
8 72 {1, 9}
9 69 {2, 3}
Round 190, Devices participated 10, Average loss 0.052, Central accuracy on global test data 91.465, Ensemble accuracy on global test data 91.162, Local accuracy on global train data 65.598, Local accuracy on local train data 98.450


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 77 {8, 1}
1 55 {4, 7}
2 61 {6}
3 34 {3, 7}
4 18 {0, 6}
5 35 {8, 2}
6 0 {1, 2}
7 97 {4, 7}
8 98 {4}
9 26 {9, 7}
Round 191, Devices participated 10, Average loss 0.041, Central accuracy on global test data 91.592, Ensemble accuracy on global test data 91.895, Local accuracy on global train data 72.439, Local accuracy on local train data 98.767


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 30 {0, 1, 2}
2 53 {0, 7}
3 77 {8, 1}
4 76 {1, 3}
5 85 {1, 4}
6 8 {8, 7}
7 35 {8, 2}
8 81 {2, 3, 7}
9 69 {2, 3}
Round 192, Devices participated 10, Average loss 0.046, Central accuracy on global test data 89.443, Ensemble accuracy on global test data 88.887, Local accuracy on global train data 71.223, Local accuracy on local train data 98.650


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 30 {0, 1, 2}
2 46 {9, 6}
3 49 {2, 7}
4 35 {8, 2}
5 90 {8, 6, 7}
6 6 {1, 6}
7 40 {3, 5}
8 20 {2, 7}
9 97 {4, 7}
Round 193, Devices participated 10, Average loss 0.045, Central accuracy on global test data 89.688, Ensemble accuracy on global test data 88.564, Local accuracy on global train data 69.519, Local accuracy on local train data 98.517


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 21 {2, 5}
2 97 {4, 7}
3 18 {0, 6}
4 44 {6, 7}
5 48 {3, 4}
6 98 {4}
7 45 {4, 5}
8 7 {0, 1}
9 95 {3, 4}
Round 194, Devices participated 10, Average loss 0.025, Central accuracy on global test data 86.094, Ensemble accuracy on global test data 86.191, Local accuracy on global train data 72.720, Local accuracy on local train data 99.350


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 67 {8, 1}
1 72 {1, 9}
2 49 {2, 7}
3 58 {9, 4}
4 2 {9}
5 66 {1, 3}
6 55 {4, 7}
7 98 {4}
8 25 {8, 2}
9 53 {0, 7}
Round 195, Devices participated 10, Average loss 0.056, Central accuracy on global test data 92.432, Ensemble accuracy on global test data 92.197, Local accuracy on global train data 68.064, Local accuracy on local train data 98.150


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 16 {0, 3}
1 2 {9}
2 36 {1, 3}
3 77 {8, 1}
4 86 {1, 2}
5 40 {3, 5}
6 87 {8, 9}
7 33 {5, 6}
8 95 {3, 4}
9 75 {8, 3}
Round 196, Devices participated 10, Average loss 0.055, Central accuracy on global test data 83.877, Ensemble accuracy on global test data 89.287, Local accuracy on global train data 64.834, Local accuracy on local train data 98.183


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 66 {1, 3}
2 81 {2, 3, 7}
3 52 {8, 7}
4 47 {9, 5}
5 99 {8, 9}
6 38 {2, 7}
7 56 {9, 2}
8 65 {3, 4}
9 4 {8, 7}
Round 197, Devices participated 10, Average loss 0.053, Central accuracy on global test data 91.182, Ensemble accuracy on global test data 90.469, Local accuracy on global train data 57.527, Local accuracy on local train data 98.317


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {0, 6}
1 93 {9, 6}
2 90 {8, 6, 7}
3 66 {1, 3}
4 22 {0, 5}
5 52 {8, 7}
6 77 {8, 1}
7 18 {0, 6}
8 71 {1, 5}
9 39 {8, 0}
Round 198, Devices participated 10, Average loss 0.044, Central accuracy on global test data 91.328, Ensemble accuracy on global test data 91.094, Local accuracy on global train data 70.669, Local accuracy on local train data 98.700


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 81 {2, 3, 7}
2 11 {4, 5}
3 2 {9}
4 36 {1, 3}
5 79 {0, 8}
6 23 {1, 5}
7 5 {0, 6, 7}
8 64 {1, 2}
9 30 {0, 1, 2}
Round 199, Devices participated 10, Average loss 0.047, Central accuracy on global test data 91.865, Ensemble accuracy on global test data 91.943, Local accuracy on global train data 72.271, Local accuracy on local train data 98.517


Testing accuracy on test data: 91.87, Testing loss: 0.18
