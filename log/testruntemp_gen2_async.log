python main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_refresh 0 --store_models testtemp_async --alpha_scale 0.5 --async_s2d 1


def DFAN_regavg(args, teacher, student, generator, optimizer, epoch):

    for local in teacher:
        local.eval()
    generator.train()
    student.train()
    loss_G = torch.tensor(0.0)
    loss_S = torch.tensor(0.0)
    optimizer_S, optimizer_G = optimizer
    sm = torch.nn.Softmax()

    w_locals = []
    for local in teacher:
        w_locals.append(copy.deepcopy(local.state_dict()))
    w_fedavg = FedAvg(w_locals)

    # mlpa, mlpt = model_fusion_MLP(teacher, 1024, 200, 10)
    # mlpa, mlpt = mlpa.to(args.device), mlpt.to(args.device)
    # w_mlpt = mlpt.state_dict()
    # mlpa, mlpt = fusion_layer_MLP(teacher, student, 1024, 200, 10)
    # mlpa = mlpa.to(args.device)
    # mlpt = mlpt.to(args.device)
    # mlpa.eval()
    # mlpt.eval()
    # print(mlpt.alpha)

    
    # w_fedavg = mlpt.state_dict()

    for i in range(args.epoch_itrs):
        for k in range(2):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            optimizer_G.zero_grad()
            fake = generator(z)
            fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
            s_logit = student(fake)
            # t_sm = ensemble(teacher, fake, detach=False, mode=args.ensemble_mode)
            # t_logit = torch.zeros_like(teacher[0](fake))
            # for k in range(10):
            #     t_logit += teacher[k](fake) 
            t_logit = mlpa(fake)
            oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
            max_Gout = torch.max(torch.abs(fake))
            if max_Gout > 8.0:
                loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0) + torch.pow(max_Gout - 8.0, 2.0)
                print(max_Gout)
            else:
                loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0)
            loss_G.backward()                   
            optimizer_G.step()
        for j in range(5):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            optimizer_S.zero_grad()
            fake = generator(z).detach()
            fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
            s_logit = student(fake)
            # t_sm = ensemble(teacher, fake, detach=True, mode=args.ensemble_mode)
            # t_logit = torch.zeros_like(teacher[0](fake).detach())
            # for k in range(10):
            #     t_logit += teacher[k](fake).detach()
            t_logit = mlpa(fake).detach()
            oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
            loss_S1 = torch.log(1. / (2. - oneMinus_P_S))

            diff_L2 = torch.FloatTensor([0.]).to(args.device)
            for ln, student_w, fedavg_w in zip(student.state_dict().keys(), student.parameters(), w_fedavg.values()):
                diff_L2 += (fedavg_w - student_w).norm(2) * mlpt.alpha[ln] * args.alpha_scale
            if epoch == 0:
                loss_S2 = 0
            else:
                loss_S2 = diff_L2

            # diff_L2 = torch.FloatTensor([0.]).to(args.device)
            # for student_w, mlpt_w in zip(student.parameters(), w_mlpt.values()):
            #     diff_L2 += ((mlpt_w - student_w)*mlpt_w.abs()).norm(2)
            # loss_S2 = diff_L2 * alpha

            loss_S = loss_S1 + loss_S2
            loss_S.backward()
            optimizer_S.step()

        if args.verbose and i % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tG_Loss: {:.6f} S_loss: {:.6f}'.format(
                epoch, i, args.epoch_itrs, 100 * float(i) / float(args.epoch_itrs), loss_G.item(), loss_S.item()))        #vp.add_scalar('Loss_S', (epoch-1)*args.epoch_itrs+i, loss_S.item())
            #vp.add_scalar('Loss_G', (epoch-1)*args.epoch_itrs+i, loss_G.item())


[1@_[1@g[1@e[1@n[1@2
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 22.910, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(12.3980, device='cuda:0', grad_fn=<MaxBackward1>)
tensor(16.5001, device='cuda:0', grad_fn=<MaxBackward1>)
0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.439, Central accuracy on global test data 30.303, Ensemble accuracy on global test data 25.371, Local accuracy on global train data 19.370, Local accuracy on local train data 89.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.501, Central accuracy on global test data 36.934, Ensemble accuracy on global test data 36.807, Local accuracy on global train data 22.258, Local accuracy on local train data 89.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.514, Central accuracy on global test data 50.986, Ensemble accuracy on global test data 50.635, Local accuracy on global train data 23.040, Local accuracy on local train data 88.017


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.397, Central accuracy on global test data 42.959, Ensemble accuracy on global test data 39.160, Local accuracy on global train data 20.488, Local accuracy on local train data 92.033


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.448, Central accuracy on global test data 35.156, Ensemble accuracy on global test data 37.471, Local accuracy on global train data 21.699, Local accuracy on local train data 91.517


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.427, Central accuracy on global test data 49.795, Ensemble accuracy on global test data 52.832, Local accuracy on global train data 23.921, Local accuracy on local train data 91.733


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.506, Central accuracy on global test data 43.379, Ensemble accuracy on global test data 42.744, Local accuracy on global train data 26.284, Local accuracy on local train data 91.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.409, Central accuracy on global test data 54.756, Ensemble accuracy on global test data 54.600, Local accuracy on global train data 28.770, Local accuracy on local train data 92.950


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.490, Central accuracy on global test data 59.131, Ensemble accuracy on global test data 57.520, Local accuracy on global train data 28.286, Local accuracy on local train data 92.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.4154, device='cuda:0', grad_fn=<MaxBackward1>)
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.339, Central accuracy on global test data 54.414, Ensemble accuracy on global test data 51.426, Local accuracy on global train data 26.282, Local accuracy on local train data 94.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.230, Central accuracy on global test data 55.439, Ensemble accuracy on global test data 55.039, Local accuracy on global train data 30.842, Local accuracy on local train data 95.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.530, Central accuracy on global test data 41.729, Ensemble accuracy on global test data 41.035, Local accuracy on global train data 23.530, Local accuracy on local train data 88.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.444, Central accuracy on global test data 54.648, Ensemble accuracy on global test data 53.535, Local accuracy on global train data 27.966, Local accuracy on local train data 92.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.540, Central accuracy on global test data 55.557, Ensemble accuracy on global test data 54.971, Local accuracy on global train data 33.884, Local accuracy on local train data 91.717


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.464, Central accuracy on global test data 58.955, Ensemble accuracy on global test data 58.262, Local accuracy on global train data 30.598, Local accuracy on local train data 94.017


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 83 {9, 2}
2 22 {0, 5}
3 69 {2, 3}
4 1 {8, 4}
5 54 {1, 6}
6 62 {0, 7}
7 36 {1, 3}
8 58 {9, 4}
9 63 {4, 5}
Round  16, Devices participated 10, Average loss 0.424, Central accuracy on global test data 68.867, Ensemble accuracy on global test data 68.730, Local accuracy on global train data 28.999, Local accuracy on local train data 92.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 68 {9, 6}
2 4 {8, 7}
3 10 {3, 7}
4 33 {5, 6}
5 96 {1, 3}
6 15 {5, 7}
7 64 {1, 2}
8 89 {0, 9}
9 54 {1, 6}
Round  17, Devices participated 10, Average loss 0.577, Central accuracy on global test data 65.703, Ensemble accuracy on global test data 65.107, Local accuracy on global train data 32.949, Local accuracy on local train data 92.733


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {1, 3}
1 22 {0, 5}
2 1 {8, 4}
3 40 {3, 5}
4 41 {0, 3}
5 52 {8, 7}
6 23 {1, 5}
7 75 {8, 3}
8 62 {0, 7}
9 11 {4, 5}
Round  18, Devices participated 10, Average loss 0.549, Central accuracy on global test data 57.725, Ensemble accuracy on global test data 58.477, Local accuracy on global train data 35.630, Local accuracy on local train data 91.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 27 {2, 5}
2 8 {8, 7}
3 34 {3, 7}
4 98 {4}
5 74 {5}
6 36 {1, 3}
7 78 {2, 3, 4}
8 85 {1, 4}
9 88 {0}
Round  19, Devices participated 10, Average loss 0.316, Central accuracy on global test data 64.414, Ensemble accuracy on global test data 64.238, Local accuracy on global train data 38.616, Local accuracy on local train data 96.267


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 2 {9}
2 83 {9, 2}
3 32 {0, 9}
4 78 {2, 3, 4}
5 39 {8, 0}
6 25 {8, 2}
7 56 {9, 2}
8 76 {1, 3}
9 35 {8, 2}
Round  20, Devices participated 10, Average loss 0.417, Central accuracy on global test data 54.883, Ensemble accuracy on global test data 55.264, Local accuracy on global train data 36.475, Local accuracy on local train data 94.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 2 {9}
2 40 {3, 5}
3 64 {1, 2}
4 12 {8, 9, 3}
5 94 {0, 1}
6 60 {2}
7 24 {4, 5}
8 61 {6}
9 29 {9}
Round  21, Devices participated 10, Average loss 0.319, Central accuracy on global test data 58.994, Ensemble accuracy on global test data 59.795, Local accuracy on global train data 30.115, Local accuracy on local train data 94.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 26 {9, 7}
2 80 {6}
3 38 {2, 7}
4 85 {1, 4}
5 86 {1, 2}
6 11 {4, 5}
7 5 {0, 6, 7}
8 98 {4}
9 96 {1, 3}
Round  22, Devices participated 10, Average loss 0.391, Central accuracy on global test data 61.143, Ensemble accuracy on global test data 61.904, Local accuracy on global train data 39.153, Local accuracy on local train data 94.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 41 {0, 3}
1 36 {1, 3}
2 92 {1, 3}
3 48 {3, 4}
4 96 {1, 3}
5 76 {1, 3}
6 77 {8, 1}
7 57 {4, 6}
8 78 {2, 3, 4}
9 28 {2, 7}
Round  23, Devices participated 10, Average loss 0.567, Central accuracy on global test data 50.791, Ensemble accuracy on global test data 48.818, Local accuracy on global train data 39.385, Local accuracy on local train data 93.950


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.5953, device='cuda:0', grad_fn=<MaxBackward1>)
0 3 {8, 6}
1 65 {3, 4}
2 87 {8, 9}
3 31 {0, 1}
4 89 {0, 9}
5 10 {3, 7}
6 70 {8, 5}
7 44 {6, 7}
8 28 {2, 7}
9 41 {0, 3}
Round  24, Devices participated 10, Average loss 0.463, Central accuracy on global test data 51.572, Ensemble accuracy on global test data 72.529, Local accuracy on global train data 35.088, Local accuracy on local train data 94.267


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 18 {0, 6}
2 59 {0, 7}
3 53 {0, 7}
4 99 {8, 9}
5 72 {1, 9}
6 90 {8, 6, 7}
7 14 {9, 5}
8 13 {8, 5, 6}
9 80 {6}
Round  25, Devices participated 10, Average loss 0.359, Central accuracy on global test data 36.855, Ensemble accuracy on global test data 53.252, Local accuracy on global train data 29.165, Local accuracy on local train data 94.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 61 {6}
2 33 {5, 6}
3 12 {8, 9, 3}
4 87 {8, 9}
5 20 {2, 7}
6 3 {8, 6}
7 34 {3, 7}
8 9 {1}
9 63 {4, 5}
Round  26, Devices participated 10, Average loss 0.487, Central accuracy on global test data 37.725, Ensemble accuracy on global test data 65.752, Local accuracy on global train data 36.729, Local accuracy on local train data 93.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 31 {0, 1}
2 28 {2, 7}
3 90 {8, 6, 7}
4 19 {5, 6}
5 26 {9, 7}
6 73 {0, 6}
7 23 {1, 5}
8 11 {4, 5}
9 96 {1, 3}
Round  27, Devices participated 10, Average loss 0.365, Central accuracy on global test data 55.947, Ensemble accuracy on global test data 66.172, Local accuracy on global train data 34.285, Local accuracy on local train data 95.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 55 {4, 7}
2 1 {8, 4}
3 5 {0, 6, 7}
4 93 {9, 6}
5 53 {0, 7}
6 7 {0, 1}
7 96 {1, 3}
8 57 {4, 6}
9 64 {1, 2}
Round  28, Devices participated 10, Average loss 0.374, Central accuracy on global test data 50.332, Ensemble accuracy on global test data 64.746, Local accuracy on global train data 36.680, Local accuracy on local train data 95.033


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 46 {9, 6}
2 99 {8, 9}
3 78 {2, 3, 4}
4 33 {5, 6}
5 79 {0, 8}
6 63 {4, 5}
7 13 {8, 5, 6}
8 26 {9, 7}
9 10 {3, 7}
Round  29, Devices participated 10, Average loss 0.428, Central accuracy on global test data 52.295, Ensemble accuracy on global test data 67.012, Local accuracy on global train data 35.432, Local accuracy on local train data 93.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 21 {2, 5}
2 40 {3, 5}
3 74 {5}
4 49 {2, 7}
5 36 {1, 3}
6 1 {8, 4}
7 24 {4, 5}
8 71 {1, 5}
9 79 {0, 8}
Round  30, Devices participated 10, Average loss 0.370, Central accuracy on global test data 55.234, Ensemble accuracy on global test data 63.516, Local accuracy on global train data 33.347, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 68 {9, 6}
2 43 {8, 3}
3 85 {1, 4}
4 10 {3, 7}
5 2 {9}
6 83 {9, 2}
7 94 {0, 1}
8 0 {1, 2}
9 66 {1, 3}
Round  31, Devices participated 10, Average loss 0.305, Central accuracy on global test data 51.709, Ensemble accuracy on global test data 58.115, Local accuracy on global train data 36.396, Local accuracy on local train data 95.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 16 {0, 3}
2 32 {0, 9}
3 76 {1, 3}
4 4 {8, 7}
5 51 {4, 7}
6 92 {1, 3}
7 33 {5, 6}
8 27 {2, 5}
9 86 {1, 2}
Round  32, Devices participated 10, Average loss 0.432, Central accuracy on global test data 65.459, Ensemble accuracy on global test data 70.127, Local accuracy on global train data 38.748, Local accuracy on local train data 95.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 40 {3, 5}
2 15 {5, 7}
3 16 {0, 3}
4 66 {1, 3}
5 10 {3, 7}
6 87 {8, 9}
7 1 {8, 4}
8 2 {9}
9 41 {0, 3}
Round  33, Devices participated 10, Average loss 0.445, Central accuracy on global test data 65.645, Ensemble accuracy on global test data 70.029, Local accuracy on global train data 36.108, Local accuracy on local train data 93.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 15 {5, 7}
2 38 {2, 7}
3 44 {6, 7}
4 88 {0}
5 56 {9, 2}
6 19 {5, 6}
7 71 {1, 5}
8 29 {9}
9 61 {6}
Round  34, Devices participated 10, Average loss 0.231, Central accuracy on global test data 61.260, Ensemble accuracy on global test data 64.326, Local accuracy on global train data 35.979, Local accuracy on local train data 97.267


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 95 {3, 4}
1 67 {8, 1}
2 46 {9, 6}
3 57 {4, 6}
4 1 {8, 4}
5 27 {2, 5}
6 48 {3, 4}
7 44 {6, 7}
8 16 {0, 3}
9 87 {8, 9}
Round  35, Devices participated 10, Average loss 0.297, Central accuracy on global test data 59.404, Ensemble accuracy on global test data 64.287, Local accuracy on global train data 35.381, Local accuracy on local train data 95.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 0 {1, 2}
2 40 {3, 5}
3 32 {0, 9}
4 39 {8, 0}
5 76 {1, 3}
6 73 {0, 6}
7 79 {0, 8}
8 42 {8, 1}
9 24 {4, 5}
Round  36, Devices participated 10, Average loss 0.306, Central accuracy on global test data 71.660, Ensemble accuracy on global test data 71.885, Local accuracy on global train data 39.775, Local accuracy on local train data 94.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 91 {2, 6}
2 93 {9, 6}
3 36 {1, 3}
4 92 {1, 3}
5 58 {9, 4}
6 73 {0, 6}
7 77 {8, 1}
8 13 {8, 5, 6}
9 35 {8, 2}
Round  37, Devices participated 10, Average loss 0.567, Central accuracy on global test data 69.492, Ensemble accuracy on global test data 69.072, Local accuracy on global train data 42.188, Local accuracy on local train data 93.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 10 {3, 7}
2 35 {8, 2}
3 76 {1, 3}
4 90 {8, 6, 7}
5 41 {0, 3}
6 60 {2}
7 97 {4, 7}
8 19 {5, 6}
9 92 {1, 3}
Round  38, Devices participated 10, Average loss 0.309, Central accuracy on global test data 70.938, Ensemble accuracy on global test data 71.787, Local accuracy on global train data 39.973, Local accuracy on local train data 95.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 56 {9, 2}
2 50 {2, 3}
3 98 {4}
4 88 {0}
5 2 {9}
6 39 {8, 0}
7 22 {0, 5}
8 30 {0, 1, 2}
9 23 {1, 5}
Round  39, Devices participated 10, Average loss 0.327, Central accuracy on global test data 68.389, Ensemble accuracy on global test data 70.654, Local accuracy on global train data 39.138, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 21 {2, 5}
2 87 {8, 9}
3 35 {8, 2}
4 34 {3, 7}
5 5 {0, 6, 7}
6 89 {0, 9}
7 44 {6, 7}
8 17 {3, 4}
9 65 {3, 4}
Round  40, Devices participated 10, Average loss 0.564, Central accuracy on global test data 70.713, Ensemble accuracy on global test data 70.205, Local accuracy on global train data 32.683, Local accuracy on local train data 93.400


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 42 {8, 1}
2 5 {0, 6, 7}
3 15 {5, 7}
4 71 {1, 5}
5 28 {2, 7}
6 12 {8, 9, 3}
7 22 {0, 5}
8 35 {8, 2}
9 14 {9, 5}
Round  41, Devices participated 10, Average loss 0.532, Central accuracy on global test data 69.189, Ensemble accuracy on global test data 68.916, Local accuracy on global train data 44.553, Local accuracy on local train data 92.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 45 {4, 5}
2 55 {4, 7}
3 25 {8, 2}
4 38 {2, 7}
5 91 {2, 6}
6 57 {4, 6}
7 61 {6}
8 83 {9, 2}
9 53 {0, 7}
Round  42, Devices participated 10, Average loss 0.382, Central accuracy on global test data 65.449, Ensemble accuracy on global test data 65.996, Local accuracy on global train data 39.182, Local accuracy on local train data 95.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 42 {8, 1}
2 89 {0, 9}
3 35 {8, 2}
4 69 {2, 3}
5 36 {1, 3}
6 74 {5}
7 53 {0, 7}
8 11 {4, 5}
9 68 {9, 6}
Round  43, Devices participated 10, Average loss 0.335, Central accuracy on global test data 70.957, Ensemble accuracy on global test data 71.357, Local accuracy on global train data 37.065, Local accuracy on local train data 95.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 75 {8, 3}
2 26 {9, 7}
3 31 {0, 1}
4 47 {9, 5}
5 13 {8, 5, 6}
6 36 {1, 3}
7 79 {0, 8}
8 60 {2}
9 12 {8, 9, 3}
Round  44, Devices participated 10, Average loss 0.491, Central accuracy on global test data 73.154, Ensemble accuracy on global test data 73.672, Local accuracy on global train data 43.594, Local accuracy on local train data 93.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 91 {2, 6}
2 12 {8, 9, 3}
3 7 {0, 1}
4 9 {1}
5 23 {1, 5}
6 51 {4, 7}
7 6 {1, 6}
8 43 {8, 3}
9 44 {6, 7}
Round  45, Devices participated 10, Average loss 0.435, Central accuracy on global test data 63.721, Ensemble accuracy on global test data 63.330, Local accuracy on global train data 40.659, Local accuracy on local train data 94.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 78 {2, 3, 4}
2 82 {8, 4}
3 68 {9, 6}
4 92 {1, 3}
5 30 {0, 1, 2}
6 12 {8, 9, 3}
7 58 {9, 4}
8 89 {0, 9}
9 44 {6, 7}
Round  46, Devices participated 10, Average loss 0.418, Central accuracy on global test data 69.404, Ensemble accuracy on global test data 70.840, Local accuracy on global train data 44.138, Local accuracy on local train data 93.967


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 65 {3, 4}
1 80 {6}
2 62 {0, 7}
3 19 {5, 6}
4 51 {4, 7}
5 92 {1, 3}
6 71 {1, 5}
7 48 {3, 4}
8 6 {1, 6}
9 60 {2}
Round  47, Devices participated 10, Average loss 0.226, Central accuracy on global test data 65.234, Ensemble accuracy on global test data 65.576, Local accuracy on global train data 39.517, Local accuracy on local train data 97.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.4944, device='cuda:0', grad_fn=<MaxBackward1>)
0 85 {1, 4}
1 26 {9, 7}
2 5 {0, 6, 7}
3 88 {0}
4 83 {9, 2}
5 62 {0, 7}
6 75 {8, 3}
7 80 {6}
8 87 {8, 9}
9 36 {1, 3}
Round  48, Devices participated 10, Average loss 0.391, Central accuracy on global test data 47.598, Ensemble accuracy on global test data 74.219, Local accuracy on global train data 41.035, Local accuracy on local train data 94.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 16 {0, 3}
1 93 {9, 6}
2 17 {3, 4}
3 77 {8, 1}
4 88 {0}
5 91 {2, 6}
6 73 {0, 6}
7 76 {1, 3}
8 31 {0, 1}
9 47 {9, 5}
Round  49, Devices participated 10, Average loss 0.330, Central accuracy on global test data 38.037, Ensemble accuracy on global test data 69.658, Local accuracy on global train data 44.897, Local accuracy on local train data 96.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 5 {0, 6, 7}
2 24 {4, 5}
3 47 {9, 5}
4 98 {4}
5 15 {5, 7}
6 55 {4, 7}
7 58 {9, 4}
8 95 {3, 4}
9 32 {0, 9}
Round  50, Devices participated 10, Average loss 0.389, Central accuracy on global test data 36.191, Ensemble accuracy on global test data 66.396, Local accuracy on global train data 46.475, Local accuracy on local train data 95.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 11 {4, 5}
1 80 {6}
2 87 {8, 9}
3 70 {8, 5}
4 6 {1, 6}
5 91 {2, 6}
6 51 {4, 7}
7 89 {0, 9}
8 42 {8, 1}
9 62 {0, 7}
Round  51, Devices participated 10, Average loss 0.417, Central accuracy on global test data 31.318, Ensemble accuracy on global test data 73.623, Local accuracy on global train data 42.622, Local accuracy on local train data 95.283


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 11 {4, 5}
2 16 {0, 3}
3 81 {2, 3, 7}
4 13 {8, 5, 6}
5 25 {8, 2}
6 33 {5, 6}
7 92 {1, 3}
8 41 {0, 3}
9 48 {3, 4}
Round  52, Devices participated 10, Average loss 0.526, Central accuracy on global test data 40.566, Ensemble accuracy on global test data 60.879, Local accuracy on global train data 43.181, Local accuracy on local train data 93.533


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 99 {8, 9}
2 22 {0, 5}
3 33 {5, 6}
4 7 {0, 1}
5 8 {8, 7}
6 49 {2, 7}
7 23 {1, 5}
8 79 {0, 8}
9 15 {5, 7}
Round  53, Devices participated 10, Average loss 0.437, Central accuracy on global test data 52.686, Ensemble accuracy on global test data 67.129, Local accuracy on global train data 39.612, Local accuracy on local train data 94.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 46 {9, 6}
2 23 {1, 5}
3 84 {0, 9}
4 27 {2, 5}
5 4 {8, 7}
6 32 {0, 9}
7 64 {1, 2}
8 44 {6, 7}
9 79 {0, 8}
Round  54, Devices participated 10, Average loss 0.310, Central accuracy on global test data 54.453, Ensemble accuracy on global test data 68.457, Local accuracy on global train data 38.088, Local accuracy on local train data 95.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 55 {4, 7}
2 23 {1, 5}
3 56 {9, 2}
4 21 {2, 5}
5 52 {8, 7}
6 17 {3, 4}
7 57 {4, 6}
8 99 {8, 9}
9 71 {1, 5}
Round  55, Devices participated 10, Average loss 0.410, Central accuracy on global test data 58.916, Ensemble accuracy on global test data 74.424, Local accuracy on global train data 35.364, Local accuracy on local train data 94.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 43 {8, 3}
2 85 {1, 4}
3 22 {0, 5}
4 84 {0, 9}
5 0 {1, 2}
6 78 {2, 3, 4}
7 24 {4, 5}
8 65 {3, 4}
9 44 {6, 7}
Round  56, Devices participated 10, Average loss 0.390, Central accuracy on global test data 56.543, Ensemble accuracy on global test data 64.941, Local accuracy on global train data 44.294, Local accuracy on local train data 94.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 49 {2, 7}
2 0 {1, 2}
3 98 {4}
4 24 {4, 5}
5 42 {8, 1}
6 31 {0, 1}
7 57 {4, 6}
8 67 {8, 1}
9 19 {5, 6}
Round  57, Devices participated 10, Average loss 0.421, Central accuracy on global test data 58.613, Ensemble accuracy on global test data 67.285, Local accuracy on global train data 40.334, Local accuracy on local train data 94.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 50 {2, 3}
2 82 {8, 4}
3 54 {1, 6}
4 60 {2}
5 74 {5}
6 75 {8, 3}
7 26 {9, 7}
8 7 {0, 1}
9 1 {8, 4}
Round  58, Devices participated 10, Average loss 0.332, Central accuracy on global test data 67.676, Ensemble accuracy on global test data 69.326, Local accuracy on global train data 38.728, Local accuracy on local train data 94.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 79 {0, 8}
2 92 {1, 3}
3 43 {8, 3}
4 17 {3, 4}
5 66 {1, 3}
6 19 {5, 6}
7 51 {4, 7}
8 46 {9, 6}
9 32 {0, 9}
Round  59, Devices participated 10, Average loss 0.337, Central accuracy on global test data 52.158, Ensemble accuracy on global test data 52.266, Local accuracy on global train data 38.923, Local accuracy on local train data 93.950


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 19 {5, 6}
2 42 {8, 1}
3 21 {2, 5}
4 15 {5, 7}
5 50 {2, 3}
6 35 {8, 2}
7 81 {2, 3, 7}
8 65 {3, 4}
9 95 {3, 4}
Round  60, Devices participated 10, Average loss 0.534, Central accuracy on global test data 59.414, Ensemble accuracy on global test data 60.195, Local accuracy on global train data 38.293, Local accuracy on local train data 93.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 29 {9}
2 19 {5, 6}
3 60 {2}
4 35 {8, 2}
5 25 {8, 2}
6 93 {9, 6}
7 24 {4, 5}
8 18 {0, 6}
9 85 {1, 4}
Round  61, Devices participated 10, Average loss 0.494, Central accuracy on global test data 65.850, Ensemble accuracy on global test data 66.982, Local accuracy on global train data 40.271, Local accuracy on local train data 94.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 20 {2, 7}
1 78 {2, 3, 4}
2 96 {1, 3}
3 8 {8, 7}
4 39 {8, 0}
5 71 {1, 5}
6 28 {2, 7}
7 9 {1}
8 32 {0, 9}
9 29 {9}
Round  62, Devices participated 10, Average loss 0.415, Central accuracy on global test data 74.600, Ensemble accuracy on global test data 74.521, Local accuracy on global train data 41.357, Local accuracy on local train data 94.967


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 82 {8, 4}
1 17 {3, 4}
2 75 {8, 3}
3 68 {9, 6}
4 15 {5, 7}
5 33 {5, 6}
6 22 {0, 5}
7 23 {1, 5}
8 38 {2, 7}
9 90 {8, 6, 7}
Round  63, Devices participated 10, Average loss 0.346, Central accuracy on global test data 71.445, Ensemble accuracy on global test data 70.967, Local accuracy on global train data 42.285, Local accuracy on local train data 94.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.6850, device='cuda:0', grad_fn=<MaxBackward1>)
0 87 {8, 9}
1 59 {0, 7}
2 39 {8, 0}
3 9 {1}
4 35 {8, 2}
5 73 {0, 6}
6 30 {0, 1, 2}
7 75 {8, 3}
8 60 {2}
9 4 {8, 7}
Round  64, Devices participated 10, Average loss 0.447, Central accuracy on global test data 63.213, Ensemble accuracy on global test data 66.943, Local accuracy on global train data 42.300, Local accuracy on local train data 93.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {3, 5}
1 97 {4, 7}
2 6 {1, 6}
3 12 {8, 9, 3}
4 13 {8, 5, 6}
5 98 {4}
6 70 {8, 5}
7 9 {1}
8 5 {0, 6, 7}
9 77 {8, 1}
Round  65, Devices participated 10, Average loss 0.486, Central accuracy on global test data 67.832, Ensemble accuracy on global test data 71.328, Local accuracy on global train data 44.470, Local accuracy on local train data 92.883


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 93 {9, 6}
2 88 {0}
3 16 {0, 3}
4 90 {8, 6, 7}
5 26 {9, 7}
6 49 {2, 7}
7 44 {6, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round  66, Devices participated 10, Average loss 0.478, Central accuracy on global test data 62.432, Ensemble accuracy on global test data 67.773, Local accuracy on global train data 43.467, Local accuracy on local train data 94.517


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {2, 3, 4}
1 11 {4, 5}
2 52 {8, 7}
3 82 {8, 4}
4 7 {0, 1}
5 62 {0, 7}
6 38 {2, 7}
7 41 {0, 3}
8 46 {9, 6}
9 13 {8, 5, 6}
Round  67, Devices participated 10, Average loss 0.539, Central accuracy on global test data 68.408, Ensemble accuracy on global test data 71.797, Local accuracy on global train data 43.235, Local accuracy on local train data 94.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {9, 2}
1 85 {1, 4}
2 24 {4, 5}
3 89 {0, 9}
4 44 {6, 7}
5 18 {0, 6}
6 97 {4, 7}
7 15 {5, 7}
8 6 {1, 6}
9 3 {8, 6}
Round  68, Devices participated 10, Average loss 0.319, Central accuracy on global test data 66.475, Ensemble accuracy on global test data 68.857, Local accuracy on global train data 40.259, Local accuracy on local train data 95.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {9, 5}
1 19 {5, 6}
2 62 {0, 7}
3 61 {6}
4 60 {2}
5 2 {9}
6 87 {8, 9}
7 16 {0, 3}
8 7 {0, 1}
9 89 {0, 9}
Round  69, Devices participated 10, Average loss 0.387, Central accuracy on global test data 76.025, Ensemble accuracy on global test data 76.846, Local accuracy on global train data 41.919, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 70 {8, 5}
3 26 {9, 7}
4 48 {3, 4}
5 76 {1, 3}
6 97 {4, 7}
7 71 {1, 5}
8 59 {0, 7}
9 30 {0, 1, 2}
Round  70, Devices participated 10, Average loss 0.571, Central accuracy on global test data 67.861, Ensemble accuracy on global test data 67.881, Local accuracy on global train data 44.126, Local accuracy on local train data 93.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 49 {2, 7}
1 2 {9}
2 29 {9}
3 96 {1, 3}
4 71 {1, 5}
5 18 {0, 6}
6 88 {0}
7 40 {3, 5}
8 59 {0, 7}
9 65 {3, 4}
Round  71, Devices participated 10, Average loss 0.357, Central accuracy on global test data 67.363, Ensemble accuracy on global test data 68.066, Local accuracy on global train data 46.362, Local accuracy on local train data 96.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 31 {0, 1}
2 50 {2, 3}
3 38 {2, 7}
4 33 {5, 6}
5 51 {4, 7}
6 91 {2, 6}
7 68 {9, 6}
8 81 {2, 3, 7}
9 30 {0, 1, 2}
Round  72, Devices participated 10, Average loss 0.550, Central accuracy on global test data 66.416, Ensemble accuracy on global test data 66.865, Local accuracy on global train data 44.866, Local accuracy on local train data 94.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 18 {0, 6}
2 65 {3, 4}
3 32 {0, 9}
4 7 {0, 1}
5 92 {1, 3}
6 24 {4, 5}
7 86 {1, 2}
8 53 {0, 7}
9 67 {8, 1}
Round  73, Devices participated 10, Average loss 0.324, Central accuracy on global test data 68.066, Ensemble accuracy on global test data 68.213, Local accuracy on global train data 45.664, Local accuracy on local train data 96.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 86 {1, 2}
2 64 {1, 2}
3 57 {4, 6}
4 67 {8, 1}
5 94 {0, 1}
6 3 {8, 6}
7 19 {5, 6}
8 8 {8, 7}
9 69 {2, 3}
Round  74, Devices participated 10, Average loss 0.565, Central accuracy on global test data 65.596, Ensemble accuracy on global test data 68.018, Local accuracy on global train data 44.368, Local accuracy on local train data 94.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 26 {9, 7}
2 37 {4, 5, 7}
3 90 {8, 6, 7}
4 47 {9, 5}
5 89 {0, 9}
6 16 {0, 3}
7 65 {3, 4}
8 36 {1, 3}
9 61 {6}
Round  75, Devices participated 10, Average loss 0.446, Central accuracy on global test data 72.764, Ensemble accuracy on global test data 73.545, Local accuracy on global train data 45.342, Local accuracy on local train data 94.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {2, 5}
1 63 {4, 5}
2 77 {8, 1}
3 16 {0, 3}
4 36 {1, 3}
5 28 {2, 7}
6 59 {0, 7}
7 12 {8, 9, 3}
8 97 {4, 7}
9 55 {4, 7}
Round  76, Devices participated 10, Average loss 0.499, Central accuracy on global test data 68.828, Ensemble accuracy on global test data 68.291, Local accuracy on global train data 46.633, Local accuracy on local train data 94.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 55 {4, 7}
2 14 {9, 5}
3 80 {6}
4 74 {5}
5 25 {8, 2}
6 26 {9, 7}
7 57 {4, 6}
8 41 {0, 3}
9 22 {0, 5}
Round  77, Devices participated 10, Average loss 0.424, Central accuracy on global test data 70.117, Ensemble accuracy on global test data 69.727, Local accuracy on global train data 41.094, Local accuracy on local train data 94.883


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 79 {0, 8}
2 86 {1, 2}
3 13 {8, 5, 6}
4 21 {2, 5}
5 32 {0, 9}
6 95 {3, 4}
7 52 {8, 7}
8 27 {2, 5}
9 47 {9, 5}
Round  78, Devices participated 10, Average loss 0.471, Central accuracy on global test data 69.375, Ensemble accuracy on global test data 70.752, Local accuracy on global train data 43.030, Local accuracy on local train data 95.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 42 {8, 1}
2 0 {1, 2}
3 39 {8, 0}
4 54 {1, 6}
5 16 {0, 3}
6 33 {5, 6}
7 65 {3, 4}
8 83 {9, 2}
9 5 {0, 6, 7}
Round  79, Devices participated 10, Average loss 0.447, Central accuracy on global test data 72.158, Ensemble accuracy on global test data 71.885, Local accuracy on global train data 44.119, Local accuracy on local train data 95.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 19 {5, 6}
2 70 {8, 5}
3 91 {2, 6}
4 10 {3, 7}
5 86 {1, 2}
6 78 {2, 3, 4}
7 81 {2, 3, 7}
8 97 {4, 7}
9 22 {0, 5}
Round  80, Devices participated 10, Average loss 0.674, Central accuracy on global test data 69.746, Ensemble accuracy on global test data 68.818, Local accuracy on global train data 47.913, Local accuracy on local train data 93.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 12 {8, 9, 3}
1 79 {0, 8}
2 66 {1, 3}
3 20 {2, 7}
4 2 {9}
5 13 {8, 5, 6}
6 6 {1, 6}
7 19 {5, 6}
8 91 {2, 6}
9 29 {9}
Round  81, Devices participated 10, Average loss 0.454, Central accuracy on global test data 73.467, Ensemble accuracy on global test data 72.012, Local accuracy on global train data 42.361, Local accuracy on local train data 95.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 10 {3, 7}
2 21 {2, 5}
3 83 {9, 2}
4 59 {0, 7}
5 3 {8, 6}
6 46 {9, 6}
7 0 {1, 2}
8 16 {0, 3}
9 94 {0, 1}
Round  82, Devices participated 10, Average loss 0.433, Central accuracy on global test data 70.176, Ensemble accuracy on global test data 70.195, Local accuracy on global train data 45.576, Local accuracy on local train data 96.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {2, 5}
1 96 {1, 3}
2 36 {1, 3}
3 17 {3, 4}
4 74 {5}
5 46 {9, 6}
6 73 {0, 6}
7 80 {6}
8 20 {2, 7}
9 40 {3, 5}
Round  83, Devices participated 10, Average loss 0.462, Central accuracy on global test data 65.869, Ensemble accuracy on global test data 66.025, Local accuracy on global train data 41.477, Local accuracy on local train data 95.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 81 {2, 3, 7}
2 10 {3, 7}
3 91 {2, 6}
4 31 {0, 1}
5 4 {8, 7}
6 42 {8, 1}
7 26 {9, 7}
8 77 {8, 1}
9 99 {8, 9}
Round  84, Devices participated 10, Average loss 0.461, Central accuracy on global test data 66.836, Ensemble accuracy on global test data 66.914, Local accuracy on global train data 47.180, Local accuracy on local train data 94.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 78 {2, 3, 4}
2 93 {9, 6}
3 75 {8, 3}
4 31 {0, 1}
5 86 {1, 2}
6 7 {0, 1}
7 17 {3, 4}
8 77 {8, 1}
9 28 {2, 7}
Round  85, Devices participated 10, Average loss 0.482, Central accuracy on global test data 71.064, Ensemble accuracy on global test data 71.035, Local accuracy on global train data 48.027, Local accuracy on local train data 95.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 22 {0, 5}
2 44 {6, 7}
3 18 {0, 6}
4 68 {9, 6}
5 34 {3, 7}
6 91 {2, 6}
7 98 {4}
8 84 {0, 9}
9 83 {9, 2}
Round  86, Devices participated 10, Average loss 0.388, Central accuracy on global test data 69.336, Ensemble accuracy on global test data 69.844, Local accuracy on global train data 47.090, Local accuracy on local train data 95.967


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 59 {0, 7}
2 51 {4, 7}
3 42 {8, 1}
4 49 {2, 7}
5 69 {2, 3}
6 52 {8, 7}
7 37 {4, 5, 7}
8 2 {9}
9 4 {8, 7}
Round  87, Devices participated 10, Average loss 0.382, Central accuracy on global test data 69.697, Ensemble accuracy on global test data 69.980, Local accuracy on global train data 44.890, Local accuracy on local train data 95.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 10 {3, 7}
2 27 {2, 5}
3 34 {3, 7}
4 7 {0, 1}
5 76 {1, 3}
6 74 {5}
7 3 {8, 6}
8 59 {0, 7}
9 52 {8, 7}
Round  88, Devices participated 10, Average loss 0.311, Central accuracy on global test data 72.549, Ensemble accuracy on global test data 72.520, Local accuracy on global train data 43.491, Local accuracy on local train data 96.567


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {6, 7}
1 9 {1}
2 73 {0, 6}
3 3 {8, 6}
4 55 {4, 7}
5 81 {2, 3, 7}
6 17 {3, 4}
7 59 {0, 7}
8 97 {4, 7}
9 63 {4, 5}
Round  89, Devices participated 10, Average loss 0.315, Central accuracy on global test data 71.074, Ensemble accuracy on global test data 71.270, Local accuracy on global train data 51.355, Local accuracy on local train data 96.517


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 63 {4, 5}
1 0 {1, 2}
2 33 {5, 6}
3 88 {0}
4 93 {9, 6}
5 87 {8, 9}
6 72 {1, 9}
7 84 {0, 9}
8 86 {1, 2}
9 20 {2, 7}
Round  90, Devices participated 10, Average loss 0.488, Central accuracy on global test data 74.023, Ensemble accuracy on global test data 76.006, Local accuracy on global train data 39.849, Local accuracy on local train data 95.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 19 {5, 6}
1 95 {3, 4}
2 33 {5, 6}
3 92 {1, 3}
4 65 {3, 4}
5 69 {2, 3}
6 81 {2, 3, 7}
7 26 {9, 7}
8 14 {9, 5}
9 52 {8, 7}
Round  91, Devices participated 10, Average loss 0.631, Central accuracy on global test data 61.875, Ensemble accuracy on global test data 62.041, Local accuracy on global train data 45.610, Local accuracy on local train data 93.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 22 {0, 5}
1 62 {0, 7}
2 3 {8, 6}
3 14 {9, 5}
4 64 {1, 2}
5 96 {1, 3}
6 33 {5, 6}
7 97 {4, 7}
8 31 {0, 1}
9 42 {8, 1}
Round  92, Devices participated 10, Average loss 0.388, Central accuracy on global test data 74.199, Ensemble accuracy on global test data 73.779, Local accuracy on global train data 49.578, Local accuracy on local train data 95.883


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 37 {4, 5, 7}
1 10 {3, 7}
2 88 {0}
3 63 {4, 5}
4 53 {0, 7}
5 68 {9, 6}
6 2 {9}
7 52 {8, 7}
8 26 {9, 7}
9 59 {0, 7}
Round  93, Devices participated 10, Average loss 0.353, Central accuracy on global test data 65.068, Ensemble accuracy on global test data 67.080, Local accuracy on global train data 45.613, Local accuracy on local train data 95.883


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 71 {1, 5}
2 85 {1, 4}
3 49 {2, 7}
4 20 {2, 7}
5 10 {3, 7}
6 3 {8, 6}
7 86 {1, 2}
8 88 {0}
9 33 {5, 6}
Round  94, Devices participated 10, Average loss 0.346, Central accuracy on global test data 71.523, Ensemble accuracy on global test data 71.953, Local accuracy on global train data 46.172, Local accuracy on local train data 96.733


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {1}
1 0 {1, 2}
2 18 {0, 6}
3 66 {1, 3}
4 44 {6, 7}
5 17 {3, 4}
6 92 {1, 3}
7 95 {3, 4}
8 64 {1, 2}
9 96 {1, 3}
Round  95, Devices participated 10, Average loss 0.282, Central accuracy on global test data 66.201, Ensemble accuracy on global test data 64.932, Local accuracy on global train data 48.386, Local accuracy on local train data 97.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {8, 5}
1 58 {9, 4}
2 24 {4, 5}
3 57 {4, 6}
4 32 {0, 9}
5 19 {5, 6}
6 43 {8, 3}
7 65 {3, 4}
8 5 {0, 6, 7}
9 88 {0}
Round  96, Devices participated 10, Average loss 0.484, Central accuracy on global test data 72.783, Ensemble accuracy on global test data 70.986, Local accuracy on global train data 45.942, Local accuracy on local train data 92.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {6}
1 75 {8, 3}
2 68 {9, 6}
3 2 {9}
4 6 {1, 6}
5 26 {9, 7}
6 1 {8, 4}
7 56 {9, 2}
8 78 {2, 3, 4}
9 74 {5}
Round  97, Devices participated 10, Average loss 0.536, Central accuracy on global test data 74.062, Ensemble accuracy on global test data 76.035, Local accuracy on global train data 44.504, Local accuracy on local train data 94.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 57 {4, 6}
2 22 {0, 5}
3 90 {8, 6, 7}
4 46 {9, 6}
5 9 {1}
6 56 {9, 2}
7 99 {8, 9}
8 89 {0, 9}
9 4 {8, 7}
Round  98, Devices participated 10, Average loss 0.404, Central accuracy on global test data 74.375, Ensemble accuracy on global test data 73.330, Local accuracy on global train data 46.882, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {9, 2}
1 41 {0, 3}
2 69 {2, 3}
3 7 {0, 1}
4 63 {4, 5}
5 51 {4, 7}
6 78 {2, 3, 4}
7 89 {0, 9}
8 43 {8, 3}
9 81 {2, 3, 7}
Round  99, Devices participated 10, Average loss 0.548, Central accuracy on global test data 63.926, Ensemble accuracy on global test data 64.082, Local accuracy on global train data 48.020, Local accuracy on local train data 93.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {5}
1 76 {1, 3}
2 14 {9, 5}
3 98 {4}
4 73 {0, 6}
5 37 {4, 5, 7}
6 94 {0, 1}
7 68 {9, 6}
8 39 {8, 0}
9 11 {4, 5}
Round 100, Devices participated 10, Average loss 0.314, Central accuracy on global test data 71.006, Ensemble accuracy on global test data 70.938, Local accuracy on global train data 48.445, Local accuracy on local train data 96.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 38 {2, 7}
1 78 {2, 3, 4}
2 17 {3, 4}
3 56 {9, 2}
4 90 {8, 6, 7}
5 34 {3, 7}
6 42 {8, 1}
7 0 {1, 2}
8 1 {8, 4}
9 12 {8, 9, 3}
Round 101, Devices participated 10, Average loss 0.448, Central accuracy on global test data 72.041, Ensemble accuracy on global test data 72.412, Local accuracy on global train data 51.147, Local accuracy on local train data 94.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 36 {1, 3}
2 95 {3, 4}
3 8 {8, 7}
4 99 {8, 9}
5 75 {8, 3}
6 93 {9, 6}
7 4 {8, 7}
8 24 {4, 5}
9 58 {9, 4}
Round 102, Devices participated 10, Average loss 0.398, Central accuracy on global test data 71.035, Ensemble accuracy on global test data 72.588, Local accuracy on global train data 46.870, Local accuracy on local train data 93.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 66 {1, 3}
1 46 {9, 6}
2 89 {0, 9}
3 23 {1, 5}
4 4 {8, 7}
5 97 {4, 7}
6 78 {2, 3, 4}
7 26 {9, 7}
8 79 {0, 8}
9 52 {8, 7}
Round 103, Devices participated 10, Average loss 0.345, Central accuracy on global test data 70.498, Ensemble accuracy on global test data 71.191, Local accuracy on global train data 47.593, Local accuracy on local train data 95.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 79 {0, 8}
2 23 {1, 5}
3 7 {0, 1}
4 97 {4, 7}
5 86 {1, 2}
6 72 {1, 9}
7 57 {4, 6}
8 70 {8, 5}
9 64 {1, 2}
Round 104, Devices participated 10, Average loss 0.345, Central accuracy on global test data 72.021, Ensemble accuracy on global test data 72.139, Local accuracy on global train data 49.363, Local accuracy on local train data 95.983


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 55 {4, 7}
1 14 {9, 5}
2 21 {2, 5}
3 43 {8, 3}
4 5 {0, 6, 7}
5 30 {0, 1, 2}
6 35 {8, 2}
7 15 {5, 7}
8 28 {2, 7}
9 34 {3, 7}
Round 105, Devices participated 10, Average loss 0.536, Central accuracy on global test data 70.928, Ensemble accuracy on global test data 68.965, Local accuracy on global train data 48.584, Local accuracy on local train data 94.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 51 {4, 7}
2 22 {0, 5}
3 85 {1, 4}
4 73 {0, 6}
5 90 {8, 6, 7}
6 4 {8, 7}
7 69 {2, 3}
8 75 {8, 3}
9 6 {1, 6}
Round 106, Devices participated 10, Average loss 0.322, Central accuracy on global test data 72.334, Ensemble accuracy on global test data 72.881, Local accuracy on global train data 55.220, Local accuracy on local train data 95.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {2, 3, 4}
1 98 {4}
2 48 {3, 4}
3 28 {2, 7}
4 76 {1, 3}
5 96 {1, 3}
6 97 {4, 7}
7 69 {2, 3}
8 66 {1, 3}
9 65 {3, 4}
Round 107, Devices participated 10, Average loss 0.337, Central accuracy on global test data 59.883, Ensemble accuracy on global test data 57.656, Local accuracy on global train data 47.600, Local accuracy on local train data 95.733


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 54 {1, 6}
2 70 {8, 5}
3 39 {8, 0}
4 89 {0, 9}
5 14 {9, 5}
6 77 {8, 1}
7 74 {5}
8 15 {5, 7}
9 88 {0}
Round 108, Devices participated 10, Average loss 0.306, Central accuracy on global test data 68.662, Ensemble accuracy on global test data 66.592, Local accuracy on global train data 45.879, Local accuracy on local train data 95.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 12 {8, 9, 3}
2 87 {8, 9}
3 84 {0, 9}
4 91 {2, 6}
5 62 {0, 7}
6 82 {8, 4}
7 57 {4, 6}
8 6 {1, 6}
9 21 {2, 5}
Round 109, Devices participated 10, Average loss 0.393, Central accuracy on global test data 72.920, Ensemble accuracy on global test data 77.061, Local accuracy on global train data 50.449, Local accuracy on local train data 94.417


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 99 {8, 9}
2 54 {1, 6}
3 55 {4, 7}
4 14 {9, 5}
5 63 {4, 5}
6 7 {0, 1}
7 88 {0}
8 91 {2, 6}
9 52 {8, 7}
Round 110, Devices participated 10, Average loss 0.263, Central accuracy on global test data 72.939, Ensemble accuracy on global test data 73.594, Local accuracy on global train data 50.945, Local accuracy on local train data 96.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {6, 7}
1 73 {0, 6}
2 39 {8, 0}
3 28 {2, 7}
4 18 {0, 6}
5 92 {1, 3}
6 52 {8, 7}
7 79 {0, 8}
8 54 {1, 6}
9 77 {8, 1}
Round 111, Devices participated 10, Average loss 0.267, Central accuracy on global test data 70.996, Ensemble accuracy on global test data 70.527, Local accuracy on global train data 50.342, Local accuracy on local train data 96.717


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 70 {8, 5}
2 54 {1, 6}
3 27 {2, 5}
4 89 {0, 9}
5 21 {2, 5}
6 42 {8, 1}
7 25 {8, 2}
8 60 {2}
9 49 {2, 7}
Round 112, Devices participated 10, Average loss 0.531, Central accuracy on global test data 65.166, Ensemble accuracy on global test data 65.752, Local accuracy on global train data 46.755, Local accuracy on local train data 95.233


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 56 {9, 2}
2 59 {0, 7}
3 80 {6}
4 62 {0, 7}
5 19 {5, 6}
6 42 {8, 1}
7 33 {5, 6}
8 72 {1, 9}
9 35 {8, 2}
Round 113, Devices participated 10, Average loss 0.348, Central accuracy on global test data 76.260, Ensemble accuracy on global test data 76.621, Local accuracy on global train data 49.316, Local accuracy on local train data 95.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 53 {0, 7}
1 17 {3, 4}
2 11 {4, 5}
3 78 {2, 3, 4}
4 66 {1, 3}
5 39 {8, 0}
6 94 {0, 1}
7 4 {8, 7}
8 47 {9, 5}
9 90 {8, 6, 7}
Round 114, Devices participated 10, Average loss 0.262, Central accuracy on global test data 70.918, Ensemble accuracy on global test data 71.162, Local accuracy on global train data 48.970, Local accuracy on local train data 96.400


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 48 {3, 4}
1 51 {4, 7}
2 34 {3, 7}
3 78 {2, 3, 4}
4 92 {1, 3}
5 54 {1, 6}
6 96 {1, 3}
7 84 {0, 9}
8 88 {0}
9 74 {5}
Round 115, Devices participated 10, Average loss 0.221, Central accuracy on global test data 68.203, Ensemble accuracy on global test data 66.631, Local accuracy on global train data 50.876, Local accuracy on local train data 96.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 15 {5, 7}
1 25 {8, 2}
2 50 {2, 3}
3 89 {0, 9}
4 72 {1, 9}
5 20 {2, 7}
6 85 {1, 4}
7 47 {9, 5}
8 1 {8, 4}
9 3 {8, 6}
Round 116, Devices participated 10, Average loss 0.436, Central accuracy on global test data 73.496, Ensemble accuracy on global test data 73.945, Local accuracy on global train data 48.096, Local accuracy on local train data 95.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 91 {2, 6}
1 39 {8, 0}
2 11 {4, 5}
3 12 {8, 9, 3}
4 93 {9, 6}
5 64 {1, 2}
6 30 {0, 1, 2}
7 79 {0, 8}
8 29 {9}
9 49 {2, 7}
Round 117, Devices participated 10, Average loss 0.407, Central accuracy on global test data 78.936, Ensemble accuracy on global test data 79.717, Local accuracy on global train data 48.376, Local accuracy on local train data 94.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 88 {0}
2 95 {3, 4}
3 65 {3, 4}
4 60 {2}
5 8 {8, 7}
6 46 {9, 6}
7 31 {0, 1}
8 63 {4, 5}
9 72 {1, 9}
Round 118, Devices participated 10, Average loss 0.206, Central accuracy on global test data 70.449, Ensemble accuracy on global test data 71.133, Local accuracy on global train data 45.254, Local accuracy on local train data 97.200


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 63 {4, 5}
2 58 {9, 4}
3 77 {8, 1}
4 95 {3, 4}
5 23 {1, 5}
6 61 {6}
7 29 {9}
8 91 {2, 6}
9 59 {0, 7}
Round 119, Devices participated 10, Average loss 0.253, Central accuracy on global test data 74.023, Ensemble accuracy on global test data 75.967, Local accuracy on global train data 52.141, Local accuracy on local train data 95.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 69 {2, 3}
2 44 {6, 7}
3 89 {0, 9}
4 49 {2, 7}
5 36 {1, 3}
6 77 {8, 1}
7 19 {5, 6}
8 92 {1, 3}
9 72 {1, 9}
Round 120, Devices participated 10, Average loss 0.337, Central accuracy on global test data 70.361, Ensemble accuracy on global test data 69.619, Local accuracy on global train data 45.161, Local accuracy on local train data 95.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 49 {2, 7}
2 1 {8, 4}
3 80 {6}
4 27 {2, 5}
5 9 {1}
6 83 {9, 2}
7 72 {1, 9}
8 76 {1, 3}
9 92 {1, 3}
Round 121, Devices participated 10, Average loss 0.275, Central accuracy on global test data 75.449, Ensemble accuracy on global test data 77.061, Local accuracy on global train data 47.166, Local accuracy on local train data 96.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 57 {4, 6}
2 54 {1, 6}
3 1 {8, 4}
4 80 {6}
5 76 {1, 3}
6 56 {9, 2}
7 39 {8, 0}
8 14 {9, 5}
9 20 {2, 7}
Round 122, Devices participated 10, Average loss 0.286, Central accuracy on global test data 77.812, Ensemble accuracy on global test data 77.959, Local accuracy on global train data 48.877, Local accuracy on local train data 96.033


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 6 {1, 6}
1 62 {0, 7}
2 31 {0, 1}
3 90 {8, 6, 7}
4 3 {8, 6}
5 51 {4, 7}
6 24 {4, 5}
7 91 {2, 6}
8 23 {1, 5}
9 55 {4, 7}
Round 123, Devices participated 10, Average loss 0.207, Central accuracy on global test data 71.875, Ensemble accuracy on global test data 71.523, Local accuracy on global train data 53.335, Local accuracy on local train data 96.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 18 {0, 6}
2 78 {2, 3, 4}
3 89 {0, 9}
4 27 {2, 5}
5 80 {6}
6 33 {5, 6}
7 86 {1, 2}
8 88 {0}
9 15 {5, 7}
Round 124, Devices participated 10, Average loss 0.252, Central accuracy on global test data 69.180, Ensemble accuracy on global test data 69.746, Local accuracy on global train data 47.585, Local accuracy on local train data 96.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 12 {8, 9, 3}
2 59 {0, 7}
3 60 {2}
4 73 {0, 6}
5 80 {6}
6 0 {1, 2}
7 10 {3, 7}
8 23 {1, 5}
9 13 {8, 5, 6}
Round 125, Devices participated 10, Average loss 0.295, Central accuracy on global test data 73.623, Ensemble accuracy on global test data 74.297, Local accuracy on global train data 49.631, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 46 {9, 6}
2 54 {1, 6}
3 87 {8, 9}
4 84 {0, 9}
5 51 {4, 7}
6 27 {2, 5}
7 95 {3, 4}
8 21 {2, 5}
9 61 {6}
Round 126, Devices participated 10, Average loss 0.290, Central accuracy on global test data 77.930, Ensemble accuracy on global test data 78.994, Local accuracy on global train data 47.737, Local accuracy on local train data 95.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 6 {1, 6}
2 96 {1, 3}
3 98 {4}
4 27 {2, 5}
5 71 {1, 5}
6 18 {0, 6}
7 3 {8, 6}
8 1 {8, 4}
9 95 {3, 4}
Round 127, Devices participated 10, Average loss 0.231, Central accuracy on global test data 74.043, Ensemble accuracy on global test data 74.346, Local accuracy on global train data 49.263, Local accuracy on local train data 97.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 88 {0}
1 90 {8, 6, 7}
2 96 {1, 3}
3 35 {8, 2}
4 69 {2, 3}
5 11 {4, 5}
6 60 {2}
7 48 {3, 4}
8 23 {1, 5}
9 42 {8, 1}
Round 128, Devices participated 10, Average loss 0.303, Central accuracy on global test data 73.125, Ensemble accuracy on global test data 72.832, Local accuracy on global train data 46.184, Local accuracy on local train data 96.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 88 {0}
2 43 {8, 3}
3 57 {4, 6}
4 48 {3, 4}
5 99 {8, 9}
6 63 {4, 5}
7 53 {0, 7}
8 94 {0, 1}
9 55 {4, 7}
Round 129, Devices participated 10, Average loss 0.273, Central accuracy on global test data 70.283, Ensemble accuracy on global test data 71.201, Local accuracy on global train data 53.071, Local accuracy on local train data 95.567


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 75 {8, 3}
2 74 {5}
3 76 {1, 3}
4 39 {8, 0}
5 80 {6}
6 34 {3, 7}
7 12 {8, 9, 3}
8 60 {2}
9 20 {2, 7}
Round 130, Devices participated 10, Average loss 0.281, Central accuracy on global test data 70.752, Ensemble accuracy on global test data 70.527, Local accuracy on global train data 46.519, Local accuracy on local train data 95.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 20 {2, 7}
1 47 {9, 5}
2 38 {2, 7}
3 59 {0, 7}
4 64 {1, 2}
5 82 {8, 4}
6 60 {2}
7 45 {4, 5}
8 63 {4, 5}
9 85 {1, 4}
Round 131, Devices participated 10, Average loss 0.275, Central accuracy on global test data 70.137, Ensemble accuracy on global test data 69.854, Local accuracy on global train data 46.311, Local accuracy on local train data 96.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 6 {1, 6}
2 51 {4, 7}
3 4 {8, 7}
4 8 {8, 7}
5 22 {0, 5}
6 72 {1, 9}
7 86 {1, 2}
8 55 {4, 7}
9 12 {8, 9, 3}
Round 132, Devices participated 10, Average loss 0.325, Central accuracy on global test data 75.967, Ensemble accuracy on global test data 78.867, Local accuracy on global train data 51.748, Local accuracy on local train data 95.100


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 71 {1, 5}
2 84 {0, 9}
3 65 {3, 4}
4 61 {6}
5 83 {9, 2}
6 87 {8, 9}
7 53 {0, 7}
8 77 {8, 1}
9 51 {4, 7}
Round 133, Devices participated 10, Average loss 0.255, Central accuracy on global test data 75.840, Ensemble accuracy on global test data 76.338, Local accuracy on global train data 48.140, Local accuracy on local train data 96.383


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 71 {1, 5}
2 56 {9, 2}
3 63 {4, 5}
4 49 {2, 7}
5 44 {6, 7}
6 85 {1, 4}
7 76 {1, 3}
8 50 {2, 3}
9 93 {9, 6}
Round 134, Devices participated 10, Average loss 0.330, Central accuracy on global test data 77.129, Ensemble accuracy on global test data 77.578, Local accuracy on global train data 48.398, Local accuracy on local train data 95.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 52 {8, 7}
2 95 {3, 4}
3 68 {9, 6}
4 64 {1, 2}
5 15 {5, 7}
6 29 {9}
7 54 {1, 6}
8 43 {8, 3}
9 93 {9, 6}
Round 135, Devices participated 10, Average loss 0.225, Central accuracy on global test data 76.748, Ensemble accuracy on global test data 78.242, Local accuracy on global train data 47.483, Local accuracy on local train data 96.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 70 {8, 5}
2 25 {8, 2}
3 56 {9, 2}
4 87 {8, 9}
5 27 {2, 5}
6 94 {0, 1}
7 1 {8, 4}
8 74 {5}
9 44 {6, 7}
Round 136, Devices participated 10, Average loss 0.396, Central accuracy on global test data 76.836, Ensemble accuracy on global test data 77.236, Local accuracy on global train data 44.473, Local accuracy on local train data 94.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 7 {0, 1}
2 31 {0, 1}
3 45 {4, 5}
4 81 {2, 3, 7}
5 62 {0, 7}
6 23 {1, 5}
7 97 {4, 7}
8 18 {0, 6}
9 27 {2, 5}
Round 137, Devices participated 10, Average loss 0.240, Central accuracy on global test data 70.107, Ensemble accuracy on global test data 71.123, Local accuracy on global train data 51.809, Local accuracy on local train data 96.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 13 {8, 5, 6}
2 40 {3, 5}
3 64 {1, 2}
4 52 {8, 7}
5 44 {6, 7}
6 22 {0, 5}
7 48 {3, 4}
8 3 {8, 6}
9 46 {9, 6}
Round 138, Devices participated 10, Average loss 0.430, Central accuracy on global test data 76.885, Ensemble accuracy on global test data 78.164, Local accuracy on global train data 48.982, Local accuracy on local train data 94.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.3333333333333333, 'layer_hidden2.bias': 0.3333333333333333}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 9 {1}
2 72 {1, 9}
3 56 {9, 2}
4 13 {8, 5, 6}
5 98 {4}
6 21 {2, 5}
7 79 {0, 8}
8 71 {1, 5}
9 32 {0, 9}
Round 139, Devices participated 10, Average loss 0.247, Central accuracy on global test data 76.289, Ensemble accuracy on global test data 76.465, Local accuracy on global train data 49.832, Local accuracy on local train data 96.983


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.3333333333333333, 'layer_hidden2.bias': 0.3333333333333333}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 2 {9}
2 36 {1, 3}
3 91 {2, 6}
4 58 {9, 4}
5 81 {2, 3, 7}
6 87 {8, 9}
7 68 {9, 6}
8 25 {8, 2}
9 5 {0, 6, 7}
Round 140, Devices participated 10, Average loss 0.393, Central accuracy on global test data 59.775, Ensemble accuracy on global test data 71.631, Local accuracy on global train data 48.840, Local accuracy on local train data 94.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 41 {0, 3}
2 74 {5}
3 21 {2, 5}
4 97 {4, 7}
5 69 {2, 3}
6 4 {8, 7}
7 17 {3, 4}
8 3 {8, 6}
9 88 {0}
Round 141, Devices participated 10, Average loss 0.311, Central accuracy on global test data 69.746, Ensemble accuracy on global test data 70.742, Local accuracy on global train data 46.140, Local accuracy on local train data 95.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 19 {5, 6}
2 97 {4, 7}
3 75 {8, 3}
4 60 {2}
5 93 {9, 6}
6 78 {2, 3, 4}
7 40 {3, 5}
8 32 {0, 9}
9 87 {8, 9}
Round 142, Devices participated 10, Average loss 0.374, Central accuracy on global test data 76.660, Ensemble accuracy on global test data 78.643, Local accuracy on global train data 49.595, Local accuracy on local train data 94.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {8, 5}
1 59 {0, 7}
2 68 {9, 6}
3 16 {0, 3}
4 36 {1, 3}
5 72 {1, 9}
6 71 {1, 5}
7 45 {4, 5}
8 91 {2, 6}
9 56 {9, 2}
Round 143, Devices participated 10, Average loss 0.334, Central accuracy on global test data 74.932, Ensemble accuracy on global test data 74.902, Local accuracy on global train data 47.058, Local accuracy on local train data 95.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {9, 2}
1 81 {2, 3, 7}
2 9 {1}
3 39 {8, 0}
4 2 {9}
5 67 {8, 1}
6 66 {1, 3}
7 98 {4}
8 80 {6}
9 51 {4, 7}
Round 144, Devices participated 10, Average loss 0.257, Central accuracy on global test data 74.033, Ensemble accuracy on global test data 77.002, Local accuracy on global train data 51.816, Local accuracy on local train data 96.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 43 {8, 3}
1 64 {1, 2}
2 42 {8, 1}
3 4 {8, 7}
4 78 {2, 3, 4}
5 0 {1, 2}
6 39 {8, 0}
7 57 {4, 6}
8 37 {4, 5, 7}
9 98 {4}
Round 145, Devices participated 10, Average loss 0.354, Central accuracy on global test data 74.482, Ensemble accuracy on global test data 74.580, Local accuracy on global train data 52.722, Local accuracy on local train data 95.233


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 11 {4, 5}
1 6 {1, 6}
2 16 {0, 3}
3 48 {3, 4}
4 14 {9, 5}
5 19 {5, 6}
6 68 {9, 6}
7 82 {8, 4}
8 94 {0, 1}
9 39 {8, 0}
Round 146, Devices participated 10, Average loss 0.262, Central accuracy on global test data 77.979, Ensemble accuracy on global test data 79.697, Local accuracy on global train data 52.725, Local accuracy on local train data 96.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 7 {0, 1}
2 93 {9, 6}
3 77 {8, 1}
4 78 {2, 3, 4}
5 38 {2, 7}
6 46 {9, 6}
7 99 {8, 9}
8 42 {8, 1}
9 54 {1, 6}
Round 147, Devices participated 10, Average loss 0.256, Central accuracy on global test data 79.062, Ensemble accuracy on global test data 79.111, Local accuracy on global train data 50.869, Local accuracy on local train data 96.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 70 {8, 5}
2 71 {1, 5}
3 57 {4, 6}
4 63 {4, 5}
5 73 {0, 6}
6 69 {2, 3}
7 17 {3, 4}
8 48 {3, 4}
9 47 {9, 5}
Round 148, Devices participated 10, Average loss 0.397, Central accuracy on global test data 69.941, Ensemble accuracy on global test data 70.137, Local accuracy on global train data 47.415, Local accuracy on local train data 95.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 42 {8, 1}
2 60 {2}
3 43 {8, 3}
4 85 {1, 4}
5 48 {3, 4}
6 1 {8, 4}
7 91 {2, 6}
8 22 {0, 5}
9 26 {9, 7}
Round 149, Devices participated 10, Average loss 0.343, Central accuracy on global test data 70.566, Ensemble accuracy on global test data 73.125, Local accuracy on global train data 52.036, Local accuracy on local train data 94.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 3 {8, 6}
2 23 {1, 5}
3 96 {1, 3}
4 47 {9, 5}
5 32 {0, 9}
6 52 {8, 7}
7 50 {2, 3}
8 82 {8, 4}
9 28 {2, 7}
Round 150, Devices participated 10, Average loss 0.291, Central accuracy on global test data 79.863, Ensemble accuracy on global test data 80.059, Local accuracy on global train data 46.846, Local accuracy on local train data 95.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 55 {4, 7}
2 26 {9, 7}
3 94 {0, 1}
4 14 {9, 5}
5 72 {1, 9}
6 5 {0, 6, 7}
7 50 {2, 3}
8 91 {2, 6}
9 42 {8, 1}
Round 151, Devices participated 10, Average loss 0.307, Central accuracy on global test data 61.006, Ensemble accuracy on global test data 70.596, Local accuracy on global train data 46.660, Local accuracy on local train data 94.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 75 {8, 3}
1 69 {2, 3}
2 0 {1, 2}
3 7 {0, 1}
4 53 {0, 7}
5 77 {8, 1}
6 67 {8, 1}
7 64 {1, 2}
8 62 {0, 7}
9 73 {0, 6}
Round 152, Devices participated 10, Average loss 0.315, Central accuracy on global test data 73.125, Ensemble accuracy on global test data 74.609, Local accuracy on global train data 51.477, Local accuracy on local train data 95.200


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {9, 5}
1 27 {2, 5}
2 66 {1, 3}
3 5 {0, 6, 7}
4 15 {5, 7}
5 75 {8, 3}
6 91 {2, 6}
7 28 {2, 7}
8 63 {4, 5}
9 62 {0, 7}
Round 153, Devices participated 10, Average loss 0.401, Central accuracy on global test data 70.713, Ensemble accuracy on global test data 72.734, Local accuracy on global train data 44.658, Local accuracy on local train data 94.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {5}
1 39 {8, 0}
2 76 {1, 3}
3 51 {4, 7}
4 46 {9, 6}
5 12 {8, 9, 3}
6 95 {3, 4}
7 17 {3, 4}
8 68 {9, 6}
9 93 {9, 6}
Round 154, Devices participated 10, Average loss 0.231, Central accuracy on global test data 74.658, Ensemble accuracy on global test data 73.896, Local accuracy on global train data 49.187, Local accuracy on local train data 96.717


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 28 {2, 7}
2 88 {0}
3 96 {1, 3}
4 20 {2, 7}
5 23 {1, 5}
6 19 {5, 6}
7 15 {5, 7}
8 91 {2, 6}
9 11 {4, 5}
Round 155, Devices participated 10, Average loss 0.292, Central accuracy on global test data 72.207, Ensemble accuracy on global test data 73.184, Local accuracy on global train data 44.302, Local accuracy on local train data 96.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 18 {0, 6}
1 46 {9, 6}
2 87 {8, 9}
3 61 {6}
4 79 {0, 8}
5 50 {2, 3}
6 69 {2, 3}
7 97 {4, 7}
8 72 {1, 9}
9 37 {4, 5, 7}
Round 156, Devices participated 10, Average loss 0.312, Central accuracy on global test data 78.262, Ensemble accuracy on global test data 79.531, Local accuracy on global train data 48.289, Local accuracy on local train data 95.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 57 {4, 6}
2 53 {0, 7}
3 30 {0, 1, 2}
4 72 {1, 9}
5 9 {1}
6 37 {4, 5, 7}
7 19 {5, 6}
8 5 {0, 6, 7}
9 14 {9, 5}
Round 157, Devices participated 10, Average loss 0.274, Central accuracy on global test data 75.986, Ensemble accuracy on global test data 76.494, Local accuracy on global train data 53.967, Local accuracy on local train data 96.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 67 {8, 1}
2 61 {6}
3 35 {8, 2}
4 96 {1, 3}
5 86 {1, 2}
6 63 {4, 5}
7 9 {1}
8 8 {8, 7}
9 36 {1, 3}
Round 158, Devices participated 10, Average loss 0.269, Central accuracy on global test data 74.824, Ensemble accuracy on global test data 75.820, Local accuracy on global train data 51.804, Local accuracy on local train data 96.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {0, 9}
1 41 {0, 3}
2 20 {2, 7}
3 92 {1, 3}
4 67 {8, 1}
5 44 {6, 7}
6 61 {6}
7 77 {8, 1}
8 3 {8, 6}
9 9 {1}
Round 159, Devices participated 10, Average loss 0.221, Central accuracy on global test data 75.518, Ensemble accuracy on global test data 76.367, Local accuracy on global train data 52.349, Local accuracy on local train data 96.883


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {8, 9}
1 20 {2, 7}
2 97 {4, 7}
3 50 {2, 3}
4 35 {8, 2}
5 68 {9, 6}
6 33 {5, 6}
7 98 {4}
8 53 {0, 7}
9 96 {1, 3}
Round 160, Devices participated 10, Average loss 0.296, Central accuracy on global test data 76.348, Ensemble accuracy on global test data 77.383, Local accuracy on global train data 49.966, Local accuracy on local train data 95.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {0, 9}
1 57 {4, 6}
2 26 {9, 7}
3 69 {2, 3}
4 23 {1, 5}
5 54 {1, 6}
6 68 {9, 6}
7 92 {1, 3}
8 4 {8, 7}
9 82 {8, 4}
Round 161, Devices participated 10, Average loss 0.251, Central accuracy on global test data 76.143, Ensemble accuracy on global test data 78.174, Local accuracy on global train data 50.557, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 31 {0, 1}
2 11 {4, 5}
3 28 {2, 7}
4 39 {8, 0}
5 71 {1, 5}
6 7 {0, 1}
7 91 {2, 6}
8 4 {8, 7}
9 93 {9, 6}
Round 162, Devices participated 10, Average loss 0.253, Central accuracy on global test data 77.676, Ensemble accuracy on global test data 78.486, Local accuracy on global train data 49.214, Local accuracy on local train data 96.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 85 {1, 4}
2 50 {2, 3}
3 23 {1, 5}
4 46 {9, 6}
5 82 {8, 4}
6 83 {9, 2}
7 20 {2, 7}
8 49 {2, 7}
9 6 {1, 6}
Round 163, Devices participated 10, Average loss 0.249, Central accuracy on global test data 78.564, Ensemble accuracy on global test data 77.520, Local accuracy on global train data 46.143, Local accuracy on local train data 96.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 42 {8, 1}
2 84 {0, 9}
3 62 {0, 7}
4 69 {2, 3}
5 25 {8, 2}
6 23 {1, 5}
7 76 {1, 3}
8 52 {8, 7}
9 75 {8, 3}
Round 164, Devices participated 10, Average loss 0.405, Central accuracy on global test data 67.812, Ensemble accuracy on global test data 67.158, Local accuracy on global train data 47.537, Local accuracy on local train data 94.733


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 95 {3, 4}
2 1 {8, 4}
3 21 {2, 5}
4 83 {9, 2}
5 23 {1, 5}
6 48 {3, 4}
7 78 {2, 3, 4}
8 42 {8, 1}
9 31 {0, 1}
Round 165, Devices participated 10, Average loss 0.273, Central accuracy on global test data 73.623, Ensemble accuracy on global test data 74.072, Local accuracy on global train data 49.324, Local accuracy on local train data 96.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 38 {2, 7}
1 6 {1, 6}
2 37 {4, 5, 7}
3 55 {4, 7}
4 34 {3, 7}
5 97 {4, 7}
6 98 {4}
7 74 {5}
8 56 {9, 2}
9 45 {4, 5}
Round 166, Devices participated 10, Average loss 0.214, Central accuracy on global test data 72.432, Ensemble accuracy on global test data 70.723, Local accuracy on global train data 46.997, Local accuracy on local train data 96.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 93 {9, 6}
2 74 {5}
3 85 {1, 4}
4 16 {0, 3}
5 98 {4}
6 50 {2, 3}
7 59 {0, 7}
8 70 {8, 5}
9 3 {8, 6}
Round 167, Devices participated 10, Average loss 0.229, Central accuracy on global test data 75.928, Ensemble accuracy on global test data 80.107, Local accuracy on global train data 50.461, Local accuracy on local train data 96.600


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 39 {8, 0}
1 20 {2, 7}
2 43 {8, 3}
3 1 {8, 4}
4 78 {2, 3, 4}
5 0 {1, 2}
6 60 {2}
7 31 {0, 1}
8 15 {5, 7}
9 6 {1, 6}
Round 168, Devices participated 10, Average loss 0.290, Central accuracy on global test data 75.635, Ensemble accuracy on global test data 76.094, Local accuracy on global train data 47.092, Local accuracy on local train data 95.633


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 76 {1, 3}
2 2 {9}
3 31 {0, 1}
4 19 {5, 6}
5 17 {3, 4}
6 83 {9, 2}
7 92 {1, 3}
8 44 {6, 7}
9 35 {8, 2}
Round 169, Devices participated 10, Average loss 0.229, Central accuracy on global test data 75.244, Ensemble accuracy on global test data 77.148, Local accuracy on global train data 49.631, Local accuracy on local train data 96.600


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {9, 2}
1 91 {2, 6}
2 92 {1, 3}
3 28 {2, 7}
4 38 {2, 7}
5 82 {8, 4}
6 32 {0, 9}
7 98 {4}
8 63 {4, 5}
9 85 {1, 4}
Round 170, Devices participated 10, Average loss 0.267, Central accuracy on global test data 74.160, Ensemble accuracy on global test data 75.078, Local accuracy on global train data 50.022, Local accuracy on local train data 96.533


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 5 {0, 6, 7}
1 84 {0, 9}
2 60 {2}
3 47 {9, 5}
4 3 {8, 6}
5 13 {8, 5, 6}
6 65 {3, 4}
7 92 {1, 3}
8 19 {5, 6}
9 74 {5}
Round 171, Devices participated 10, Average loss 0.305, Central accuracy on global test data 77.500, Ensemble accuracy on global test data 76.816, Local accuracy on global train data 46.592, Local accuracy on local train data 96.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 45 {4, 5}
2 39 {8, 0}
3 67 {8, 1}
4 6 {1, 6}
5 78 {2, 3, 4}
6 84 {0, 9}
7 80 {6}
8 22 {0, 5}
9 57 {4, 6}
Round 172, Devices participated 10, Average loss 0.264, Central accuracy on global test data 77.090, Ensemble accuracy on global test data 78.271, Local accuracy on global train data 55.808, Local accuracy on local train data 96.417


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 42 {8, 1}
2 43 {8, 3}
3 38 {2, 7}
4 94 {0, 1}
5 36 {1, 3}
6 68 {9, 6}
7 8 {8, 7}
8 11 {4, 5}
9 89 {0, 9}
Round 173, Devices participated 10, Average loss 0.267, Central accuracy on global test data 79.463, Ensemble accuracy on global test data 79.902, Local accuracy on global train data 49.102, Local accuracy on local train data 96.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 69 {2, 3}
2 89 {0, 9}
3 9 {1}
4 42 {8, 1}
5 74 {5}
6 62 {0, 7}
7 68 {9, 6}
8 57 {4, 6}
9 93 {9, 6}
Round 174, Devices participated 10, Average loss 0.208, Central accuracy on global test data 78.477, Ensemble accuracy on global test data 78.936, Local accuracy on global train data 47.246, Local accuracy on local train data 96.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 66 {1, 3}
2 93 {9, 6}
3 44 {6, 7}
4 48 {3, 4}
5 13 {8, 5, 6}
6 3 {8, 6}
7 21 {2, 5}
8 23 {1, 5}
9 39 {8, 0}
Round 175, Devices participated 10, Average loss 0.251, Central accuracy on global test data 79.385, Ensemble accuracy on global test data 80.068, Local accuracy on global train data 49.141, Local accuracy on local train data 96.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 94 {0, 1}
2 18 {0, 6}
3 68 {9, 6}
4 65 {3, 4}
5 53 {0, 7}
6 66 {1, 3}
7 75 {8, 3}
8 12 {8, 9, 3}
9 92 {1, 3}
Round 176, Devices participated 10, Average loss 0.256, Central accuracy on global test data 71.592, Ensemble accuracy on global test data 72.393, Local accuracy on global train data 52.031, Local accuracy on local train data 95.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 75 {8, 3}
2 39 {8, 0}
3 64 {1, 2}
4 49 {2, 7}
5 44 {6, 7}
6 66 {1, 3}
7 50 {2, 3}
8 60 {2}
9 98 {4}
Round 177, Devices participated 10, Average loss 0.312, Central accuracy on global test data 71.768, Ensemble accuracy on global test data 71.719, Local accuracy on global train data 45.239, Local accuracy on local train data 95.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {6}
1 27 {2, 5}
2 77 {8, 1}
3 23 {1, 5}
4 66 {1, 3}
5 33 {5, 6}
6 26 {9, 7}
7 42 {8, 1}
8 37 {4, 5, 7}
9 78 {2, 3, 4}
Round 178, Devices participated 10, Average loss 0.361, Central accuracy on global test data 79.844, Ensemble accuracy on global test data 80.410, Local accuracy on global train data 53.518, Local accuracy on local train data 94.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 56 {9, 2}
2 5 {0, 6, 7}
3 30 {0, 1, 2}
4 76 {1, 3}
5 95 {3, 4}
6 53 {0, 7}
7 73 {0, 6}
8 63 {4, 5}
9 88 {0}
Round 179, Devices participated 10, Average loss 0.255, Central accuracy on global test data 77.295, Ensemble accuracy on global test data 74.863, Local accuracy on global train data 52.593, Local accuracy on local train data 96.633


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 89 {0, 9}
2 68 {9, 6}
3 64 {1, 2}
4 78 {2, 3, 4}
5 62 {0, 7}
6 28 {2, 7}
7 30 {0, 1, 2}
8 49 {2, 7}
9 41 {0, 3}
Round 180, Devices participated 10, Average loss 0.276, Central accuracy on global test data 72.920, Ensemble accuracy on global test data 73.975, Local accuracy on global train data 51.123, Local accuracy on local train data 96.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 67 {8, 1}
1 45 {4, 5}
2 94 {0, 1}
3 35 {8, 2}
4 39 {8, 0}
5 72 {1, 9}
6 5 {0, 6, 7}
7 22 {0, 5}
8 7 {0, 1}
9 62 {0, 7}
Round 181, Devices participated 10, Average loss 0.297, Central accuracy on global test data 77.646, Ensemble accuracy on global test data 79.746, Local accuracy on global train data 55.457, Local accuracy on local train data 96.400


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 91 {2, 6}
2 9 {1}
3 58 {9, 4}
4 98 {4}
5 5 {0, 6, 7}
6 97 {4, 7}
7 92 {1, 3}
8 55 {4, 7}
9 12 {8, 9, 3}
Round 182, Devices participated 10, Average loss 0.294, Central accuracy on global test data 74.248, Ensemble accuracy on global test data 75.078, Local accuracy on global train data 54.971, Local accuracy on local train data 95.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 9 {1}
2 22 {0, 5}
3 95 {3, 4}
4 91 {2, 6}
5 55 {4, 7}
6 21 {2, 5}
7 15 {5, 7}
8 13 {8, 5, 6}
9 1 {8, 4}
Round 183, Devices participated 10, Average loss 0.251, Central accuracy on global test data 77.988, Ensemble accuracy on global test data 79.561, Local accuracy on global train data 52.549, Local accuracy on local train data 96.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 53 {0, 7}
2 10 {3, 7}
3 7 {0, 1}
4 6 {1, 6}
5 90 {8, 6, 7}
6 21 {2, 5}
7 92 {1, 3}
8 78 {2, 3, 4}
9 46 {9, 6}
Round 184, Devices participated 10, Average loss 0.229, Central accuracy on global test data 76.523, Ensemble accuracy on global test data 77.051, Local accuracy on global train data 51.904, Local accuracy on local train data 96.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 92 {1, 3}
2 66 {1, 3}
3 58 {9, 4}
4 24 {4, 5}
5 86 {1, 2}
6 82 {8, 4}
7 2 {9}
8 7 {0, 1}
9 64 {1, 2}
Round 185, Devices participated 10, Average loss 0.209, Central accuracy on global test data 75.596, Ensemble accuracy on global test data 75.166, Local accuracy on global train data 50.254, Local accuracy on local train data 96.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {3, 5}
1 50 {2, 3}
2 36 {1, 3}
3 99 {8, 9}
4 33 {5, 6}
5 79 {0, 8}
6 62 {0, 7}
7 74 {5}
8 16 {0, 3}
9 22 {0, 5}
Round 186, Devices participated 10, Average loss 0.330, Central accuracy on global test data 72.646, Ensemble accuracy on global test data 74.639, Local accuracy on global train data 50.278, Local accuracy on local train data 94.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 25 {8, 2}
1 33 {5, 6}
2 10 {3, 7}
3 14 {9, 5}
4 87 {8, 9}
5 8 {8, 7}
6 51 {4, 7}
7 83 {9, 2}
8 69 {2, 3}
9 57 {4, 6}
Round 187, Devices participated 10, Average loss 0.369, Central accuracy on global test data 76.787, Ensemble accuracy on global test data 79.160, Local accuracy on global train data 47.290, Local accuracy on local train data 93.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 15 {5, 7}
2 38 {2, 7}
3 19 {5, 6}
4 52 {8, 7}
5 46 {9, 6}
6 71 {1, 5}
7 9 {1}
8 47 {9, 5}
9 75 {8, 3}
Round 188, Devices participated 10, Average loss 0.257, Central accuracy on global test data 77.520, Ensemble accuracy on global test data 75.908, Local accuracy on global train data 46.462, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 86 {1, 2}
1 92 {1, 3}
2 14 {9, 5}
3 8 {8, 7}
4 45 {4, 5}
5 38 {2, 7}
6 60 {2}
7 63 {4, 5}
8 82 {8, 4}
9 46 {9, 6}
Round 189, Devices participated 10, Average loss 0.251, Central accuracy on global test data 77.324, Ensemble accuracy on global test data 77.188, Local accuracy on global train data 42.061, Local accuracy on local train data 96.233


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {0, 6}
1 57 {4, 6}
2 67 {8, 1}
3 14 {9, 5}
4 40 {3, 5}
5 24 {4, 5}
6 63 {4, 5}
7 35 {8, 2}
8 72 {1, 9}
9 69 {2, 3}
Round 190, Devices participated 10, Average loss 0.344, Central accuracy on global test data 76.719, Ensemble accuracy on global test data 77.520, Local accuracy on global train data 46.975, Local accuracy on local train data 94.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 77 {8, 1}
1 55 {4, 7}
2 61 {6}
3 34 {3, 7}
4 18 {0, 6}
5 35 {8, 2}
6 0 {1, 2}
7 97 {4, 7}
8 98 {4}
9 26 {9, 7}
Round 191, Devices participated 10, Average loss 0.237, Central accuracy on global test data 74.814, Ensemble accuracy on global test data 77.158, Local accuracy on global train data 57.234, Local accuracy on local train data 96.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 30 {0, 1, 2}
2 53 {0, 7}
3 77 {8, 1}
4 76 {1, 3}
5 85 {1, 4}
6 8 {8, 7}
7 35 {8, 2}
8 81 {2, 3, 7}
9 69 {2, 3}
Round 192, Devices participated 10, Average loss 0.356, Central accuracy on global test data 76.025, Ensemble accuracy on global test data 74.883, Local accuracy on global train data 52.070, Local accuracy on local train data 95.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 30 {0, 1, 2}
2 46 {9, 6}
3 49 {2, 7}
4 35 {8, 2}
5 90 {8, 6, 7}
6 6 {1, 6}
7 40 {3, 5}
8 20 {2, 7}
9 97 {4, 7}
Round 193, Devices participated 10, Average loss 0.293, Central accuracy on global test data 75.117, Ensemble accuracy on global test data 74.746, Local accuracy on global train data 52.390, Local accuracy on local train data 95.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 21 {2, 5}
2 97 {4, 7}
3 18 {0, 6}
4 44 {6, 7}
5 48 {3, 4}
6 98 {4}
7 45 {4, 5}
8 7 {0, 1}
9 95 {3, 4}
Round 194, Devices participated 10, Average loss 0.196, Central accuracy on global test data 71.543, Ensemble accuracy on global test data 72.725, Local accuracy on global train data 51.802, Local accuracy on local train data 96.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 67 {8, 1}
1 72 {1, 9}
2 49 {2, 7}
3 58 {9, 4}
4 2 {9}
5 66 {1, 3}
6 55 {4, 7}
7 98 {4}
8 25 {8, 2}
9 53 {0, 7}
Round 195, Devices participated 10, Average loss 0.300, Central accuracy on global test data 76.641, Ensemble accuracy on global test data 76.836, Local accuracy on global train data 49.893, Local accuracy on local train data 95.400


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 16 {0, 3}
1 2 {9}
2 36 {1, 3}
3 77 {8, 1}
4 86 {1, 2}
5 40 {3, 5}
6 87 {8, 9}
7 33 {5, 6}
8 95 {3, 4}
9 75 {8, 3}
Round 196, Devices participated 10, Average loss 0.345, Central accuracy on global test data 64.453, Ensemble accuracy on global test data 71.445, Local accuracy on global train data 48.538, Local accuracy on local train data 94.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 66 {1, 3}
2 81 {2, 3, 7}
3 52 {8, 7}
4 47 {9, 5}
5 99 {8, 9}
6 38 {2, 7}
7 56 {9, 2}
8 65 {3, 4}
9 4 {8, 7}
Round 197, Devices participated 10, Average loss 0.286, Central accuracy on global test data 77.646, Ensemble accuracy on global test data 78.008, Local accuracy on global train data 47.988, Local accuracy on local train data 95.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {0, 6}
1 93 {9, 6}
2 90 {8, 6, 7}
3 66 {1, 3}
4 22 {0, 5}
5 52 {8, 7}
6 77 {8, 1}
7 18 {0, 6}
8 71 {1, 5}
9 39 {8, 0}
Round 198, Devices participated 10, Average loss 0.284, Central accuracy on global test data 77.988, Ensemble accuracy on global test data 80.459, Local accuracy on global train data 54.421, Local accuracy on local train data 96.383


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:335: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 81 {2, 3, 7}
2 11 {4, 5}
3 2 {9}
4 36 {1, 3}
5 79 {0, 8}
6 23 {1, 5}
7 5 {0, 6, 7}
8 64 {1, 2}
9 30 {0, 1, 2}
Round 199, Devices participated 10, Average loss 0.303, Central accuracy on global test data 70.566, Ensemble accuracy on global test data 80.918, Local accuracy on global train data 50.913, Local accuracy on local train data 95.367


Testing accuracy on test data: 70.57, Testing loss: 2.02

