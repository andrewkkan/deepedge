python main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_refresh 0 --store_models testtemp_async_gen2 --alpha_scale 0.5 --async_s2d 1


def DFAN_regavg(args, teacher, student, generator, optimizer, epoch):

    for local in teacher:
        local.eval()
    generator.train()
    student.train()
    loss_G = torch.tensor(0.0)
    loss_S = torch.tensor(0.0)
    optimizer_S, optimizer_G = optimizer
    sm = torch.nn.Softmax()

    # w_locals = []
    # for local in teacher:
    #     w_locals.append(copy.deepcopy(local.state_dict()))
    # w_fedavg = FedAvg(w_locals)

    # mlpa, mlpt = model_fusion_MLP(teacher, 1024, 200, 10)
    # mlpa, mlpt = mlpa.to(args.device), mlpt.to(args.device)
    # w_mlpt = mlpt.state_dict()
    mlpa, mlpt = fusion_layer_MLP(teacher, student, 1024, 200, 10)
    mlpa = mlpa.to(args.device)
    mlpt = mlpt.to(args.device)
    mlpa.eval()
    mlpt.eval()
    print(mlpt.alpha)

    w_fedavg = mlpt.state_dict()

    for i in range(args.epoch_itrs):
        for k in range(2):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            optimizer_G.zero_grad()
            fake = generator(z)
            fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
            s_logit = student(fake)
            # t_sm = ensemble(teacher, fake, detach=False, mode=args.ensemble_mode)
            # t_logit = torch.zeros_like(teacher[0](fake))
            # for k in range(10):
            #     t_logit += teacher[k](fake) 
            t_logit = mlpa(fake)
            oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
            max_Gout = torch.max(torch.abs(fake))
            if max_Gout > 8.0:
                loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0) + torch.pow(max_Gout - 8.0, 2.0)
                print(max_Gout)
            else:
                loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0)
            loss_G.backward()                   
            optimizer_G.step()
        for j in range(5):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            optimizer_S.zero_grad()
            fake = generator(z).detach()
            fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
            s_logit = student(fake)
            # t_sm = ensemble(teacher, fake, detach=True, mode=args.ensemble_mode)
            # t_logit = torch.zeros_like(teacher[0](fake).detach())
            # for k in range(10):
            #     t_logit += teacher[k](fake).detach()
            t_logit = mlpa(fake).detach()
            oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
            loss_S1 = torch.log(1. / (2. - oneMinus_P_S))

            diff_L2 = torch.FloatTensor([0.]).to(args.device)
            for ln, student_w, fedavg_w in zip(student.state_dict().keys(), student.parameters(), w_fedavg.values()):
                diff_L2 += (fedavg_w - student_w).norm(2) * mlpt.alpha[ln] * args.alpha_scale
            if epoch == 0:
                loss_S2 = 0
            else:
                loss_S2 = diff_L2

            # diff_L2 = torch.FloatTensor([0.]).to(args.device)
            # for student_w, mlpt_w in zip(student.parameters(), w_mlpt.values()):
            #     diff_L2 += ((mlpt_w - student_w)*mlpt_w.abs()).norm(2)
            # loss_S2 = diff_L2 * alpha

            loss_S = loss_S1 + loss_S2
            loss_S.backward()
            optimizer_S.step()

        if args.verbose and i % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tG_Loss: {:.6f} S_loss: {:.6f}'.format(
                epoch, i, args.epoch_itrs, 100 * float(i) / float(args.epoch_itrs), loss_G.item(), loss_S.item()))        #vp.add_scalar('Loss_S', (epoch-1)*args.epoch_itrs+i, loss_S.item())
            #vp.add_scalar('Loss_G', (epoch-1)*args.epoch_itrs+i, loss_G.item())


]0;root@38fdb776567a: /workspace/deepedgeroot@38fdb776567a:/workspace/deepedge# [Kpython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num
m_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_
_refresh 0 --store_models testtemp_async_gen2 --alpha_scale 0.5 --async_s2d 1[1@_[1@m[1@l[1@p[1@t
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 65 {3, 4}
2 47 {9, 5}
3 24 {4, 5}
4 15 {5, 7}
5 34 {3, 7}
6 49 {2, 7}
7 42 {8, 1}
8 6 {1, 6}
9 70 {8, 5}
Round   0, Devices participated 10, Average loss 0.556, Central accuracy on global test data 22.910, Ensemble accuracy on global test data 28.135, Local accuracy on global train data 19.353, Local accuracy on local train data 88.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(12.6359, device='cuda:0', grad_fn=<MaxBackward1>)
tensor(17.0139, device='cuda:0', grad_fn=<MaxBackward1>)
^[[A    ^[[B    0 31 {0, 1}
1 99 {8, 9}
2 35 {8, 2}
3 9 {1}
4 58 {9, 4}
5 70 {8, 5}
6 79 {0, 8}
7 60 {2}
8 67 {8, 1}
9 47 {9, 5}
Round   1, Devices participated 10, Average loss 0.439, Central accuracy on global test data 30.645, Ensemble accuracy on global test data 25.371, Local accuracy on global train data 19.370, Local accuracy on local train data 89.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 14 {9, 5}
2 76 {1, 3}
3 81 {2, 3, 7}
4 9 {1}
5 13 {8, 5, 6}
6 86 {1, 2}
7 48 {3, 4}
8 66 {1, 3}
9 27 {2, 5}
Round   2, Devices participated 10, Average loss 0.505, Central accuracy on global test data 34.639, Ensemble accuracy on global test data 34.219, Local accuracy on global train data 22.014, Local accuracy on local train data 89.400


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 94 {0, 1}
2 66 {1, 3}
3 11 {4, 5}
4 87 {8, 9}
5 20 {2, 7}
6 67 {8, 1}
7 45 {4, 5}
8 99 {8, 9}
9 21 {2, 5}
Round   3, Devices participated 10, Average loss 0.485, Central accuracy on global test data 45.020, Ensemble accuracy on global test data 48.447, Local accuracy on global train data 22.498, Local accuracy on local train data 88.750


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 62 {0, 7}
1 68 {9, 6}
2 19 {5, 6}
3 77 {8, 1}
4 59 {0, 7}
5 31 {0, 1}
6 37 {4, 5, 7}
7 86 {1, 2}
8 60 {2}
9 22 {0, 5}
Round   4, Devices participated 10, Average loss 0.390, Central accuracy on global test data 19.287, Ensemble accuracy on global test data 35.918, Local accuracy on global train data 19.429, Local accuracy on local train data 93.033


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 7 {0, 1}
2 52 {8, 7}
3 36 {1, 3}
4 77 {8, 1}
5 22 {0, 5}
6 39 {8, 0}
7 89 {0, 9}
8 97 {4, 7}
9 32 {0, 9}
Round   5, Devices participated 10, Average loss 0.443, Central accuracy on global test data 29.297, Ensemble accuracy on global test data 31.270, Local accuracy on global train data 20.920, Local accuracy on local train data 90.600


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 56 {9, 2}
2 98 {4}
3 79 {0, 8}
4 24 {4, 5}
5 70 {8, 5}
6 55 {4, 7}
7 8 {8, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round   6, Devices participated 10, Average loss 0.391, Central accuracy on global test data 57.793, Ensemble accuracy on global test data 56.738, Local accuracy on global train data 24.207, Local accuracy on local train data 92.450


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 28 {2, 7}
2 39 {8, 0}
3 24 {4, 5}
4 58 {9, 4}
5 34 {3, 7}
6 48 {3, 4}
7 54 {1, 6}
8 90 {8, 6, 7}
9 83 {9, 2}
Round   7, Devices participated 10, Average loss 0.453, Central accuracy on global test data 58.506, Ensemble accuracy on global test data 57.412, Local accuracy on global train data 26.960, Local accuracy on local train data 91.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 38 {2, 7}
2 6 {1, 6}
3 41 {0, 3}
4 55 {4, 7}
5 56 {9, 2}
6 25 {8, 2}
7 98 {4}
8 13 {8, 5, 6}
9 27 {2, 5}
Round   8, Devices participated 10, Average loss 0.379, Central accuracy on global test data 62.988, Ensemble accuracy on global test data 61.543, Local accuracy on global train data 31.064, Local accuracy on local train data 93.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 37 {4, 5, 7}
3 86 {1, 2}
4 39 {8, 0}
5 26 {9, 7}
6 90 {8, 6, 7}
7 2 {9}
8 83 {9, 2}
9 55 {4, 7}
Round   9, Devices participated 10, Average loss 0.464, Central accuracy on global test data 64.678, Ensemble accuracy on global test data 63.057, Local accuracy on global train data 28.232, Local accuracy on local train data 91.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.6433, device='cuda:0', grad_fn=<MaxBackward1>)
0 31 {0, 1}
1 70 {8, 5}
2 9 {1}
3 99 {8, 9}
4 77 {8, 1}
5 14 {9, 5}
6 16 {0, 3}
7 44 {6, 7}
8 21 {2, 5}
9 97 {4, 7}
Round  10, Devices participated 10, Average loss 0.331, Central accuracy on global test data 55.664, Ensemble accuracy on global test data 51.328, Local accuracy on global train data 25.610, Local accuracy on local train data 93.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 45 {4, 5}
2 15 {5, 7}
3 30 {0, 1, 2}
4 24 {4, 5}
5 52 {8, 7}
6 35 {8, 2}
7 96 {1, 3}
8 7 {0, 1}
9 94 {0, 1}
Round  11, Devices participated 10, Average loss 0.182, Central accuracy on global test data 59.346, Ensemble accuracy on global test data 57.549, Local accuracy on global train data 30.967, Local accuracy on local train data 96.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {4, 6}
1 79 {0, 8}
2 0 {1, 2}
3 65 {3, 4}
4 68 {9, 6}
5 75 {8, 3}
6 85 {1, 4}
7 35 {8, 2}
8 78 {2, 3, 4}
9 3 {8, 6}
Round  12, Devices participated 10, Average loss 0.550, Central accuracy on global test data 44.365, Ensemble accuracy on global test data 43.887, Local accuracy on global train data 23.748, Local accuracy on local train data 88.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 49 {2, 7}
2 56 {9, 2}
3 13 {8, 5, 6}
4 70 {8, 5}
5 16 {0, 3}
6 32 {0, 9}
7 10 {3, 7}
8 74 {5}
9 51 {4, 7}
Round  13, Devices participated 10, Average loss 0.469, Central accuracy on global test data 60.654, Ensemble accuracy on global test data 61.338, Local accuracy on global train data 26.431, Local accuracy on local train data 93.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 33 {5, 6}
2 30 {0, 1, 2}
3 78 {2, 3, 4}
4 88 {0}
5 9 {1}
6 26 {9, 7}
7 43 {8, 3}
8 91 {2, 6}
9 65 {3, 4}
Round  14, Devices participated 10, Average loss 0.504, Central accuracy on global test data 72.197, Ensemble accuracy on global test data 72.920, Local accuracy on global train data 36.721, Local accuracy on local train data 92.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 58 {9, 4}
2 80 {6}
3 92 {1, 3}
4 28 {2, 7}
5 44 {6, 7}
6 45 {4, 5}
7 79 {0, 8}
8 89 {0, 9}
9 15 {5, 7}
Round  15, Devices participated 10, Average loss 0.483, Central accuracy on global test data 67.520, Ensemble accuracy on global test data 66.338, Local accuracy on global train data 35.618, Local accuracy on local train data 94.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 83 {9, 2}
2 22 {0, 5}
3 69 {2, 3}
4 1 {8, 4}
5 54 {1, 6}
6 62 {0, 7}
7 36 {1, 3}
8 58 {9, 4}
9 63 {4, 5}
Round  16, Devices participated 10, Average loss 0.487, Central accuracy on global test data 75.547, Ensemble accuracy on global test data 76.836, Local accuracy on global train data 31.331, Local accuracy on local train data 91.567


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 68 {9, 6}
2 4 {8, 7}
3 10 {3, 7}
4 33 {5, 6}
5 96 {1, 3}
6 15 {5, 7}
7 64 {1, 2}
8 89 {0, 9}
9 54 {1, 6}
Round  17, Devices participated 10, Average loss 0.419, Central accuracy on global test data 72.754, Ensemble accuracy on global test data 72.295, Local accuracy on global train data 36.719, Local accuracy on local train data 95.417


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {1, 3}
1 22 {0, 5}
2 1 {8, 4}
3 40 {3, 5}
4 41 {0, 3}
5 52 {8, 7}
6 23 {1, 5}
7 75 {8, 3}
8 62 {0, 7}
9 11 {4, 5}
Round  18, Devices participated 10, Average loss 0.511, Central accuracy on global test data 70.713, Ensemble accuracy on global test data 69.941, Local accuracy on global train data 35.925, Local accuracy on local train data 92.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 27 {2, 5}
2 8 {8, 7}
3 34 {3, 7}
4 98 {4}
5 74 {5}
6 36 {1, 3}
7 78 {2, 3, 4}
8 85 {1, 4}
9 88 {0}
Round  19, Devices participated 10, Average loss 0.265, Central accuracy on global test data 73.359, Ensemble accuracy on global test data 73.457, Local accuracy on global train data 39.414, Local accuracy on local train data 97.133


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 2 {9}
2 83 {9, 2}
3 32 {0, 9}
4 78 {2, 3, 4}
5 39 {8, 0}
6 25 {8, 2}
7 56 {9, 2}
8 76 {1, 3}
9 35 {8, 2}
Round  20, Devices participated 10, Average loss 0.459, Central accuracy on global test data 57.627, Ensemble accuracy on global test data 58.252, Local accuracy on global train data 38.113, Local accuracy on local train data 95.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 2 {9}
2 40 {3, 5}
3 64 {1, 2}
4 12 {8, 9, 3}
5 94 {0, 1}
6 60 {2}
7 24 {4, 5}
8 61 {6}
9 29 {9}
Round  21, Devices participated 10, Average loss 0.380, Central accuracy on global test data 71.934, Ensemble accuracy on global test data 72.217, Local accuracy on global train data 32.129, Local accuracy on local train data 94.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.3333333333333333, 'layer_hidden2.bias': 0.3333333333333333}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {0, 9}
1 26 {9, 7}
2 80 {6}
3 38 {2, 7}
4 85 {1, 4}
5 86 {1, 2}
6 11 {4, 5}
7 5 {0, 6, 7}
8 98 {4}
9 96 {1, 3}
Round  22, Devices participated 10, Average loss 0.319, Central accuracy on global test data 77.598, Ensemble accuracy on global test data 77.871, Local accuracy on global train data 45.715, Local accuracy on local train data 95.300


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 41 {0, 3}
1 36 {1, 3}
2 92 {1, 3}
3 48 {3, 4}
4 96 {1, 3}
5 76 {1, 3}
6 77 {8, 1}
7 57 {4, 6}
8 78 {2, 3, 4}
9 28 {2, 7}
Round  23, Devices participated 10, Average loss 0.438, Central accuracy on global test data 60.508, Ensemble accuracy on global test data 61.211, Local accuracy on global train data 46.047, Local accuracy on local train data 95.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 65 {3, 4}
2 87 {8, 9}
3 31 {0, 1}
4 89 {0, 9}
5 10 {3, 7}
6 70 {8, 5}
7 44 {6, 7}
8 28 {2, 7}
9 41 {0, 3}
Round  24, Devices participated 10, Average loss 0.423, Central accuracy on global test data 76.816, Ensemble accuracy on global test data 77.305, Local accuracy on global train data 37.649, Local accuracy on local train data 95.467


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 18 {0, 6}
2 59 {0, 7}
3 53 {0, 7}
4 99 {8, 9}
5 72 {1, 9}
6 90 {8, 6, 7}
7 14 {9, 5}
8 13 {8, 5, 6}
9 80 {6}
Round  25, Devices participated 10, Average loss 0.394, Central accuracy on global test data 62.930, Ensemble accuracy on global test data 63.291, Local accuracy on global train data 30.112, Local accuracy on local train data 94.283


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 61 {6}
2 33 {5, 6}
3 12 {8, 9, 3}
4 87 {8, 9}
5 20 {2, 7}
6 3 {8, 6}
7 34 {3, 7}
8 9 {1}
9 63 {4, 5}
Round  26, Devices participated 10, Average loss 0.450, Central accuracy on global test data 77.139, Ensemble accuracy on global test data 76.885, Local accuracy on global train data 42.388, Local accuracy on local train data 94.983


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {6}
1 31 {0, 1}
2 28 {2, 7}
3 90 {8, 6, 7}
4 19 {5, 6}
5 26 {9, 7}
6 73 {0, 6}
7 23 {1, 5}
8 11 {4, 5}
9 96 {1, 3}
Round  27, Devices participated 10, Average loss 0.382, Central accuracy on global test data 77.227, Ensemble accuracy on global test data 77.480, Local accuracy on global train data 43.599, Local accuracy on local train data 94.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {8, 6}
1 55 {4, 7}
2 1 {8, 4}
3 5 {0, 6, 7}
4 93 {9, 6}
5 53 {0, 7}
6 7 {0, 1}
7 96 {1, 3}
8 57 {4, 6}
9 64 {1, 2}
Round  28, Devices participated 10, Average loss 0.344, Central accuracy on global test data 75.381, Ensemble accuracy on global test data 75.527, Local accuracy on global train data 43.215, Local accuracy on local train data 95.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 46 {9, 6}
2 99 {8, 9}
3 78 {2, 3, 4}
4 33 {5, 6}
5 79 {0, 8}
6 63 {4, 5}
7 13 {8, 5, 6}
8 26 {9, 7}
9 10 {3, 7}
Round  29, Devices participated 10, Average loss 0.571, Central accuracy on global test data 79.229, Ensemble accuracy on global test data 79.238, Local accuracy on global train data 42.803, Local accuracy on local train data 93.667


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0279, device='cuda:0', grad_fn=<MaxBackward1>)
0 80 {6}
1 21 {2, 5}
2 40 {3, 5}
3 74 {5}
4 49 {2, 7}
5 36 {1, 3}
6 1 {8, 4}
7 24 {4, 5}
8 71 {1, 5}
9 79 {0, 8}
Round  30, Devices participated 10, Average loss 0.355, Central accuracy on global test data 75.010, Ensemble accuracy on global test data 74.883, Local accuracy on global train data 40.334, Local accuracy on local train data 96.517


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 68 {9, 6}
2 43 {8, 3}
3 85 {1, 4}
4 10 {3, 7}
5 2 {9}
6 83 {9, 2}
7 94 {0, 1}
8 0 {1, 2}
9 66 {1, 3}
Round  31, Devices participated 10, Average loss 0.267, Central accuracy on global test data 69.434, Ensemble accuracy on global test data 70.762, Local accuracy on global train data 46.414, Local accuracy on local train data 96.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 16 {0, 3}
2 32 {0, 9}
3 76 {1, 3}
4 4 {8, 7}
5 51 {4, 7}
6 92 {1, 3}
7 33 {5, 6}
8 27 {2, 5}
9 86 {1, 2}
Round  32, Devices participated 10, Average loss 0.405, Central accuracy on global test data 79.697, Ensemble accuracy on global test data 80.049, Local accuracy on global train data 48.672, Local accuracy on local train data 96.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 40 {3, 5}
2 15 {5, 7}
3 16 {0, 3}
4 66 {1, 3}
5 10 {3, 7}
6 87 {8, 9}
7 1 {8, 4}
8 2 {9}
9 41 {0, 3}
Round  33, Devices participated 10, Average loss 0.411, Central accuracy on global test data 69.980, Ensemble accuracy on global test data 71.191, Local accuracy on global train data 44.424, Local accuracy on local train data 95.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {1, 2}
1 15 {5, 7}
2 38 {2, 7}
3 44 {6, 7}
4 88 {0}
5 56 {9, 2}
6 19 {5, 6}
7 71 {1, 5}
8 29 {9}
9 61 {6}
Round  34, Devices participated 10, Average loss 0.258, Central accuracy on global test data 75.830, Ensemble accuracy on global test data 75.654, Local accuracy on global train data 43.289, Local accuracy on local train data 97.400


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 95 {3, 4}
1 67 {8, 1}
2 46 {9, 6}
3 57 {4, 6}
4 1 {8, 4}
5 27 {2, 5}
6 48 {3, 4}
7 44 {6, 7}
8 16 {0, 3}
9 87 {8, 9}
Round  35, Devices participated 10, Average loss 0.343, Central accuracy on global test data 78.193, Ensemble accuracy on global test data 78.438, Local accuracy on global train data 42.571, Local accuracy on local train data 95.550


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 0 {1, 2}
2 40 {3, 5}
3 32 {0, 9}
4 39 {8, 0}
5 76 {1, 3}
6 73 {0, 6}
7 79 {0, 8}
8 42 {8, 1}
9 24 {4, 5}
Round  36, Devices participated 10, Average loss 0.380, Central accuracy on global test data 79.854, Ensemble accuracy on global test data 79.648, Local accuracy on global train data 48.667, Local accuracy on local train data 95.400


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 33 {5, 6}
1 91 {2, 6}
2 93 {9, 6}
3 36 {1, 3}
4 92 {1, 3}
5 58 {9, 4}
6 73 {0, 6}
7 77 {8, 1}
8 13 {8, 5, 6}
9 35 {8, 2}
Round  37, Devices participated 10, Average loss 0.471, Central accuracy on global test data 76.592, Ensemble accuracy on global test data 76.875, Local accuracy on global train data 48.704, Local accuracy on local train data 94.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 10 {3, 7}
2 35 {8, 2}
3 76 {1, 3}
4 90 {8, 6, 7}
5 41 {0, 3}
6 60 {2}
7 97 {4, 7}
8 19 {5, 6}
9 92 {1, 3}
Round  38, Devices participated 10, Average loss 0.377, Central accuracy on global test data 78.623, Ensemble accuracy on global test data 78.809, Local accuracy on global train data 51.311, Local accuracy on local train data 96.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.3333333333333333, 'layer_hidden2.bias': 0.3333333333333333}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {0, 8}
1 56 {9, 2}
2 50 {2, 3}
3 98 {4}
4 88 {0}
5 2 {9}
6 39 {8, 0}
7 22 {0, 5}
8 30 {0, 1, 2}
9 23 {1, 5}
Round  39, Devices participated 10, Average loss 0.293, Central accuracy on global test data 81.152, Ensemble accuracy on global test data 80.449, Local accuracy on global train data 47.153, Local accuracy on local train data 96.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 21 {2, 5}
2 87 {8, 9}
3 35 {8, 2}
4 34 {3, 7}
5 5 {0, 6, 7}
6 89 {0, 9}
7 44 {6, 7}
8 17 {3, 4}
9 65 {3, 4}
Round  40, Devices participated 10, Average loss 0.452, Central accuracy on global test data 79.902, Ensemble accuracy on global test data 80.020, Local accuracy on global train data 43.848, Local accuracy on local train data 94.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0431, device='cuda:0', grad_fn=<MaxBackward1>)
0 30 {0, 1, 2}
1 42 {8, 1}
2 5 {0, 6, 7}
3 15 {5, 7}
4 71 {1, 5}
5 28 {2, 7}
6 12 {8, 9, 3}
7 22 {0, 5}
8 35 {8, 2}
9 14 {9, 5}
Round  41, Devices participated 10, Average loss 0.484, Central accuracy on global test data 79.189, Ensemble accuracy on global test data 78.887, Local accuracy on global train data 47.661, Local accuracy on local train data 95.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 45 {4, 5}
2 55 {4, 7}
3 25 {8, 2}
4 38 {2, 7}
5 91 {2, 6}
6 57 {4, 6}
7 61 {6}
8 83 {9, 2}
9 53 {0, 7}
Round  42, Devices participated 10, Average loss 0.395, Central accuracy on global test data 78.047, Ensemble accuracy on global test data 78.691, Local accuracy on global train data 45.330, Local accuracy on local train data 95.767


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^[[A^[[A^[[A            0 60 {2}
1 42 {8, 1}
2 89 {0, 9}
3 35 {8, 2}
4 69 {2, 3}
5 36 {1, 3}
6 74 {5}
7 53 {0, 7}
8 11 {4, 5}
9 68 {9, 6}
Round  43, Devices participated 10, Average loss 0.318, Central accuracy on global test data 79.482, Ensemble accuracy on global test data 80.352, Local accuracy on global train data 41.750, Local accuracy on local train data 96.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 75 {8, 3}
2 26 {9, 7}
3 31 {0, 1}
4 47 {9, 5}
5 13 {8, 5, 6}
6 36 {1, 3}
7 79 {0, 8}
8 60 {2}
9 12 {8, 9, 3}
Round  44, Devices participated 10, Average loss 0.385, Central accuracy on global test data 79.873, Ensemble accuracy on global test data 79.932, Local accuracy on global train data 48.975, Local accuracy on local train data 95.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 91 {2, 6}
2 12 {8, 9, 3}
3 7 {0, 1}
4 9 {1}
5 23 {1, 5}
6 51 {4, 7}
7 6 {1, 6}
8 43 {8, 3}
9 44 {6, 7}
Round  45, Devices participated 10, Average loss 0.394, Central accuracy on global test data 77.344, Ensemble accuracy on global test data 77.314, Local accuracy on global train data 51.526, Local accuracy on local train data 95.633


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 78 {2, 3, 4}
2 82 {8, 4}
3 68 {9, 6}
4 92 {1, 3}
5 30 {0, 1, 2}
6 12 {8, 9, 3}
7 58 {9, 4}
8 89 {0, 9}
9 44 {6, 7}
Round  46, Devices participated 10, Average loss 0.435, Central accuracy on global test data 79.170, Ensemble accuracy on global test data 79.443, Local accuracy on global train data 54.412, Local accuracy on local train data 94.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 65 {3, 4}
1 80 {6}
2 62 {0, 7}
3 19 {5, 6}
4 51 {4, 7}
5 92 {1, 3}
6 71 {1, 5}
7 48 {3, 4}
8 6 {1, 6}
9 60 {2}
Round  47, Devices participated 10, Average loss 0.236, Central accuracy on global test data 76.104, Ensemble accuracy on global test data 76.836, Local accuracy on global train data 47.893, Local accuracy on local train data 97.567


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 26 {9, 7}
2 5 {0, 6, 7}
3 88 {0}
4 83 {9, 2}
5 62 {0, 7}
6 75 {8, 3}
7 80 {6}
8 87 {8, 9}
9 36 {1, 3}
Round  48, Devices participated 10, Average loss 0.343, Central accuracy on global test data 80.566, Ensemble accuracy on global test data 80.918, Local accuracy on global train data 55.625, Local accuracy on local train data 95.683


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 16 {0, 3}
1 93 {9, 6}
2 17 {3, 4}
3 77 {8, 1}
4 88 {0}
5 91 {2, 6}
6 73 {0, 6}
7 76 {1, 3}
8 31 {0, 1}
9 47 {9, 5}
Round  49, Devices participated 10, Average loss 0.284, Central accuracy on global test data 75.459, Ensemble accuracy on global test data 75.342, Local accuracy on global train data 52.971, Local accuracy on local train data 96.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 5 {0, 6, 7}
2 24 {4, 5}
3 47 {9, 5}
4 98 {4}
5 15 {5, 7}
6 55 {4, 7}
7 58 {9, 4}
8 95 {3, 4}
9 32 {0, 9}
Round  50, Devices participated 10, Average loss 0.335, Central accuracy on global test data 79.189, Ensemble accuracy on global test data 78.955, Local accuracy on global train data 53.831, Local accuracy on local train data 96.017


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 11 {4, 5}
1 80 {6}
2 87 {8, 9}
3 70 {8, 5}
4 6 {1, 6}
5 91 {2, 6}
6 51 {4, 7}
7 89 {0, 9}
8 42 {8, 1}
9 62 {0, 7}
Round  51, Devices participated 10, Average loss 0.343, Central accuracy on global test data 81.963, Ensemble accuracy on global test data 82.324, Local accuracy on global train data 56.799, Local accuracy on local train data 95.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 11 {4, 5}
2 16 {0, 3}
3 81 {2, 3, 7}
4 13 {8, 5, 6}
5 25 {8, 2}
6 33 {5, 6}
7 92 {1, 3}
8 41 {0, 3}
9 48 {3, 4}
Round  52, Devices participated 10, Average loss 0.391, Central accuracy on global test data 74.521, Ensemble accuracy on global test data 76.133, Local accuracy on global train data 52.581, Local accuracy on local train data 95.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 99 {8, 9}
2 22 {0, 5}
3 33 {5, 6}
4 7 {0, 1}
5 8 {8, 7}
6 49 {2, 7}
7 23 {1, 5}
8 79 {0, 8}
9 15 {5, 7}
Round  53, Devices participated 10, Average loss 0.343, Central accuracy on global test data 78.242, Ensemble accuracy on global test data 77.441, Local accuracy on global train data 51.045, Local accuracy on local train data 96.433


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 46 {9, 6}
2 23 {1, 5}
3 84 {0, 9}
4 27 {2, 5}
5 4 {8, 7}
6 32 {0, 9}
7 64 {1, 2}
8 44 {6, 7}
9 79 {0, 8}
Round  54, Devices participated 10, Average loss 0.275, Central accuracy on global test data 77.236, Ensemble accuracy on global test data 76.533, Local accuracy on global train data 45.984, Local accuracy on local train data 97.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 55 {4, 7}
2 23 {1, 5}
3 56 {9, 2}
4 21 {2, 5}
5 52 {8, 7}
6 17 {3, 4}
7 57 {4, 6}
8 99 {8, 9}
9 71 {1, 5}
Round  55, Devices participated 10, Average loss 0.339, Central accuracy on global test data 82.676, Ensemble accuracy on global test data 82.578, Local accuracy on global train data 47.529, Local accuracy on local train data 96.200


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {8, 6, 7}
1 43 {8, 3}
2 85 {1, 4}
3 22 {0, 5}
4 84 {0, 9}
5 0 {1, 2}
6 78 {2, 3, 4}
7 24 {4, 5}
8 65 {3, 4}
9 44 {6, 7}
Round  56, Devices participated 10, Average loss 0.304, Central accuracy on global test data 82.461, Ensemble accuracy on global test data 82.031, Local accuracy on global train data 53.542, Local accuracy on local train data 96.017


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {2}
1 49 {2, 7}
2 0 {1, 2}
3 98 {4}
4 24 {4, 5}
5 42 {8, 1}
6 31 {0, 1}
7 57 {4, 6}
8 67 {8, 1}
9 19 {5, 6}
Round  57, Devices participated 10, Average loss 0.320, Central accuracy on global test data 79.863, Ensemble accuracy on global test data 81.729, Local accuracy on global train data 50.520, Local accuracy on local train data 96.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.3333333333333333, 'layer_hidden2.bias': 0.3333333333333333}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 50 {2, 3}
2 82 {8, 4}
3 54 {1, 6}
4 60 {2}
5 74 {5}
6 75 {8, 3}
7 26 {9, 7}
8 7 {0, 1}
9 1 {8, 4}
Round  58, Devices participated 10, Average loss 0.364, Central accuracy on global test data 82.666, Ensemble accuracy on global test data 83.477, Local accuracy on global train data 47.092, Local accuracy on local train data 95.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {0, 1}
1 79 {0, 8}
2 92 {1, 3}
3 43 {8, 3}
4 17 {3, 4}
5 66 {1, 3}
6 19 {5, 6}
7 51 {4, 7}
8 46 {9, 6}
9 32 {0, 9}
Round  59, Devices participated 10, Average loss 0.325, Central accuracy on global test data 78.496, Ensemble accuracy on global test data 78.955, Local accuracy on global train data 55.020, Local accuracy on local train data 96.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 64 {1, 2}
1 19 {5, 6}
2 42 {8, 1}
3 21 {2, 5}
4 15 {5, 7}
5 50 {2, 3}
6 35 {8, 2}
7 81 {2, 3, 7}
8 65 {3, 4}
9 95 {3, 4}
Round  60, Devices participated 10, Average loss 0.416, Central accuracy on global test data 79.609, Ensemble accuracy on global test data 81.182, Local accuracy on global train data 48.843, Local accuracy on local train data 95.267


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {8, 5, 6}
1 29 {9}
2 19 {5, 6}
3 60 {2}
4 35 {8, 2}
5 25 {8, 2}
6 93 {9, 6}
7 24 {4, 5}
8 18 {0, 6}
9 85 {1, 4}
Round  61, Devices participated 10, Average loss 0.335, Central accuracy on global test data 80.293, Ensemble accuracy on global test data 80.098, Local accuracy on global train data 51.924, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 20 {2, 7}
1 78 {2, 3, 4}
2 96 {1, 3}
3 8 {8, 7}
4 39 {8, 0}
5 71 {1, 5}
6 28 {2, 7}
7 9 {1}
8 32 {0, 9}
9 29 {9}
Round  62, Devices participated 10, Average loss 0.347, Central accuracy on global test data 80.166, Ensemble accuracy on global test data 81.035, Local accuracy on global train data 50.957, Local accuracy on local train data 96.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 82 {8, 4}
1 17 {3, 4}
2 75 {8, 3}
3 68 {9, 6}
4 15 {5, 7}
5 33 {5, 6}
6 22 {0, 5}
7 23 {1, 5}
8 38 {2, 7}
9 90 {8, 6, 7}
Round  63, Devices participated 10, Average loss 0.337, Central accuracy on global test data 80.420, Ensemble accuracy on global test data 82.959, Local accuracy on global train data 54.155, Local accuracy on local train data 95.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {8, 9}
1 59 {0, 7}
2 39 {8, 0}
3 9 {1}
4 35 {8, 2}
5 73 {0, 6}
6 30 {0, 1, 2}
7 75 {8, 3}
8 60 {2}
9 4 {8, 7}
Round  64, Devices participated 10, Average loss 0.440, Central accuracy on global test data 81.523, Ensemble accuracy on global test data 81.699, Local accuracy on global train data 56.072, Local accuracy on local train data 95.533


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {3, 5}
1 97 {4, 7}
2 6 {1, 6}
3 12 {8, 9, 3}
4 13 {8, 5, 6}
5 98 {4}
6 70 {8, 5}
7 9 {1}
8 5 {0, 6, 7}
9 77 {8, 1}
Round  65, Devices participated 10, Average loss 0.430, Central accuracy on global test data 80.332, Ensemble accuracy on global test data 81.895, Local accuracy on global train data 60.688, Local accuracy on local train data 94.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {0, 1, 2}
1 93 {9, 6}
2 88 {0}
3 16 {0, 3}
4 90 {8, 6, 7}
5 26 {9, 7}
6 49 {2, 7}
7 44 {6, 7}
8 81 {2, 3, 7}
9 82 {8, 4}
Round  66, Devices participated 10, Average loss 0.361, Central accuracy on global test data 77.041, Ensemble accuracy on global test data 80.049, Local accuracy on global train data 56.753, Local accuracy on local train data 96.233


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {2, 3, 4}
1 11 {4, 5}
2 52 {8, 7}
3 82 {8, 4}
4 7 {0, 1}
5 62 {0, 7}
6 38 {2, 7}
7 41 {0, 3}
8 46 {9, 6}
9 13 {8, 5, 6}
Round  67, Devices participated 10, Average loss 0.363, Central accuracy on global test data 80.518, Ensemble accuracy on global test data 83.350, Local accuracy on global train data 52.881, Local accuracy on local train data 96.367


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {9, 2}
1 85 {1, 4}
2 24 {4, 5}
3 89 {0, 9}
4 44 {6, 7}
5 18 {0, 6}
6 97 {4, 7}
7 15 {5, 7}
8 6 {1, 6}
9 3 {8, 6}
Round  68, Devices participated 10, Average loss 0.250, Central accuracy on global test data 78.564, Ensemble accuracy on global test data 82.266, Local accuracy on global train data 55.330, Local accuracy on local train data 97.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {9, 5}
1 19 {5, 6}
2 62 {0, 7}
3 61 {6}
4 60 {2}
5 2 {9}
6 87 {8, 9}
7 16 {0, 3}
8 7 {0, 1}
9 89 {0, 9}
Round  69, Devices participated 10, Average loss 0.273, Central accuracy on global test data 73.994, Ensemble accuracy on global test data 74.316, Local accuracy on global train data 51.250, Local accuracy on local train data 97.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3, 7}
1 84 {0, 9}
2 70 {8, 5}
3 26 {9, 7}
4 48 {3, 4}
5 76 {1, 3}
6 97 {4, 7}
7 71 {1, 5}
8 59 {0, 7}
9 30 {0, 1, 2}
Round  70, Devices participated 10, Average loss 0.365, Central accuracy on global test data 80.391, Ensemble accuracy on global test data 80.938, Local accuracy on global train data 58.718, Local accuracy on local train data 95.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 49 {2, 7}
1 2 {9}
2 29 {9}
3 96 {1, 3}
4 71 {1, 5}
5 18 {0, 6}
6 88 {0}
7 40 {3, 5}
8 59 {0, 7}
9 65 {3, 4}
Round  71, Devices participated 10, Average loss 0.321, Central accuracy on global test data 79.404, Ensemble accuracy on global test data 79.092, Local accuracy on global train data 55.361, Local accuracy on local train data 96.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 4 {8, 7}
1 31 {0, 1}
2 50 {2, 3}
3 38 {2, 7}
4 33 {5, 6}
5 51 {4, 7}
6 91 {2, 6}
7 68 {9, 6}
8 81 {2, 3, 7}
9 30 {0, 1, 2}
Round  72, Devices participated 10, Average loss 0.366, Central accuracy on global test data 75.811, Ensemble accuracy on global test data 79.961, Local accuracy on global train data 55.518, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {1, 5}
1 18 {0, 6}
2 65 {3, 4}
3 32 {0, 9}
4 7 {0, 1}
5 92 {1, 3}
6 24 {4, 5}
7 86 {1, 2}
8 53 {0, 7}
9 67 {8, 1}
Round  73, Devices participated 10, Average loss 0.275, Central accuracy on global test data 78.418, Ensemble accuracy on global test data 81.689, Local accuracy on global train data 57.002, Local accuracy on local train data 97.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 86 {1, 2}
2 64 {1, 2}
3 57 {4, 6}
4 67 {8, 1}
5 94 {0, 1}
6 3 {8, 6}
7 19 {5, 6}
8 8 {8, 7}
9 69 {2, 3}
Round  74, Devices participated 10, Average loss 0.474, Central accuracy on global test data 80.088, Ensemble accuracy on global test data 80.000, Local accuracy on global train data 48.530, Local accuracy on local train data 95.217


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {8, 1}
1 26 {9, 7}
2 37 {4, 5, 7}
3 90 {8, 6, 7}
4 47 {9, 5}
5 89 {0, 9}
6 16 {0, 3}
7 65 {3, 4}
8 36 {1, 3}
9 61 {6}
Round  75, Devices participated 10, Average loss 0.338, Central accuracy on global test data 78.701, Ensemble accuracy on global test data 80.498, Local accuracy on global train data 57.698, Local accuracy on local train data 96.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {2, 5}
1 63 {4, 5}
2 77 {8, 1}
3 16 {0, 3}
4 36 {1, 3}
5 28 {2, 7}
6 59 {0, 7}
7 12 {8, 9, 3}
8 97 {4, 7}
9 55 {4, 7}
Round  76, Devices participated 10, Average loss 0.372, Central accuracy on global test data 75.635, Ensemble accuracy on global test data 80.566, Local accuracy on global train data 55.598, Local accuracy on local train data 95.883


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 55 {4, 7}
2 14 {9, 5}
3 80 {6}
4 74 {5}
5 25 {8, 2}
6 26 {9, 7}
7 57 {4, 6}
8 41 {0, 3}
9 22 {0, 5}
Round  77, Devices participated 10, Average loss 0.417, Central accuracy on global test data 78.730, Ensemble accuracy on global test data 78.398, Local accuracy on global train data 55.486, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 68 {9, 6}
1 79 {0, 8}
2 86 {1, 2}
3 13 {8, 5, 6}
4 21 {2, 5}
5 32 {0, 9}
6 95 {3, 4}
7 52 {8, 7}
8 27 {2, 5}
9 47 {9, 5}
Round  78, Devices participated 10, Average loss 0.312, Central accuracy on global test data 82.666, Ensemble accuracy on global test data 82.119, Local accuracy on global train data 49.858, Local accuracy on local train data 96.600


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {2, 7}
1 42 {8, 1}
2 0 {1, 2}
3 39 {8, 0}
4 54 {1, 6}
5 16 {0, 3}
6 33 {5, 6}
7 65 {3, 4}
8 83 {9, 2}
9 5 {0, 6, 7}
Round  79, Devices participated 10, Average loss 0.359, Central accuracy on global test data 76.562, Ensemble accuracy on global test data 82.695, Local accuracy on global train data 53.452, Local accuracy on local train data 96.417


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 19 {5, 6}
2 70 {8, 5}
3 91 {2, 6}
4 10 {3, 7}
5 86 {1, 2}
6 78 {2, 3, 4}
7 81 {2, 3, 7}
8 97 {4, 7}
9 22 {0, 5}
Round  80, Devices participated 10, Average loss 0.515, Central accuracy on global test data 73.350, Ensemble accuracy on global test data 75.684, Local accuracy on global train data 57.471, Local accuracy on local train data 95.000


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 12 {8, 9, 3}
1 79 {0, 8}
2 66 {1, 3}
3 20 {2, 7}
4 2 {9}
5 13 {8, 5, 6}
6 6 {1, 6}
7 19 {5, 6}
8 91 {2, 6}
9 29 {9}
Round  81, Devices participated 10, Average loss 0.364, Central accuracy on global test data 79.883, Ensemble accuracy on global test data 79.873, Local accuracy on global train data 55.454, Local accuracy on local train data 96.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.3333333333333333, 'layer_hidden2.bias': 0.3333333333333333}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 10 {3, 7}
2 21 {2, 5}
3 83 {9, 2}
4 59 {0, 7}
5 3 {8, 6}
6 46 {9, 6}
7 0 {1, 2}
8 16 {0, 3}
9 94 {0, 1}
Round  82, Devices participated 10, Average loss 0.324, Central accuracy on global test data 80.049, Ensemble accuracy on global test data 81.221, Local accuracy on global train data 54.463, Local accuracy on local train data 96.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {2, 5}
1 96 {1, 3}
2 36 {1, 3}
3 17 {3, 4}
4 74 {5}
5 46 {9, 6}
6 73 {0, 6}
7 80 {6}
8 20 {2, 7}
9 40 {3, 5}
Round  83, Devices participated 10, Average loss 0.367, Central accuracy on global test data 76.016, Ensemble accuracy on global test data 79.043, Local accuracy on global train data 53.840, Local accuracy on local train data 95.967


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 1 {8, 4}
1 81 {2, 3, 7}
2 10 {3, 7}
3 91 {2, 6}
4 31 {0, 1}
5 4 {8, 7}
6 42 {8, 1}
7 26 {9, 7}
8 77 {8, 1}
9 99 {8, 9}
Round  84, Devices participated 10, Average loss 0.353, Central accuracy on global test data 78.213, Ensemble accuracy on global test data 76.221, Local accuracy on global train data 54.709, Local accuracy on local train data 95.433


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {0, 9}
1 78 {2, 3, 4}
2 93 {9, 6}
3 75 {8, 3}
4 31 {0, 1}
5 86 {1, 2}
6 7 {0, 1}
7 17 {3, 4}
8 77 {8, 1}
9 28 {2, 7}
Round  85, Devices participated 10, Average loss 0.366, Central accuracy on global test data 78.584, Ensemble accuracy on global test data 80.947, Local accuracy on global train data 57.432, Local accuracy on local train data 95.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {1, 3}
1 22 {0, 5}
2 44 {6, 7}
3 18 {0, 6}
4 68 {9, 6}
5 34 {3, 7}
6 91 {2, 6}
7 98 {4}
8 84 {0, 9}
9 83 {9, 2}
Round  86, Devices participated 10, Average loss 0.299, Central accuracy on global test data 80.508, Ensemble accuracy on global test data 80.879, Local accuracy on global train data 59.155, Local accuracy on local train data 96.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 59 {0, 7}
2 51 {4, 7}
3 42 {8, 1}
4 49 {2, 7}
5 69 {2, 3}
6 52 {8, 7}
7 37 {4, 5, 7}
8 2 {9}
9 4 {8, 7}
Round  87, Devices participated 10, Average loss 0.380, Central accuracy on global test data 77.715, Ensemble accuracy on global test data 76.484, Local accuracy on global train data 54.575, Local accuracy on local train data 96.417


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.3333333333333333, 'layer_hidden2.bias': 0.3333333333333333}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {9, 6}
1 10 {3, 7}
2 27 {2, 5}
3 34 {3, 7}
4 7 {0, 1}
5 76 {1, 3}
6 74 {5}
7 3 {8, 6}
8 59 {0, 7}
9 52 {8, 7}
Round  88, Devices participated 10, Average loss 0.261, Central accuracy on global test data 79.766, Ensemble accuracy on global test data 80.029, Local accuracy on global train data 54.231, Local accuracy on local train data 96.967


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {6, 7}
1 9 {1}
2 73 {0, 6}
3 3 {8, 6}
4 55 {4, 7}
5 81 {2, 3, 7}
6 17 {3, 4}
7 59 {0, 7}
8 97 {4, 7}
9 63 {4, 5}
Round  89, Devices participated 10, Average loss 0.271, Central accuracy on global test data 75.664, Ensemble accuracy on global test data 75.430, Local accuracy on global train data 59.148, Local accuracy on local train data 97.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 63 {4, 5}
1 0 {1, 2}
2 33 {5, 6}
3 88 {0}
4 93 {9, 6}
5 87 {8, 9}
6 72 {1, 9}
7 84 {0, 9}
8 86 {1, 2}
9 20 {2, 7}
Round  90, Devices participated 10, Average loss 0.365, Central accuracy on global test data 78.672, Ensemble accuracy on global test data 78.057, Local accuracy on global train data 51.055, Local accuracy on local train data 96.017


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 19 {5, 6}
1 95 {3, 4}
2 33 {5, 6}
3 92 {1, 3}
4 65 {3, 4}
5 69 {2, 3}
6 81 {2, 3, 7}
7 26 {9, 7}
8 14 {9, 5}
9 52 {8, 7}
Round  91, Devices participated 10, Average loss 0.442, Central accuracy on global test data 74.043, Ensemble accuracy on global test data 74.854, Local accuracy on global train data 53.540, Local accuracy on local train data 94.933


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 22 {0, 5}
1 62 {0, 7}
2 3 {8, 6}
3 14 {9, 5}
4 64 {1, 2}
5 96 {1, 3}
6 33 {5, 6}
7 97 {4, 7}
8 31 {0, 1}
9 42 {8, 1}
Round  92, Devices participated 10, Average loss 0.335, Central accuracy on global test data 81.719, Ensemble accuracy on global test data 83.369, Local accuracy on global train data 56.318, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 37 {4, 5, 7}
1 10 {3, 7}
2 88 {0}
3 63 {4, 5}
4 53 {0, 7}
5 68 {9, 6}
6 2 {9}
7 52 {8, 7}
8 26 {9, 7}
9 59 {0, 7}
Round  93, Devices participated 10, Average loss 0.284, Central accuracy on global test data 76.885, Ensemble accuracy on global test data 76.797, Local accuracy on global train data 55.862, Local accuracy on local train data 96.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 71 {1, 5}
2 85 {1, 4}
3 49 {2, 7}
4 20 {2, 7}
5 10 {3, 7}
6 3 {8, 6}
7 86 {1, 2}
8 88 {0}
9 33 {5, 6}
Round  94, Devices participated 10, Average loss 0.294, Central accuracy on global test data 76.699, Ensemble accuracy on global test data 79.111, Local accuracy on global train data 51.912, Local accuracy on local train data 96.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {1}
1 0 {1, 2}
2 18 {0, 6}
3 66 {1, 3}
4 44 {6, 7}
5 17 {3, 4}
6 92 {1, 3}
7 95 {3, 4}
8 64 {1, 2}
9 96 {1, 3}
Round  95, Devices participated 10, Average loss 0.272, Central accuracy on global test data 71.318, Ensemble accuracy on global test data 72.725, Local accuracy on global train data 57.183, Local accuracy on local train data 96.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {8, 5}
1 58 {9, 4}
2 24 {4, 5}
3 57 {4, 6}
4 32 {0, 9}
5 19 {5, 6}
6 43 {8, 3}
7 65 {3, 4}
8 5 {0, 6, 7}
9 88 {0}
Round  96, Devices participated 10, Average loss 0.397, Central accuracy on global test data 79.053, Ensemble accuracy on global test data 80.791, Local accuracy on global train data 57.805, Local accuracy on local train data 94.800


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {6}
1 75 {8, 3}
2 68 {9, 6}
3 2 {9}
4 6 {1, 6}
5 26 {9, 7}
6 1 {8, 4}
7 56 {9, 2}
8 78 {2, 3, 4}
9 74 {5}
Round  97, Devices participated 10, Average loss 0.334, Central accuracy on global test data 75.146, Ensemble accuracy on global test data 77.598, Local accuracy on global train data 53.904, Local accuracy on local train data 95.600


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 57 {4, 6}
2 22 {0, 5}
3 90 {8, 6, 7}
4 46 {9, 6}
5 9 {1}
6 56 {9, 2}
7 99 {8, 9}
8 89 {0, 9}
9 4 {8, 7}
Round  98, Devices participated 10, Average loss 0.326, Central accuracy on global test data 78.652, Ensemble accuracy on global test data 80.752, Local accuracy on global train data 54.758, Local accuracy on local train data 96.500


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {9, 2}
1 41 {0, 3}
2 69 {2, 3}
3 7 {0, 1}
4 63 {4, 5}
5 51 {4, 7}
6 78 {2, 3, 4}
7 89 {0, 9}
8 43 {8, 3}
9 81 {2, 3, 7}
Round  99, Devices participated 10, Average loss 0.503, Central accuracy on global test data 76.670, Ensemble accuracy on global test data 75.859, Local accuracy on global train data 54.307, Local accuracy on local train data 94.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {5}
1 76 {1, 3}
2 14 {9, 5}
3 98 {4}
4 73 {0, 6}
5 37 {4, 5, 7}
6 94 {0, 1}
7 68 {9, 6}
8 39 {8, 0}
9 11 {4, 5}
Round 100, Devices participated 10, Average loss 0.293, Central accuracy on global test data 81.084, Ensemble accuracy on global test data 79.795, Local accuracy on global train data 54.028, Local accuracy on local train data 97.017


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 38 {2, 7}
1 78 {2, 3, 4}
2 17 {3, 4}
3 56 {9, 2}
4 90 {8, 6, 7}
5 34 {3, 7}
6 42 {8, 1}
7 0 {1, 2}
8 1 {8, 4}
9 12 {8, 9, 3}
Round 101, Devices participated 10, Average loss 0.410, Central accuracy on global test data 78.389, Ensemble accuracy on global test data 80.479, Local accuracy on global train data 51.462, Local accuracy on local train data 95.450


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {4, 5}
1 36 {1, 3}
2 95 {3, 4}
3 8 {8, 7}
4 99 {8, 9}
5 75 {8, 3}
6 93 {9, 6}
7 4 {8, 7}
8 24 {4, 5}
9 58 {9, 4}
Round 102, Devices participated 10, Average loss 0.369, Central accuracy on global test data 79.541, Ensemble accuracy on global test data 81.152, Local accuracy on global train data 52.388, Local accuracy on local train data 95.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 66 {1, 3}
1 46 {9, 6}
2 89 {0, 9}
3 23 {1, 5}
4 4 {8, 7}
5 97 {4, 7}
6 78 {2, 3, 4}
7 26 {9, 7}
8 79 {0, 8}
9 52 {8, 7}
Round 103, Devices participated 10, Average loss 0.336, Central accuracy on global test data 81.865, Ensemble accuracy on global test data 82.100, Local accuracy on global train data 57.617, Local accuracy on local train data 95.967


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 71 {1, 5}
1 79 {0, 8}
2 23 {1, 5}
3 7 {0, 1}
4 97 {4, 7}
5 86 {1, 2}
6 72 {1, 9}
7 57 {4, 6}
8 70 {8, 5}
9 64 {1, 2}
Round 104, Devices participated 10, Average loss 0.380, Central accuracy on global test data 83.037, Ensemble accuracy on global test data 81.895, Local accuracy on global train data 57.512, Local accuracy on local train data 96.050


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 55 {4, 7}
1 14 {9, 5}
2 21 {2, 5}
3 43 {8, 3}
4 5 {0, 6, 7}
5 30 {0, 1, 2}
6 35 {8, 2}
7 15 {5, 7}
8 28 {2, 7}
9 34 {3, 7}
Round 105, Devices participated 10, Average loss 0.469, Central accuracy on global test data 78.066, Ensemble accuracy on global test data 78.174, Local accuracy on global train data 55.010, Local accuracy on local train data 94.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {1, 6}
1 51 {4, 7}
2 22 {0, 5}
3 85 {1, 4}
4 73 {0, 6}
5 90 {8, 6, 7}
6 4 {8, 7}
7 69 {2, 3}
8 75 {8, 3}
9 6 {1, 6}
Round 106, Devices participated 10, Average loss 0.387, Central accuracy on global test data 80.703, Ensemble accuracy on global test data 81.885, Local accuracy on global train data 61.836, Local accuracy on local train data 95.817


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {2, 3, 4}
1 98 {4}
2 48 {3, 4}
3 28 {2, 7}
4 76 {1, 3}
5 96 {1, 3}
6 97 {4, 7}
7 69 {2, 3}
8 66 {1, 3}
9 65 {3, 4}
Round 107, Devices participated 10, Average loss 0.438, Central accuracy on global test data 66.572, Ensemble accuracy on global test data 65.020, Local accuracy on global train data 56.001, Local accuracy on local train data 95.583


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {8, 7}
1 54 {1, 6}
2 70 {8, 5}
3 39 {8, 0}
4 89 {0, 9}
5 14 {9, 5}
6 77 {8, 1}
7 74 {5}
8 15 {5, 7}
9 88 {0}
Round 108, Devices participated 10, Average loss 0.309, Central accuracy on global test data 80.225, Ensemble accuracy on global test data 78.955, Local accuracy on global train data 53.823, Local accuracy on local train data 96.433


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {9, 4}
1 12 {8, 9, 3}
2 87 {8, 9}
3 84 {0, 9}
4 91 {2, 6}
5 62 {0, 7}
6 82 {8, 4}
7 57 {4, 6}
8 6 {1, 6}
9 21 {2, 5}
Round 109, Devices participated 10, Average loss 0.413, Central accuracy on global test data 80.615, Ensemble accuracy on global test data 82.471, Local accuracy on global train data 59.604, Local accuracy on local train data 94.633


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 99 {8, 9}
2 54 {1, 6}
3 55 {4, 7}
4 14 {9, 5}
5 63 {4, 5}
6 7 {0, 1}
7 88 {0}
8 91 {2, 6}
9 52 {8, 7}
Round 110, Devices participated 10, Average loss 0.292, Central accuracy on global test data 82.686, Ensemble accuracy on global test data 81.748, Local accuracy on global train data 59.414, Local accuracy on local train data 96.433


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {6, 7}
1 73 {0, 6}
2 39 {8, 0}
3 28 {2, 7}
4 18 {0, 6}
5 92 {1, 3}
6 52 {8, 7}
7 79 {0, 8}
8 54 {1, 6}
9 77 {8, 1}
Round 111, Devices participated 10, Average loss 0.318, Central accuracy on global test data 72.324, Ensemble accuracy on global test data 74.199, Local accuracy on global train data 57.375, Local accuracy on local train data 96.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {0, 7}
1 70 {8, 5}
2 54 {1, 6}
3 27 {2, 5}
4 89 {0, 9}
5 21 {2, 5}
6 42 {8, 1}
7 25 {8, 2}
8 60 {2}
9 49 {2, 7}
Round 112, Devices participated 10, Average loss 0.399, Central accuracy on global test data 77.324, Ensemble accuracy on global test data 77.734, Local accuracy on global train data 50.696, Local accuracy on local train data 96.100


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 2 {9}
1 56 {9, 2}
2 59 {0, 7}
3 80 {6}
4 62 {0, 7}
5 19 {5, 6}
6 42 {8, 1}
7 33 {5, 6}
8 72 {1, 9}
9 35 {8, 2}
Round 113, Devices participated 10, Average loss 0.339, Central accuracy on global test data 76.699, Ensemble accuracy on global test data 79.961, Local accuracy on global train data 52.097, Local accuracy on local train data 96.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.3333333333333333, 'layer_hidden2.bias': 0.3333333333333333}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 53 {0, 7}
1 17 {3, 4}
2 11 {4, 5}
3 78 {2, 3, 4}
4 66 {1, 3}
5 39 {8, 0}
6 94 {0, 1}
7 4 {8, 7}
8 47 {9, 5}
9 90 {8, 6, 7}
Round 114, Devices participated 10, Average loss 0.340, Central accuracy on global test data 81.318, Ensemble accuracy on global test data 80.410, Local accuracy on global train data 56.843, Local accuracy on local train data 96.450


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 48 {3, 4}
1 51 {4, 7}
2 34 {3, 7}
3 78 {2, 3, 4}
4 92 {1, 3}
5 54 {1, 6}
6 96 {1, 3}
7 84 {0, 9}
8 88 {0}
9 74 {5}
Round 115, Devices participated 10, Average loss 0.269, Central accuracy on global test data 80.762, Ensemble accuracy on global test data 78.789, Local accuracy on global train data 58.186, Local accuracy on local train data 97.233


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 15 {5, 7}
1 25 {8, 2}
2 50 {2, 3}
3 89 {0, 9}
4 72 {1, 9}
5 20 {2, 7}
6 85 {1, 4}
7 47 {9, 5}
8 1 {8, 4}
9 3 {8, 6}
Round 116, Devices participated 10, Average loss 0.403, Central accuracy on global test data 78.369, Ensemble accuracy on global test data 83.359, Local accuracy on global train data 54.631, Local accuracy on local train data 95.717


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 91 {2, 6}
1 39 {8, 0}
2 11 {4, 5}
3 12 {8, 9, 3}
4 93 {9, 6}
5 64 {1, 2}
6 30 {0, 1, 2}
7 79 {0, 8}
8 29 {9}
9 49 {2, 7}
Round 117, Devices participated 10, Average loss 0.422, Central accuracy on global test data 72.002, Ensemble accuracy on global test data 81.738, Local accuracy on global train data 57.722, Local accuracy on local train data 95.700


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2, 5}
1 88 {0}
2 95 {3, 4}
3 65 {3, 4}
4 60 {2}
5 8 {8, 7}
6 46 {9, 6}
7 31 {0, 1}
8 63 {4, 5}
9 72 {1, 9}
Round 118, Devices participated 10, Average loss 0.229, Central accuracy on global test data 82.119, Ensemble accuracy on global test data 83.740, Local accuracy on global train data 51.687, Local accuracy on local train data 97.483


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {1, 3}
1 63 {4, 5}
2 58 {9, 4}
3 77 {8, 1}
4 95 {3, 4}
5 23 {1, 5}
6 61 {6}
7 29 {9}
8 91 {2, 6}
9 59 {0, 7}
Round 119, Devices participated 10, Average loss 0.277, Central accuracy on global test data 82.881, Ensemble accuracy on global test data 83.408, Local accuracy on global train data 59.146, Local accuracy on local train data 96.317


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 8 {8, 7}
1 69 {2, 3}
2 44 {6, 7}
3 89 {0, 9}
4 49 {2, 7}
5 36 {1, 3}
6 77 {8, 1}
7 19 {5, 6}
8 92 {1, 3}
9 72 {1, 9}
Round 120, Devices participated 10, Average loss 0.409, Central accuracy on global test data 82.109, Ensemble accuracy on global test data 80.654, Local accuracy on global train data 54.004, Local accuracy on local train data 95.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 47 {9, 5}
1 49 {2, 7}
2 1 {8, 4}
3 80 {6}
4 27 {2, 5}
5 9 {1}
6 83 {9, 2}
7 72 {1, 9}
8 76 {1, 3}
9 92 {1, 3}
Round 121, Devices participated 10, Average loss 0.289, Central accuracy on global test data 79.160, Ensemble accuracy on global test data 78.389, Local accuracy on global train data 52.297, Local accuracy on local train data 96.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {3, 7}
1 57 {4, 6}
2 54 {1, 6}
3 1 {8, 4}
4 80 {6}
5 76 {1, 3}
6 56 {9, 2}
7 39 {8, 0}
8 14 {9, 5}
9 20 {2, 7}
Round 122, Devices participated 10, Average loss 0.306, Central accuracy on global test data 82.539, Ensemble accuracy on global test data 82.227, Local accuracy on global train data 55.376, Local accuracy on local train data 96.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 6 {1, 6}
1 62 {0, 7}
2 31 {0, 1}
3 90 {8, 6, 7}
4 3 {8, 6}
5 51 {4, 7}
6 24 {4, 5}
7 91 {2, 6}
8 23 {1, 5}
9 55 {4, 7}
Round 123, Devices participated 10, Average loss 0.275, Central accuracy on global test data 78.262, Ensemble accuracy on global test data 79.902, Local accuracy on global train data 58.350, Local accuracy on local train data 96.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {4}
1 18 {0, 6}
2 78 {2, 3, 4}
3 89 {0, 9}
4 27 {2, 5}
5 80 {6}
6 33 {5, 6}
7 86 {1, 2}
8 88 {0}
9 15 {5, 7}
Round 124, Devices participated 10, Average loss 0.275, Central accuracy on global test data 78.066, Ensemble accuracy on global test data 83.721, Local accuracy on global train data 60.242, Local accuracy on local train data 97.417


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {1, 4}
1 12 {8, 9, 3}
2 59 {0, 7}
3 60 {2}
4 73 {0, 6}
5 80 {6}
6 0 {1, 2}
7 10 {3, 7}
8 23 {1, 5}
9 13 {8, 5, 6}
Round 125, Devices participated 10, Average loss 0.302, Central accuracy on global test data 75.117, Ensemble accuracy on global test data 81.895, Local accuracy on global train data 58.945, Local accuracy on local train data 96.633


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/deepedge/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8, 2}
1 46 {9, 6}
2 54 {1, 6}
3 87 {8, 9}
4 84 {0, 9}
5 51 {4, 7}
6 27 {2, 5}
7 95 {3, 4}
8 21 {2, 5}
9 61 {6}
Round 126, Devices participated 10, Average loss 0.331, Central accuracy on global test data 78.184, Ensemble accuracy on global test data 81.504, Local accuracy on global train data 55.083, Local accuracy on local train data 96.483


^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 142, in <module>
    acc_l, _ = test_img(local_user[user_idx].net, dataset_train, args, stop_at_batch=16, shuffle=True)
  File "/workspace/deepedge/models/test.py", line 21, in test_img
    for idx, (data, target) in enumerate(data_loader):
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py", line 97, in __getitem__
    img = self.transform(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 70, in __call__
    img = t(img)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 175, in __call__
    return F.normalize(tensor, self.mean, self.std, self.inplace)
  File "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py", line 213, in normalize
    tensor = tensor.clone()
KeyboardInterrupt
