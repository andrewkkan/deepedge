python main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_refresh 0 --store_models testtemp_hard_gen2 --alpha_scale 0.5 --noniid_hard

def DFAN_regavg(args, teacher, student, generator, optimizer, epoch):

    for local in teacher:
        local.eval()
    generator.train()
    student.train()
    loss_G = torch.tensor(0.0)
    loss_S = torch.tensor(0.0)
    optimizer_S, optimizer_G = optimizer
    sm = torch.nn.Softmax()

    # w_locals = []
    # for local in teacher:
    #     w_locals.append(copy.deepcopy(local.state_dict()))
    # w_fedavg = FedAvg(w_locals)

    # mlpa, mlpt = model_fusion_MLP(teacher, 1024, 200, 10)
    # mlpa, mlpt = mlpa.to(args.device), mlpt.to(args.device)
    # w_mlpt = mlpt.state_dict()
    mlpa, mlpt = fusion_layer_MLP(teacher, student, 1024, 200, 10)
    mlpa = mlpa.to(args.device)
    mlpt = mlpt.to(args.device)
    mlpa.eval()
    mlpt.eval()
    print(mlpt.alpha)

    w_fedavg = mlpt.state_dict()

    for i in range(args.epoch_itrs):
        for k in range(2):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            optimizer_G.zero_grad()
            fake = generator(z)
            fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
            s_logit = student(fake)
            # t_sm = ensemble(teacher, fake, detach=False, mode=args.ensemble_mode)
            # t_logit = torch.zeros_like(teacher[0](fake))
            # for k in range(10):
            #     t_logit += teacher[k](fake) 
            t_logit = mlpa(fake)
            oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
            max_Gout = torch.max(torch.abs(fake))
            if max_Gout > 8.0:
                loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0) + torch.pow(max_Gout - 8.0, 2.0)
                print(max_Gout)
            else:
                loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0)
            loss_G.backward()                   
            optimizer_G.step()
        for j in range(5):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            optimizer_S.zero_grad()
            fake = generator(z).detach()
            fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
            s_logit = student(fake)
            # t_sm = ensemble(teacher, fake, detach=True, mode=args.ensemble_mode)
            # t_logit = torch.zeros_like(teacher[0](fake).detach())
            # for k in range(10):
            #     t_logit += teacher[k](fake).detach()
            t_logit = mlpa(fake).detach()
            oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
            loss_S1 = torch.log(1. / (2. - oneMinus_P_S))

            diff_L2 = torch.FloatTensor([0.]).to(args.device)
            for ln, student_w, fedavg_w in zip(student.state_dict().keys(), student.parameters(), w_fedavg.values()):
                diff_L2 += (fedavg_w - student_w).norm(2) * mlpt.alpha[ln] * args.alpha_scale
            if epoch == 0:
                loss_S2 = 0
            else:
                loss_S2 = diff_L2

            # diff_L2 = torch.FloatTensor([0.]).to(args.device)
            # for student_w, mlpt_w in zip(student.parameters(), w_mlpt.values()):
            #     diff_L2 += ((mlpt_w - student_w)*mlpt_w.abs()).norm(2)
            # loss_S2 = diff_L2 * alpha

            loss_S = loss_S1 + loss_S2
            loss_S.backward()
            optimizer_S.step()

        if args.verbose and i % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tG_Loss: {:.6f} S_loss: {:.6f}'.format(
                epoch, i, args.epoch_itrs, 100 * float(i) / float(args.epoch_itrs), loss_G.item(), loss_S.item()))        #vp.add_scalar('Loss_S', (epoch-1)*args.epoch_itrs+i, loss_S.item())
            #vp.add_scalar('Loss_G', (epoch-1)*args.epoch_itrs+i, loss_G.item())


]0;root@995f9cfe3fea: /workspaceroot@995f9cfe3fea:/workspace# 
]0;root@995f9cfe3fea: /workspaceroot@995f9cfe3fea:/workspace# [Apython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels
s 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_refresh 0
0 --store_models testtemp_hard_gen2 --alpha_scale 0.5 --noniid_hardniid_hard[A[A[A[K

[K

[K[A[Aroot@995f9cfe3fea:/workspace# [Apython main_dfan_regavg.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels
s 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 800 --nn_refresh 0
0 --store_models testtemp_hard_gen2 --alpha_scale 0.5 --noniid_hard[1@_[1@m[1@p[1P[1@l[1@p[1@t
/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.4444444444444444, 'layer_hidden2.bias': 0.4444444444444444}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(9.6700, device='cuda:0', grad_fn=<MaxBackward1>)
0 49 {7}
1 29 {3}
2 56 {0}
3 42 {7}
4 35 {8}
5 17 {8}
6 45 {6}
7 88 {6, 7}
8 62 {0}
9 0 {8, 7}
Round   0, Devices participated 10, Average loss 0.257, Central accuracy on global test data 31.045, Ensemble accuracy on global test data 19.756, Local accuracy on global train data 12.100, Local accuracy on local train data 96.250


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 53 {1}
1 34 {3}
2 14 {6}
3 50 {8}
4 37 {9}
5 48 {4}
6 89 {9}
7 86 {0}
8 15 {2}
9 55 {3}
Round   1, Devices participated 10, Average loss 0.340, Central accuracy on global test data 34.863, Ensemble accuracy on global test data 32.188, Local accuracy on global train data 13.137, Local accuracy on local train data 96.817


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 10 {7}
1 4 {8}
2 94 {2, 3}
3 27 {9}
4 42 {7}
5 23 {7}
6 19 {9}
7 33 {0}
8 70 {5}
9 59 {3}
Round   2, Devices participated 10, Average loss 0.135, Central accuracy on global test data 38.613, Ensemble accuracy on global test data 40.244, Local accuracy on global train data 17.549, Local accuracy on local train data 97.983


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {1}
1 46 {3}
2 3 {9}
3 1 {4}
4 55 {3}
5 39 {1}
6 8 {0}
7 93 {9}
8 31 {4}
9 47 {5}
Round   3, Devices participated 10, Average loss 0.113, Central accuracy on global test data 40.088, Ensemble accuracy on global test data 38.643, Local accuracy on global train data 17.534, Local accuracy on local train data 98.117


{'layer_input.weight': 0.6666666666666666, 'layer_input.bias': 0.6666666666666666, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 84 {1}
2 20 {2}
3 10 {7}
4 95 {3}
5 73 {1, 2}
6 17 {8}
7 67 {1}
8 94 {2, 3}
9 3 {9}
Round   4, Devices participated 10, Average loss 0.077, Central accuracy on global test data 41.719, Ensemble accuracy on global test data 42.559, Local accuracy on global train data 19.963, Local accuracy on local train data 98.133


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 29 {3}
1 80 {1}
2 55 {3}
3 64 {7}
4 19 {9}
5 10 {7}
6 58 {8}
7 67 {1}
8 63 {5}
9 83 {7}
Round   5, Devices participated 10, Average loss 0.044, Central accuracy on global test data 41.924, Ensemble accuracy on global test data 41.455, Local accuracy on global train data 18.640, Local accuracy on local train data 99.033


{'layer_input.weight': 0.5555555555555556, 'layer_input.bias': 0.5555555555555556, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 83 {7}
1 40 {2}
2 99 {5}
3 23 {7}
4 60 {8}
5 26 {1}
6 27 {9}
7 41 {8, 9}
8 79 {6}
9 28 {6}
Round   6, Devices participated 10, Average loss 0.074, Central accuracy on global test data 45.596, Ensemble accuracy on global test data 47.139, Local accuracy on global train data 18.066, Local accuracy on local train data 98.200


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {3}
1 79 {6}
2 30 {8}
3 34 {3}
4 84 {1}
5 2 {4}
6 90 {5}
7 99 {5}
8 69 {5}
9 98 {5}
Round   7, Devices participated 10, Average loss 0.062, Central accuracy on global test data 31.914, Ensemble accuracy on global test data 30.566, Local accuracy on global train data 15.549, Local accuracy on local train data 98.650


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 57 {9}
1 81 {0}
2 96 {0}
3 35 {8}
4 56 {0}
5 24 {2}
6 41 {8, 9}
7 17 {8}
8 70 {5}
9 22 {3}
Round   8, Devices participated 10, Average loss 0.120, Central accuracy on global test data 50.664, Ensemble accuracy on global test data 45.537, Local accuracy on global train data 16.313, Local accuracy on local train data 98.100


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.2167, device='cuda:0', grad_fn=<MaxBackward1>)
0 51 {5, 6}
1 91 {1}
2 83 {7}
3 28 {6}
4 87 {1}
5 16 {7}
6 59 {3}
7 55 {3}
8 52 {1}
9 92 {8}
Round   9, Devices participated 10, Average loss 0.055, Central accuracy on global test data 47.031, Ensemble accuracy on global test data 49.912, Local accuracy on global train data 24.795, Local accuracy on local train data 98.917


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8}
1 86 {0}
2 66 {6}
3 56 {0}
4 91 {1}
5 50 {8}
6 77 {6}
7 96 {0}
8 48 {4}
9 81 {0}
Round  10, Devices participated 10, Average loss 0.094, Central accuracy on global test data 42.119, Ensemble accuracy on global test data 46.475, Local accuracy on global train data 24.719, Local accuracy on local train data 99.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 98 {5}
1 43 {0}
2 38 {4, 5}
3 64 {7}
4 39 {1}
5 36 {5}
6 90 {5}
7 28 {6}
8 74 {4}
9 34 {3}
Round  11, Devices participated 10, Average loss 0.128, Central accuracy on global test data 41.904, Ensemble accuracy on global test data 44.307, Local accuracy on global train data 21.113, Local accuracy on local train data 98.633


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {5}
1 47 {5}
2 62 {0}
3 28 {6}
4 77 {6}
5 37 {9}
6 14 {6}
7 36 {5}
8 88 {6, 7}
9 71 {3, 4}
Round  12, Devices participated 10, Average loss 0.082, Central accuracy on global test data 44.902, Ensemble accuracy on global test data 42.959, Local accuracy on global train data 23.662, Local accuracy on local train data 98.717


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 48 {4}
1 24 {2}
2 40 {2}
3 33 {0}
4 91 {1}
5 70 {5}
6 30 {8}
7 77 {6}
8 69 {5}
9 63 {5}
Round  13, Devices participated 10, Average loss 0.086, Central accuracy on global test data 48.975, Ensemble accuracy on global test data 50.869, Local accuracy on global train data 21.619, Local accuracy on local train data 98.867


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 3 {9}
1 93 {9}
2 67 {1}
3 11 {4}
4 75 {4}
5 56 {0}
6 52 {1}
7 48 {4}
8 18 {0}
9 64 {7}
Round  14, Devices participated 10, Average loss 0.125, Central accuracy on global test data 51.299, Ensemble accuracy on global test data 50.225, Local accuracy on global train data 29.436, Local accuracy on local train data 98.783


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 17 {8}
1 56 {0}
2 6 {2}
3 55 {3}
4 57 {9}
5 4 {8}
6 11 {4}
7 44 {2}
8 74 {4}
9 27 {9}
Round  15, Devices participated 10, Average loss 0.129, Central accuracy on global test data 48.369, Ensemble accuracy on global test data 46.309, Local accuracy on global train data 22.603, Local accuracy on local train data 98.517


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0828, device='cuda:0', grad_fn=<MaxBackward1>)
0 29 {3}
1 25 {6}
2 28 {6}
3 22 {3}
4 72 {5}
5 5 {7}
6 33 {0}
7 93 {9}
8 70 {5}
9 84 {1}
Round  16, Devices participated 10, Average loss 0.123, Central accuracy on global test data 46.348, Ensemble accuracy on global test data 53.047, Local accuracy on global train data 25.056, Local accuracy on local train data 98.350


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {2}
1 10 {7}
2 22 {3}
3 5 {7}
4 1 {4}
5 76 {0, 1}
6 61 {1}
7 96 {0}
8 94 {2, 3}
9 38 {4, 5}
Round  17, Devices participated 10, Average loss 0.110, Central accuracy on global test data 48.291, Ensemble accuracy on global test data 51.045, Local accuracy on global train data 29.080, Local accuracy on local train data 98.067


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 50 {8}
1 66 {6}
2 76 {0, 1}
3 47 {5}
4 84 {1}
5 54 {7}
6 91 {1}
7 56 {0}
8 42 {7}
9 96 {0}
Round  18, Devices participated 10, Average loss 0.056, Central accuracy on global test data 47.246, Ensemble accuracy on global test data 48.506, Local accuracy on global train data 27.214, Local accuracy on local train data 98.833


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 54 {7}
2 42 {7}
3 2 {4}
4 60 {8}
5 56 {0}
6 49 {7}
7 4 {8}
8 88 {6, 7}
9 66 {6}
Round  19, Devices participated 10, Average loss 0.096, Central accuracy on global test data 47.012, Ensemble accuracy on global test data 44.014, Local accuracy on global train data 22.676, Local accuracy on local train data 98.567


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 79 {6}
1 63 {5}
2 20 {2}
3 82 {6}
4 33 {0}
5 71 {3, 4}
6 86 {0}
7 75 {4}
8 38 {4, 5}
9 81 {0}
Round  20, Devices participated 10, Average loss 0.087, Central accuracy on global test data 42.861, Ensemble accuracy on global test data 48.037, Local accuracy on global train data 27.253, Local accuracy on local train data 98.583


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 30 {8}
1 27 {9}
2 45 {6}
3 43 {0}
4 35 {8}
5 86 {0}
6 24 {2}
7 33 {0}
8 73 {1, 2}
9 88 {6, 7}
Round  21, Devices participated 10, Average loss 0.142, Central accuracy on global test data 33.076, Ensemble accuracy on global test data 49.639, Local accuracy on global train data 23.909, Local accuracy on local train data 98.017


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 14 {6}
2 98 {5}
3 54 {7}
4 97 {3}
5 8 {0}
6 16 {7}
7 31 {4}
8 40 {2}
9 83 {7}
Round  22, Devices participated 10, Average loss 0.125, Central accuracy on global test data 35.645, Ensemble accuracy on global test data 39.980, Local accuracy on global train data 20.925, Local accuracy on local train data 98.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {1}
1 87 {1}
2 31 {4}
3 70 {5}
4 36 {5}
5 99 {5}
6 83 {7}
7 11 {4}
8 45 {6}
9 47 {5}
Round  23, Devices participated 10, Average loss 0.071, Central accuracy on global test data 40.537, Ensemble accuracy on global test data 38.311, Local accuracy on global train data 16.758, Local accuracy on local train data 98.433


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 0.6666666666666666, 'layer_hidden2.bias': 0.6666666666666666}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 28 {6}
1 18 {0}
2 27 {9}
3 93 {9}
4 87 {1}
5 70 {5}
6 14 {6}
7 88 {6, 7}
8 24 {2}
9 91 {1}
Round  24, Devices participated 10, Average loss 0.102, Central accuracy on global test data 45.332, Ensemble accuracy on global test data 51.748, Local accuracy on global train data 22.808, Local accuracy on local train data 98.817


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 56 {0}
1 46 {3}
2 65 {9}
3 5 {7}
4 25 {6}
5 63 {5}
6 35 {8}
7 32 {2}
8 36 {5}
9 33 {0}
Round  25, Devices participated 10, Average loss 0.092, Central accuracy on global test data 50.645, Ensemble accuracy on global test data 54.746, Local accuracy on global train data 20.610, Local accuracy on local train data 98.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 94 {2, 3}
1 6 {2}
2 34 {3}
3 55 {3}
4 87 {1}
5 23 {7}
6 82 {6}
7 41 {8, 9}
8 63 {5}
9 93 {9}
Round  26, Devices participated 10, Average loss 0.141, Central accuracy on global test data 53.096, Ensemble accuracy on global test data 53.135, Local accuracy on global train data 21.687, Local accuracy on local train data 97.950


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.4444444444444444, 'layer_hidden1.bias': 0.4444444444444444, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 43 {0}
1 96 {0}
2 57 {9}
3 69 {5}
4 91 {1}
5 17 {8}
6 33 {0}
7 39 {1}
8 93 {9}
9 36 {5}
Round  27, Devices participated 10, Average loss 0.079, Central accuracy on global test data 51.523, Ensemble accuracy on global test data 51.914, Local accuracy on global train data 19.531, Local accuracy on local train data 99.083


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 22 {3}
1 47 {5}
2 61 {1}
3 27 {9}
4 50 {8}
5 96 {0}
6 89 {9}
7 97 {3}
8 85 {2}
9 30 {8}
Round  28, Devices participated 10, Average loss 0.122, Central accuracy on global test data 22.090, Ensemble accuracy on global test data 19.688, Local accuracy on global train data 15.205, Local accuracy on local train data 98.900


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {2}
1 70 {5}
2 23 {7}
3 38 {4, 5}
4 74 {4}
5 21 {2}
6 82 {6}
7 4 {8}
8 88 {6, 7}
9 98 {5}
Round  29, Devices participated 10, Average loss 0.319, Central accuracy on global test data 38.008, Ensemble accuracy on global test data 42.217, Local accuracy on global train data 14.973, Local accuracy on local train data 98.117


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 90 {5}
1 72 {5}
2 65 {9}
3 50 {8}
4 85 {2}
5 56 {0}
6 4 {8}
7 80 {1}
8 31 {4}
9 17 {8}
Round  30, Devices participated 10, Average loss 0.251, Central accuracy on global test data 48.604, Ensemble accuracy on global test data 50.410, Local accuracy on global train data 16.443, Local accuracy on local train data 98.583


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 0.3333333333333333, 'layer_hidden1.bias': 0.3333333333333333, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 40 {2}
2 41 {8, 9}
3 34 {3}
4 97 {3}
5 51 {5, 6}
6 50 {8}
7 13 {4}
8 22 {3}
9 8 {0}
Round  31, Devices participated 10, Average loss 0.130, Central accuracy on global test data 41.514, Ensemble accuracy on global test data 30.762, Local accuracy on global train data 20.430, Local accuracy on local train data 98.333


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {1}
1 54 {7}
2 63 {5}
3 83 {7}
4 96 {0}
5 26 {1}
6 0 {8, 7}
7 59 {3}
8 69 {5}
9 14 {6}
Round  32, Devices participated 10, Average loss 0.078, Central accuracy on global test data 52.637, Ensemble accuracy on global test data 52.842, Local accuracy on global train data 25.618, Local accuracy on local train data 98.750


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {8}
1 47 {5}
2 83 {7}
3 36 {5}
4 8 {0}
5 2 {4}
6 56 {0}
7 49 {7}
8 10 {7}
9 38 {4, 5}
Round  33, Devices participated 10, Average loss 0.041, Central accuracy on global test data 47.900, Ensemble accuracy on global test data 48.506, Local accuracy on global train data 24.731, Local accuracy on local train data 99.117


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 90 {5}
2 81 {0}
3 54 {7}
4 59 {3}
5 4 {8}
6 53 {1}
7 47 {5}
8 52 {1}
9 19 {9}
Round  34, Devices participated 10, Average loss 0.080, Central accuracy on global test data 47.158, Ensemble accuracy on global test data 49.189, Local accuracy on global train data 21.670, Local accuracy on local train data 99.017


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 24 {2}
1 13 {4}
2 81 {0}
3 73 {1, 2}
4 19 {9}
5 55 {3}
6 40 {2}
7 76 {0, 1}
8 57 {9}
9 96 {0}
Round  35, Devices participated 10, Average loss 0.139, Central accuracy on global test data 37.500, Ensemble accuracy on global test data 41.494, Local accuracy on global train data 22.090, Local accuracy on local train data 98.567


{'layer_input.weight': 0.2222222222222222, 'layer_input.bias': 0.2222222222222222, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 31 {4}
1 75 {4}
2 94 {2, 3}
3 20 {2}
4 87 {1}
5 26 {1}
6 29 {3}
7 70 {5}
8 2 {4}
9 50 {8}
Round  36, Devices participated 10, Average loss 0.169, Central accuracy on global test data 37.461, Ensemble accuracy on global test data 38.330, Local accuracy on global train data 21.045, Local accuracy on local train data 98.333


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {8, 7}
1 9 {3}
2 46 {3}
3 37 {9}
4 72 {5}
5 83 {7}
6 73 {1, 2}
7 16 {7}
8 55 {3}
9 27 {9}
Round  37, Devices participated 10, Average loss 0.092, Central accuracy on global test data 43.760, Ensemble accuracy on global test data 44.355, Local accuracy on global train data 20.669, Local accuracy on local train data 97.883


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.5555555555555556, 'layer_hidden1.bias': 0.5555555555555556, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {1}
1 29 {3}
2 9 {3}
3 50 {8}
4 26 {1}
5 81 {0}
6 48 {4}
7 82 {6}
8 37 {9}
9 15 {2}
Round  38, Devices participated 10, Average loss 0.114, Central accuracy on global test data 62.070, Ensemble accuracy on global test data 62.041, Local accuracy on global train data 23.877, Local accuracy on local train data 98.850


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.6666666666666666, 'layer_hidden1.bias': 0.6666666666666666, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 82 {6}
1 6 {2}
2 87 {1}
3 74 {4}
4 29 {3}
5 50 {8}
6 62 {0}
7 67 {1}
8 12 {1}
9 83 {7}
Round  39, Devices participated 10, Average loss 0.066, Central accuracy on global test data 58.633, Ensemble accuracy on global test data 62.197, Local accuracy on global train data 30.479, Local accuracy on local train data 99.183


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 0.2222222222222222, 'layer_hidden2.bias': 0.2222222222222222}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {5}
1 75 {4}
2 89 {9}
3 43 {0}
4 12 {1}
5 21 {2}
6 95 {3}
7 72 {5}
8 24 {2}
9 63 {5}
Round  40, Devices participated 10, Average loss 0.112, Central accuracy on global test data 62.100, Ensemble accuracy on global test data 64.150, Local accuracy on global train data 23.953, Local accuracy on local train data 98.767


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.2222222222222222, 'layer_hidden1.bias': 0.2222222222222222, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 34 {3}
1 15 {2}
2 31 {4}
3 94 {2, 3}
4 46 {3}
5 70 {5}
6 19 {9}
7 80 {1}
8 6 {2}
9 88 {6, 7}
Round  41, Devices participated 10, Average loss 0.097, Central accuracy on global test data 52.939, Ensemble accuracy on global test data 52.627, Local accuracy on global train data 25.178, Local accuracy on local train data 98.533


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 0.1111111111111111, 'layer_hidden2.bias': 0.1111111111111111}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 70 {5}
1 92 {8}
2 72 {5}
3 77 {6}
4 23 {7}
5 53 {1}
6 1 {4}
7 12 {1}
8 97 {3}
9 93 {9}
Round  42, Devices participated 10, Average loss 0.086, Central accuracy on global test data 65.459, Ensemble accuracy on global test data 71.045, Local accuracy on global train data 29.309, Local accuracy on local train data 98.767


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 38 {4, 5}
2 36 {5}
3 14 {6}
4 57 {9}
5 77 {6}
6 66 {6}
7 73 {1, 2}
8 3 {9}
9 0 {8, 7}
Round  43, Devices participated 10, Average loss 0.105, Central accuracy on global test data 68.125, Ensemble accuracy on global test data 67.783, Local accuracy on global train data 36.990, Local accuracy on local train data 98.233


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 19 {9}
2 10 {7}
3 24 {2}
4 76 {0, 1}
5 57 {9}
6 88 {6, 7}
7 23 {7}
8 61 {1}
9 31 {4}
Round  44, Devices participated 10, Average loss 0.057, Central accuracy on global test data 69.375, Ensemble accuracy on global test data 68.955, Local accuracy on global train data 41.030, Local accuracy on local train data 98.917


{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 18 {0}
1 45 {6}
2 94 {2, 3}
3 24 {2}
4 56 {0}
5 67 {1}
6 10 {7}
7 14 {6}
8 13 {4}
9 9 {3}
Round  45, Devices participated 10, Average loss 0.064, Central accuracy on global test data 68.887, Ensemble accuracy on global test data 70.400, Local accuracy on global train data 39.167, Local accuracy on local train data 98.883


{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 48 {4}
2 73 {1, 2}
3 93 {9}
4 70 {5}
5 1 {4}
6 43 {0}
7 77 {6}
8 76 {0, 1}
9 88 {6, 7}
Round  46, Devices participated 10, Average loss 0.062, Central accuracy on global test data 67.051, Ensemble accuracy on global test data 68.105, Local accuracy on global train data 42.446, Local accuracy on local train data 98.683


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2}
1 45 {6}
2 83 {7}
3 68 {7}
4 75 {4}
5 65 {9}
6 96 {0}
7 1 {4}
8 77 {6}
9 73 {1, 2}
Round  47, Devices participated 10, Average loss 0.047, Central accuracy on global test data 67.939, Ensemble accuracy on global test data 67.627, Local accuracy on global train data 37.947, Local accuracy on local train data 98.817


{'layer_input.weight': 0.7777777777777778, 'layer_input.bias': 0.7777777777777778, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 81 {0}
1 84 {1}
2 60 {8}
3 18 {0}
4 45 {6}
5 47 {5}
6 94 {2, 3}
7 83 {7}
8 76 {0, 1}
9 22 {3}
Round  48, Devices participated 10, Average loss 0.053, Central accuracy on global test data 68.340, Ensemble accuracy on global test data 70.127, Local accuracy on global train data 38.149, Local accuracy on local train data 98.767


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 21 {2}
1 69 {5}
2 47 {5}
3 34 {3}
4 65 {9}
5 81 {0}
6 93 {9}
7 44 {2}
8 30 {8}
9 84 {1}
Round  49, Devices participated 10, Average loss 0.056, Central accuracy on global test data 71.504, Ensemble accuracy on global test data 69.795, Local accuracy on global train data 30.984, Local accuracy on local train data 98.750


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.7455, device='cuda:0', grad_fn=<MaxBackward1>)
0 22 {3}
1 38 {4, 5}
2 77 {6}
3 15 {2}
4 11 {4}
5 61 {1}
6 37 {9}
7 34 {3}
8 87 {1}
9 3 {9}
Round  50, Devices participated 10, Average loss 0.032, Central accuracy on global test data 68.633, Ensemble accuracy on global test data 68.760, Local accuracy on global train data 38.857, Local accuracy on local train data 99.267


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {0}
1 9 {3}
2 26 {1}
3 58 {8}
4 69 {5}
5 44 {2}
6 13 {4}
7 93 {9}
8 61 {1}
9 43 {0}
Round  51, Devices participated 10, Average loss 0.031, Central accuracy on global test data 69.932, Ensemble accuracy on global test data 70.430, Local accuracy on global train data 38.545, Local accuracy on local train data 99.083


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 34 {3}
2 27 {9}
3 16 {7}
4 72 {5}
5 98 {5}
6 14 {6}
7 19 {9}
8 9 {3}
9 46 {3}
Round  52, Devices participated 10, Average loss 0.028, Central accuracy on global test data 66.016, Ensemble accuracy on global test data 61.895, Local accuracy on global train data 29.436, Local accuracy on local train data 99.083


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 29 {3}
1 72 {5}
2 81 {0}
3 80 {1}
4 68 {7}
5 85 {2}
6 14 {6}
7 0 {8, 7}
8 93 {9}
9 52 {1}
Round  53, Devices participated 10, Average loss 0.044, Central accuracy on global test data 74.307, Ensemble accuracy on global test data 74.678, Local accuracy on global train data 35.137, Local accuracy on local train data 99.017


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 17 {8}
1 69 {5}
2 35 {8}
3 86 {0}
4 83 {7}
5 84 {1}
6 23 {7}
7 56 {0}
8 89 {9}
9 60 {8}
Round  54, Devices participated 10, Average loss 0.024, Central accuracy on global test data 69.639, Ensemble accuracy on global test data 68.594, Local accuracy on global train data 38.833, Local accuracy on local train data 99.300


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 94 {2, 3}
1 79 {6}
2 36 {5}
3 8 {0}
4 19 {9}
5 16 {7}
6 13 {4}
7 30 {8}
8 25 {6}
9 3 {9}
Round  55, Devices participated 10, Average loss 0.033, Central accuracy on global test data 71.992, Ensemble accuracy on global test data 70.566, Local accuracy on global train data 34.153, Local accuracy on local train data 99.033


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {6}
1 82 {6}
2 32 {2}
3 52 {1}
4 17 {8}
5 96 {0}
6 6 {2}
7 57 {9}
8 67 {1}
9 74 {4}
Round  56, Devices participated 10, Average loss 0.024, Central accuracy on global test data 73.252, Ensemble accuracy on global test data 73.818, Local accuracy on global train data 36.589, Local accuracy on local train data 99.300


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 20 {2}
2 59 {3}
3 99 {5}
4 39 {1}
5 42 {7}
6 73 {1, 2}
7 53 {1}
8 45 {6}
9 8 {0}
Round  57, Devices participated 10, Average loss 0.033, Central accuracy on global test data 70.811, Ensemble accuracy on global test data 70.107, Local accuracy on global train data 36.924, Local accuracy on local train data 99.117


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {8}
1 38 {4, 5}
2 5 {7}
3 54 {7}
4 66 {6}
5 33 {0}
6 80 {1}
7 83 {7}
8 9 {3}
9 4 {8}
Round  58, Devices participated 10, Average loss 0.026, Central accuracy on global test data 69.980, Ensemble accuracy on global test data 70.127, Local accuracy on global train data 38.345, Local accuracy on local train data 99.267


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 39 {1}
1 44 {2}
2 62 {0}
3 4 {8}
4 42 {7}
5 25 {6}
6 0 {8, 7}
7 45 {6}
8 29 {3}
9 93 {9}
Round  59, Devices participated 10, Average loss 0.028, Central accuracy on global test data 70.303, Ensemble accuracy on global test data 74.717, Local accuracy on global train data 34.741, Local accuracy on local train data 99.217


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {0}
1 54 {7}
2 19 {9}
3 97 {3}
4 92 {8}
5 31 {4}
6 81 {0}
7 2 {4}
8 24 {2}
9 9 {3}
Round  60, Devices participated 10, Average loss 0.028, Central accuracy on global test data 69.551, Ensemble accuracy on global test data 68.984, Local accuracy on global train data 33.596, Local accuracy on local train data 99.150


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 9 {3}
1 19 {9}
2 7 {8}
3 49 {7}
4 45 {6}
5 35 {8}
6 80 {1}
7 85 {2}
8 40 {2}
9 89 {9}
Round  61, Devices participated 10, Average loss 0.027, Central accuracy on global test data 63.545, Ensemble accuracy on global test data 69.951, Local accuracy on global train data 25.898, Local accuracy on local train data 99.050


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.7777777777777778, 'layer_hidden2.bias': 0.7777777777777778}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {1, 2}
1 66 {6}
2 18 {0}
3 74 {4}
4 33 {0}
5 2 {4}
6 87 {1}
7 29 {3}
8 98 {5}
9 91 {1}
Round  62, Devices participated 10, Average loss 0.042, Central accuracy on global test data 68.916, Ensemble accuracy on global test data 69.893, Local accuracy on global train data 35.613, Local accuracy on local train data 98.850


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {2}
1 31 {4}
2 27 {9}
3 50 {8}
4 71 {3, 4}
5 81 {0}
6 99 {5}
7 97 {3}
8 73 {1, 2}
9 49 {7}
Round  63, Devices participated 10, Average loss 0.038, Central accuracy on global test data 72.979, Ensemble accuracy on global test data 70.762, Local accuracy on global train data 31.516, Local accuracy on local train data 98.900


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 64 {7}
2 42 {7}
3 88 {6, 7}
4 27 {9}
5 95 {3}
6 92 {8}
7 50 {8}
8 45 {6}
9 85 {2}
Round  64, Devices participated 10, Average loss 0.022, Central accuracy on global test data 73.389, Ensemble accuracy on global test data 73.232, Local accuracy on global train data 34.551, Local accuracy on local train data 99.300


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {9}
1 8 {0}
2 49 {7}
3 32 {2}
4 15 {2}
5 35 {8}
6 70 {5}
7 59 {3}
8 31 {4}
9 30 {8}
Round  65, Devices participated 10, Average loss 0.022, Central accuracy on global test data 72.002, Ensemble accuracy on global test data 77.207, Local accuracy on global train data 29.312, Local accuracy on local train data 99.417


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 69 {5}
1 35 {8}
2 52 {1}
3 51 {5, 6}
4 26 {1}
5 15 {2}
6 76 {0, 1}
7 48 {4}
8 98 {5}
9 12 {1}
Round  66, Devices participated 10, Average loss 0.028, Central accuracy on global test data 69.697, Ensemble accuracy on global test data 64.844, Local accuracy on global train data 30.518, Local accuracy on local train data 99.283


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {3}
1 57 {9}
2 0 {8, 7}
3 86 {0}
4 26 {1}
5 80 {1}
6 73 {1, 2}
7 20 {2}
8 63 {5}
9 98 {5}
Round  67, Devices participated 10, Average loss 0.035, Central accuracy on global test data 72.920, Ensemble accuracy on global test data 71.143, Local accuracy on global train data 29.890, Local accuracy on local train data 98.917


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 81 {0}
1 6 {2}
2 64 {7}
3 84 {1}
4 20 {2}
5 2 {4}
6 8 {0}
7 4 {8}
8 66 {6}
9 92 {8}
Round  68, Devices participated 10, Average loss 0.018, Central accuracy on global test data 72.275, Ensemble accuracy on global test data 71.738, Local accuracy on global train data 33.489, Local accuracy on local train data 99.467


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 61 {1}
1 34 {3}
2 64 {7}
3 11 {4}
4 96 {0}
5 18 {0}
6 82 {6}
7 52 {1}
8 72 {5}
9 92 {8}
Round  69, Devices participated 10, Average loss 0.015, Central accuracy on global test data 75.010, Ensemble accuracy on global test data 75.078, Local accuracy on global train data 39.722, Local accuracy on local train data 99.550


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 0.0, 'layer_hidden2.bias': 0.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.1805, device='cuda:0', grad_fn=<MaxBackward1>)
0 93 {9}
1 68 {7}
2 94 {2, 3}
3 13 {4}
4 36 {5}
5 11 {4}
6 5 {7}
7 1 {4}
8 40 {2}
9 7 {8}
Round  70, Devices participated 10, Average loss 0.028, Central accuracy on global test data 17.051, Ensemble accuracy on global test data 73.779, Local accuracy on global train data 33.015, Local accuracy on local train data 99.150


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.5555555555555556, 'layer_hidden2.bias': 0.5555555555555556}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 73 {1, 2}
1 64 {7}
2 37 {9}
3 69 {5}
4 31 {4}
5 48 {4}
6 77 {6}
7 41 {8, 9}
8 53 {1}
9 75 {4}
Round  71, Devices participated 10, Average loss 0.371, Central accuracy on global test data 18.350, Ensemble accuracy on global test data 37.236, Local accuracy on global train data 19.541, Local accuracy on local train data 97.167


{'layer_input.weight': 0.0, 'layer_input.bias': 0.0, 'layer_hidden1.weight': 0.0, 'layer_hidden1.bias': 0.0, 'layer_hidden2.weight': 0.8888888888888888, 'layer_hidden2.bias': 0.8888888888888888}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {3}
1 43 {0}
2 84 {1}
3 11 {4}
4 66 {6}
5 92 {8}
6 16 {7}
7 19 {9}
8 96 {0}
9 37 {9}
Round  72, Devices participated 10, Average loss 0.147, Central accuracy on global test data 15.781, Ensemble accuracy on global test data 49.668, Local accuracy on global train data 15.771, Local accuracy on local train data 97.800


{'layer_input.weight': 0.1111111111111111, 'layer_input.bias': 0.1111111111111111, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 43 {0}
1 96 {0}
2 9 {3}
3 0 {8, 7}
4 74 {4}
5 19 {9}
6 1 {4}
7 23 {7}
8 38 {4, 5}
9 66 {6}
Round  73, Devices participated 10, Average loss 0.180, Central accuracy on global test data 11.377, Ensemble accuracy on global test data 54.453, Local accuracy on global train data 11.777, Local accuracy on local train data 95.867


{'layer_input.weight': 0.8888888888888888, 'layer_input.bias': 0.8888888888888888, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {9}
1 62 {0}
2 32 {2}
3 10 {7}
4 22 {3}
5 72 {5}
6 64 {7}
7 25 {6}
8 86 {0}
9 60 {8}
Round  74, Devices participated 10, Average loss 0.074, Central accuracy on global test data 42.783, Ensemble accuracy on global test data 42.500, Local accuracy on global train data 12.114, Local accuracy on local train data 98.200


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.7777777777777778, 'layer_hidden1.bias': 0.7777777777777778, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 84 {1}
1 7 {8}
2 64 {7}
3 76 {0, 1}
4 15 {2}
5 46 {3}
6 27 {9}
7 47 {5}
8 40 {2}
9 69 {5}
Round  75, Devices participated 10, Average loss 0.056, Central accuracy on global test data 49.238, Ensemble accuracy on global test data 48.252, Local accuracy on global train data 14.824, Local accuracy on local train data 98.767


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.1111111111111111, 'layer_hidden1.bias': 0.1111111111111111, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 75 {4}
1 97 {3}
2 79 {6}
3 87 {1}
4 11 {4}
5 7 {8}
6 37 {9}
7 92 {8}
8 41 {8, 9}
9 12 {1}
Round  76, Devices participated 10, Average loss 0.081, Central accuracy on global test data 28.555, Ensemble accuracy on global test data 53.506, Local accuracy on global train data 17.043, Local accuracy on local train data 98.033


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 4 {8}
2 17 {8}
3 21 {2}
4 76 {0, 1}
5 74 {4}
6 19 {9}
7 61 {1}
8 45 {6}
9 86 {0}
Round  77, Devices participated 10, Average loss 0.047, Central accuracy on global test data 58.154, Ensemble accuracy on global test data 55.820, Local accuracy on global train data 16.921, Local accuracy on local train data 98.633


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 23 {7}
1 41 {8, 9}
2 75 {4}
3 36 {5}
4 26 {1}
5 49 {7}
6 95 {3}
7 16 {7}
8 93 {9}
9 72 {5}
Round  78, Devices participated 10, Average loss 0.052, Central accuracy on global test data 64.062, Ensemble accuracy on global test data 59.326, Local accuracy on global train data 19.092, Local accuracy on local train data 98.633


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 59 {3}
1 53 {1}
2 65 {9}
3 8 {0}
4 68 {7}
5 35 {8}
6 13 {4}
7 55 {3}
8 23 {7}
9 97 {3}
Round  79, Devices participated 10, Average loss 0.034, Central accuracy on global test data 62.236, Ensemble accuracy on global test data 58.486, Local accuracy on global train data 19.858, Local accuracy on local train data 99.117


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {4}
1 98 {5}
2 22 {3}
3 45 {6}
4 97 {3}
5 87 {1}
6 92 {8}
7 71 {3, 4}
8 47 {5}
9 79 {6}
Round  80, Devices participated 10, Average loss 0.038, Central accuracy on global test data 72.178, Ensemble accuracy on global test data 67.842, Local accuracy on global train data 15.906, Local accuracy on local train data 99.150


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 35 {8}
1 70 {5}
2 59 {3}
3 49 {7}
4 36 {5}
5 91 {1}
6 82 {6}
7 50 {8}
8 26 {1}
9 72 {5}
Round  81, Devices participated 10, Average loss 0.022, Central accuracy on global test data 62.793, Ensemble accuracy on global test data 55.947, Local accuracy on global train data 20.259, Local accuracy on local train data 99.583


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0213, device='cuda:0', grad_fn=<MaxBackward1>)
0 22 {3}
1 83 {7}
2 75 {4}
3 29 {3}
4 66 {6}
5 48 {4}
6 12 {1}
7 10 {7}
8 86 {0}
9 1 {4}
Round  82, Devices participated 10, Average loss 0.024, Central accuracy on global test data 67.520, Ensemble accuracy on global test data 65.693, Local accuracy on global train data 23.933, Local accuracy on local train data 99.483


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 76 {0, 1}
1 33 {0}
2 40 {2}
3 59 {3}
4 65 {9}
5 63 {5}
6 73 {1, 2}
7 36 {5}
8 99 {5}
9 82 {6}
Round  83, Devices participated 10, Average loss 0.036, Central accuracy on global test data 65.908, Ensemble accuracy on global test data 61.650, Local accuracy on global train data 21.074, Local accuracy on local train data 99.083


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 85 {2}
1 88 {6, 7}
2 54 {7}
3 40 {2}
4 70 {5}
5 67 {1}
6 86 {0}
7 38 {4, 5}
8 58 {8}
9 42 {7}
Round  84, Devices participated 10, Average loss 0.025, Central accuracy on global test data 72.266, Ensemble accuracy on global test data 70.908, Local accuracy on global train data 26.172, Local accuracy on local train data 99.500


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 56 {0}
2 12 {1}
3 90 {5}
4 94 {2, 3}
5 86 {0}
6 19 {9}
7 78 {9}
8 92 {8}
9 59 {3}
Round  85, Devices participated 10, Average loss 0.035, Central accuracy on global test data 68.994, Ensemble accuracy on global test data 67.734, Local accuracy on global train data 24.641, Local accuracy on local train data 98.933


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 7 {8}
1 94 {2, 3}
2 69 {5}
3 44 {2}
4 50 {8}
5 26 {1}
6 75 {4}
7 24 {2}
8 33 {0}
9 18 {0}
Round  86, Devices participated 10, Average loss 0.025, Central accuracy on global test data 72.920, Ensemble accuracy on global test data 73.047, Local accuracy on global train data 24.636, Local accuracy on local train data 99.450


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 89 {9}
1 75 {4}
2 84 {1}
3 93 {9}
4 35 {8}
5 71 {3, 4}
6 19 {9}
7 63 {5}
8 59 {3}
9 94 {2, 3}
Round  87, Devices participated 10, Average loss 0.032, Central accuracy on global test data 59.375, Ensemble accuracy on global test data 63.730, Local accuracy on global train data 21.321, Local accuracy on local train data 99.050


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {7}
1 66 {6}
2 17 {8}
3 43 {0}
4 53 {1}
5 40 {2}
6 76 {0, 1}
7 1 {4}
8 4 {8}
9 81 {0}
Round  88, Devices participated 10, Average loss 0.025, Central accuracy on global test data 72.979, Ensemble accuracy on global test data 75.566, Local accuracy on global train data 26.387, Local accuracy on local train data 99.233


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 52 {1}
1 51 {5, 6}
2 97 {3}
3 7 {8}
4 41 {8, 9}
5 61 {1}
6 29 {3}
7 88 {6, 7}
8 49 {7}
9 26 {1}
Round  89, Devices participated 10, Average loss 0.033, Central accuracy on global test data 70.176, Ensemble accuracy on global test data 70.176, Local accuracy on global train data 31.938, Local accuracy on local train data 99.100


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0184, device='cuda:0', grad_fn=<MaxBackward1>)
0 21 {2}
1 47 {5}
2 20 {2}
3 59 {3}
4 85 {2}
5 2 {4}
6 99 {5}
7 70 {5}
8 54 {7}
9 50 {8}
Round  90, Devices participated 10, Average loss 0.028, Central accuracy on global test data 67.686, Ensemble accuracy on global test data 63.398, Local accuracy on global train data 17.285, Local accuracy on local train data 99.200


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 65 {9}
1 98 {5}
2 79 {6}
3 6 {2}
4 58 {8}
5 92 {8}
6 19 {9}
7 37 {9}
8 33 {0}
9 75 {4}
Round  91, Devices participated 10, Average loss 0.029, Central accuracy on global test data 70.820, Ensemble accuracy on global test data 68.770, Local accuracy on global train data 21.516, Local accuracy on local train data 99.100


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 44 {2}
1 84 {1}
2 56 {0}
3 0 {8, 7}
4 69 {5}
5 15 {2}
6 66 {6}
7 78 {9}
8 42 {7}
9 95 {3}
Round  92, Devices participated 10, Average loss 0.021, Central accuracy on global test data 79.053, Ensemble accuracy on global test data 78.037, Local accuracy on global train data 27.000, Local accuracy on local train data 99.567


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 36 {5}
1 20 {2}
2 53 {1}
3 26 {1}
4 80 {1}
5 67 {1}
6 25 {6}
7 37 {9}
8 39 {1}
9 10 {7}
Round  93, Devices participated 10, Average loss 0.014, Central accuracy on global test data 69.277, Ensemble accuracy on global test data 69.404, Local accuracy on global train data 37.410, Local accuracy on local train data 99.717


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 54 {7}
1 62 {0}
2 65 {9}
3 69 {5}
4 82 {6}
5 46 {3}
6 64 {7}
7 70 {5}
8 59 {3}
9 11 {4}
Round  94, Devices participated 10, Average loss 0.019, Central accuracy on global test data 71.582, Ensemble accuracy on global test data 69.590, Local accuracy on global train data 28.345, Local accuracy on local train data 99.500


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 89 {9}
2 64 {7}
3 25 {6}
4 85 {2}
5 9 {3}
6 57 {9}
7 41 {8, 9}
8 97 {3}
9 45 {6}
Round  95, Devices participated 10, Average loss 0.026, Central accuracy on global test data 61.182, Ensemble accuracy on global test data 64.502, Local accuracy on global train data 26.162, Local accuracy on local train data 99.217


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 32 {2}
1 8 {0}
2 70 {5}
3 41 {8, 9}
4 12 {1}
5 19 {9}
6 88 {6, 7}
7 20 {2}
8 98 {5}
9 33 {0}
Round  96, Devices participated 10, Average loss 0.027, Central accuracy on global test data 63.877, Ensemble accuracy on global test data 65.381, Local accuracy on global train data 30.081, Local accuracy on local train data 99.200


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 50 {8}
1 29 {3}
2 81 {0}
3 33 {0}
4 44 {2}
5 69 {5}
6 2 {4}
7 68 {7}
8 93 {9}
9 85 {2}
Round  97, Devices participated 10, Average loss 0.028, Central accuracy on global test data 74.150, Ensemble accuracy on global test data 73.350, Local accuracy on global train data 25.962, Local accuracy on local train data 99.217


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 5 {7}
1 84 {1}
2 70 {5}
3 73 {1, 2}
4 31 {4}
5 62 {0}
6 69 {5}
7 13 {4}
8 86 {0}
9 74 {4}
Round  98, Devices participated 10, Average loss 0.027, Central accuracy on global test data 67.969, Ensemble accuracy on global test data 66.494, Local accuracy on global train data 33.062, Local accuracy on local train data 99.150


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 69 {5}
1 9 {3}
2 41 {8, 9}
3 83 {7}
4 76 {0, 1}
5 63 {5}
6 34 {3}
7 67 {1}
8 36 {5}
9 87 {1}
Round  99, Devices participated 10, Average loss 0.026, Central accuracy on global test data 73.535, Ensemble accuracy on global test data 72.656, Local accuracy on global train data 27.988, Local accuracy on local train data 99.167


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {2}
1 77 {6}
2 95 {3}
3 52 {1}
4 25 {6}
5 38 {4, 5}
6 46 {3}
7 47 {5}
8 93 {9}
9 88 {6, 7}
Round 100, Devices participated 10, Average loss 0.021, Central accuracy on global test data 81.602, Ensemble accuracy on global test data 80.625, Local accuracy on global train data 25.742, Local accuracy on local train data 99.367


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {6}
1 22 {3}
2 87 {1}
3 6 {2}
4 15 {2}
5 36 {5}
6 37 {9}
7 44 {2}
8 48 {4}
9 38 {4, 5}
Round 101, Devices participated 10, Average loss 0.018, Central accuracy on global test data 75.010, Ensemble accuracy on global test data 74.473, Local accuracy on global train data 26.494, Local accuracy on local train data 99.467


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 14 {6}
1 66 {6}
2 74 {4}
3 62 {0}
4 49 {7}
5 45 {6}
6 18 {0}
7 79 {6}
8 15 {2}
9 68 {7}
Round 102, Devices participated 10, Average loss 0.014, Central accuracy on global test data 71.455, Ensemble accuracy on global test data 70.322, Local accuracy on global train data 35.784, Local accuracy on local train data 99.733


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 92 {8}
1 32 {2}
2 77 {6}
3 87 {1}
4 27 {9}
5 13 {4}
6 14 {6}
7 47 {5}
8 22 {3}
9 73 {1, 2}
Round 103, Devices participated 10, Average loss 0.024, Central accuracy on global test data 81.270, Ensemble accuracy on global test data 82.676, Local accuracy on global train data 27.417, Local accuracy on local train data 99.267


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 60 {8}
1 31 {4}
2 44 {2}
3 3 {9}
4 84 {1}
5 25 {6}
6 98 {5}
7 73 {1, 2}
8 52 {1}
9 82 {6}
Round 104, Devices participated 10, Average loss 0.019, Central accuracy on global test data 79.658, Ensemble accuracy on global test data 79.678, Local accuracy on global train data 29.888, Local accuracy on local train data 99.467


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0741, device='cuda:0', grad_fn=<MaxBackward1>)
0 66 {6}
1 38 {4, 5}
2 74 {4}
3 49 {7}
4 46 {3}
5 45 {6}
6 12 {1}
7 0 {8, 7}
8 81 {0}
9 23 {7}
Round 105, Devices participated 10, Average loss 0.023, Central accuracy on global test data 76.543, Ensemble accuracy on global test data 76.855, Local accuracy on global train data 36.060, Local accuracy on local train data 99.233


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 6 {2}
1 14 {6}
2 56 {0}
3 8 {0}
4 37 {9}
5 71 {3, 4}
6 68 {7}
7 53 {1}
8 88 {6, 7}
9 65 {9}
Round 106, Devices participated 10, Average loss 0.021, Central accuracy on global test data 81.006, Ensemble accuracy on global test data 79.160, Local accuracy on global train data 34.910, Local accuracy on local train data 99.333


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 76 {0, 1}
1 40 {2}
2 68 {7}
3 38 {4, 5}
4 91 {1}
5 92 {8}
6 18 {0}
7 11 {4}
8 69 {5}
9 94 {2, 3}
Round 107, Devices participated 10, Average loss 0.024, Central accuracy on global test data 77.246, Ensemble accuracy on global test data 77.139, Local accuracy on global train data 36.074, Local accuracy on local train data 99.350


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {8, 7}
1 23 {7}
2 91 {1}
3 83 {7}
4 98 {5}
5 64 {7}
6 90 {5}
7 32 {2}
8 84 {1}
9 36 {5}
Round 108, Devices participated 10, Average loss 0.020, Central accuracy on global test data 64.189, Ensemble accuracy on global test data 62.627, Local accuracy on global train data 31.953, Local accuracy on local train data 99.500


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 15 {2}
1 9 {3}
2 95 {3}
3 71 {3, 4}
4 80 {1}
5 49 {7}
6 39 {1}
7 81 {0}
8 30 {8}
9 8 {0}
Round 109, Devices participated 10, Average loss 0.022, Central accuracy on global test data 68.223, Ensemble accuracy on global test data 67.568, Local accuracy on global train data 30.620, Local accuracy on local train data 99.300


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 0.8888888888888888, 'layer_hidden1.bias': 0.8888888888888888, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 29 {3}
1 87 {1}
2 67 {1}
3 36 {5}
4 35 {8}
5 96 {0}
6 15 {2}
7 61 {1}
8 24 {2}
9 62 {0}
Round 110, Devices participated 10, Average loss 0.017, Central accuracy on global test data 70.752, Ensemble accuracy on global test data 72.773, Local accuracy on global train data 33.210, Local accuracy on local train data 99.583


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 13 {4}
1 58 {8}
2 87 {1}
3 52 {1}
4 50 {8}
5 60 {8}
6 3 {9}
7 19 {9}
8 35 {8}
9 0 {8, 7}
Round 111, Devices participated 10, Average loss 0.024, Central accuracy on global test data 52.910, Ensemble accuracy on global test data 50.527, Local accuracy on global train data 26.453, Local accuracy on local train data 99.317


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 55 {3}
1 46 {3}
2 66 {6}
3 67 {1}
4 73 {1, 2}
5 12 {1}
6 51 {5, 6}
7 1 {4}
8 57 {9}
9 90 {5}
Round 112, Devices participated 10, Average loss 0.040, Central accuracy on global test data 73.135, Ensemble accuracy on global test data 73.770, Local accuracy on global train data 24.634, Local accuracy on local train data 98.800


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 40 {2}
1 72 {5}
2 92 {8}
3 86 {0}
4 89 {9}
5 90 {5}
6 16 {7}
7 98 {5}
8 73 {1, 2}
9 58 {8}
Round 113, Devices participated 10, Average loss 0.044, Central accuracy on global test data 76.240, Ensemble accuracy on global test data 67.676, Local accuracy on global train data 22.175, Local accuracy on local train data 98.800


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 0 {8, 7}
1 58 {8}
2 47 {5}
3 44 {2}
4 94 {2, 3}
5 66 {6}
6 4 {8}
7 7 {8}
8 27 {9}
9 2 {4}
Round 114, Devices participated 10, Average loss 0.027, Central accuracy on global test data 68.350, Ensemble accuracy on global test data 66.104, Local accuracy on global train data 26.240, Local accuracy on local train data 99.117


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 58 {8}
1 87 {1}
2 14 {6}
3 78 {9}
4 40 {2}
5 83 {7}
6 73 {1, 2}
7 18 {0}
8 52 {1}
9 31 {4}
Round 115, Devices participated 10, Average loss 0.018, Central accuracy on global test data 79.170, Ensemble accuracy on global test data 78.975, Local accuracy on global train data 37.405, Local accuracy on local train data 99.433


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 99 {5}
1 22 {3}
2 62 {0}
3 86 {0}
4 5 {7}
5 20 {2}
6 9 {3}
7 51 {5, 6}
8 7 {8}
9 54 {7}
Round 116, Devices participated 10, Average loss 0.024, Central accuracy on global test data 79.697, Ensemble accuracy on global test data 80.693, Local accuracy on global train data 30.898, Local accuracy on local train data 99.350


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0429, device='cuda:0', grad_fn=<MaxBackward1>)
0 97 {3}
1 37 {9}
2 42 {7}
3 7 {8}
4 22 {3}
5 5 {7}
6 24 {2}
7 56 {0}
8 69 {5}
9 12 {1}
Round 117, Devices participated 10, Average loss 0.016, Central accuracy on global test data 80.654, Ensemble accuracy on global test data 78.721, Local accuracy on global train data 32.429, Local accuracy on local train data 99.583


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {0}
1 22 {3}
2 76 {0, 1}
3 44 {2}
4 59 {3}
5 78 {9}
6 12 {1}
7 68 {7}
8 94 {2, 3}
9 87 {1}
Round 118, Devices participated 10, Average loss 0.016, Central accuracy on global test data 73.516, Ensemble accuracy on global test data 70.850, Local accuracy on global train data 38.586, Local accuracy on local train data 99.500


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 45 {6}
1 7 {8}
2 65 {9}
3 19 {9}
4 95 {3}
5 16 {7}
6 35 {8}
7 23 {7}
8 83 {7}
9 88 {6, 7}
Round 119, Devices participated 10, Average loss 0.020, Central accuracy on global test data 72.354, Ensemble accuracy on global test data 72.168, Local accuracy on global train data 30.461, Local accuracy on local train data 99.433


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 12 {1}
1 91 {1}
2 4 {8}
3 90 {5}
4 47 {5}
5 66 {6}
6 33 {0}
7 9 {3}
8 61 {1}
9 84 {1}
Round 120, Devices participated 10, Average loss 0.013, Central accuracy on global test data 75.859, Ensemble accuracy on global test data 75.068, Local accuracy on global train data 39.480, Local accuracy on local train data 99.600


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 46 {3}
1 60 {8}
2 83 {7}
3 2 {4}
4 37 {9}
5 70 {5}
6 93 {9}
7 29 {3}
8 3 {9}
9 43 {0}
Round 121, Devices participated 10, Average loss 0.018, Central accuracy on global test data 71.924, Ensemble accuracy on global test data 71.465, Local accuracy on global train data 30.889, Local accuracy on local train data 99.483


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {9}
1 45 {6}
2 49 {7}
3 4 {8}
4 31 {4}
5 62 {0}
6 9 {3}
7 95 {3}
8 69 {5}
9 72 {5}
Round 122, Devices participated 10, Average loss 0.017, Central accuracy on global test data 78.955, Ensemble accuracy on global test data 79.297, Local accuracy on global train data 27.444, Local accuracy on local train data 99.383


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 27 {9}
1 28 {6}
2 1 {4}
3 92 {8}
4 55 {3}
5 48 {4}
6 74 {4}
7 7 {8}
8 99 {5}
9 62 {0}
Round 123, Devices participated 10, Average loss 0.014, Central accuracy on global test data 78.223, Ensemble accuracy on global test data 78.594, Local accuracy on global train data 29.966, Local accuracy on local train data 99.617


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 24 {2}
1 0 {8, 7}
2 9 {3}
3 11 {4}
4 31 {4}
5 45 {6}
6 58 {8}
7 35 {8}
8 69 {5}
9 21 {2}
Round 124, Devices participated 10, Average loss 0.019, Central accuracy on global test data 74.824, Ensemble accuracy on global test data 74.619, Local accuracy on global train data 26.362, Local accuracy on local train data 99.467


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 42 {7}
1 98 {5}
2 67 {1}
3 21 {2}
4 24 {2}
5 81 {0}
6 20 {2}
7 75 {4}
8 29 {3}
9 36 {5}
Round 125, Devices participated 10, Average loss 0.014, Central accuracy on global test data 73.428, Ensemble accuracy on global test data 71.152, Local accuracy on global train data 27.449, Local accuracy on local train data 99.667


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 49 {7}
1 75 {4}
2 33 {0}
3 60 {8}
4 42 {7}
5 64 {7}
6 69 {5}
7 11 {4}
8 47 {5}
9 70 {5}
Round 126, Devices participated 10, Average loss 0.014, Central accuracy on global test data 67.734, Ensemble accuracy on global test data 66.025, Local accuracy on global train data 34.216, Local accuracy on local train data 99.633


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
tensor(8.0604, device='cuda:0', grad_fn=<MaxBackward1>)
0 96 {0}
1 60 {8}
2 16 {7}
3 70 {5}
4 73 {1, 2}
5 40 {2}
6 11 {4}
7 24 {2}
8 87 {1}
9 93 {9}
Round 127, Devices participated 10, Average loss 0.024, Central accuracy on global test data 78.564, Ensemble accuracy on global test data 79.688, Local accuracy on global train data 31.860, Local accuracy on local train data 99.267


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 37 {9}
1 16 {7}
2 85 {2}
3 31 {4}
4 69 {5}
5 36 {5}
6 54 {7}
7 25 {6}
8 32 {2}
9 38 {4, 5}
Round 128, Devices participated 10, Average loss 0.016, Central accuracy on global test data 78.525, Ensemble accuracy on global test data 76.846, Local accuracy on global train data 28.901, Local accuracy on local train data 99.483


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 80 {1}
1 92 {8}
2 61 {1}
3 76 {0, 1}
4 19 {9}
5 28 {6}
6 7 {8}
7 73 {1, 2}
8 67 {1}
9 38 {4, 5}
Round 129, Devices participated 10, Average loss 0.021, Central accuracy on global test data 78.438, Ensemble accuracy on global test data 79.541, Local accuracy on global train data 40.969, Local accuracy on local train data 99.417


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 26 {1}
2 35 {8}
3 44 {2}
4 29 {3}
5 85 {2}
6 64 {7}
7 66 {6}
8 97 {3}
9 21 {2}
Round 130, Devices participated 10, Average loss 0.015, Central accuracy on global test data 78.320, Ensemble accuracy on global test data 76.084, Local accuracy on global train data 27.231, Local accuracy on local train data 99.533


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 78 {9}
1 73 {1, 2}
2 10 {7}
3 52 {1}
4 54 {7}
5 24 {2}
6 44 {2}
7 70 {5}
8 99 {5}
9 90 {5}
Round 131, Devices participated 10, Average loss 0.022, Central accuracy on global test data 77.012, Ensemble accuracy on global test data 70.068, Local accuracy on global train data 26.675, Local accuracy on local train data 99.483


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 96 {0}
1 38 {4, 5}
2 1 {4}
3 4 {8}
4 36 {5}
5 19 {9}
6 77 {6}
7 52 {1}
8 41 {8, 9}
9 49 {7}
Round 132, Devices participated 10, Average loss 0.024, Central accuracy on global test data 84.131, Ensemble accuracy on global test data 83.760, Local accuracy on global train data 36.565, Local accuracy on local train data 99.200


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 87 {1}
1 55 {3}
2 49 {7}
3 7 {8}
4 51 {5, 6}
5 67 {1}
6 40 {2}
7 21 {2}
8 34 {3}
9 0 {8, 7}
Round 133, Devices participated 10, Average loss 0.020, Central accuracy on global test data 79.629, Ensemble accuracy on global test data 78.906, Local accuracy on global train data 30.581, Local accuracy on local train data 99.383


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
0 74 {4}
1 63 {5}
2 39 {1}
3 14 {6}
4 96 {0}
5 82 {6}
6 21 {2}
7 45 {6}
8 88 {6, 7}
9 13 {4}
Round 134, Devices participated 10, Average loss 0.013, Central accuracy on global test data 75.518, Ensemble accuracy on global test data 74.814, Local accuracy on global train data 38.286, Local accuracy on local train data 99.650


{'layer_input.weight': 1.0, 'layer_input.bias': 1.0, 'layer_hidden1.weight': 1.0, 'layer_hidden1.bias': 1.0, 'layer_hidden2.weight': 1.0, 'layer_hidden2.bias': 1.0}
/workspace/models/DFAN.py:363: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/models/DFAN.py:383: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
^CTraceback (most recent call last):
  File "main_dfan_regavg.py", line 148, in <module>
    DFAN_regavg(args, net_locals, net_glob, generator, (optimizer_glob, optimizer_gen), epoch_idx)
  File "/workspace/models/DFAN.py", line 373, in DFAN_regavg
    z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
KeyboardInterrupt
