
python main_dfan_multigen.py --dataset mnist --model mlp --epochs 200 --gpu 0 --num_channels 1 --local_bs 10 --frac 0.1 --local_ep 1 --momentum 0.5 --lr_S 0.1 --lr_G 0.01 --nz 100 --epoch_itrs 400 --nn_refresh 0 --store_models testtemp_mgen --alpha_scale 0.5

def DFAN_multigen(args, teacher, student, proxy, generator, optimizer, epoch):

    loss_G = []
    for ii in range(10):
        teacher[ii].eval()
        generator[ii].train()
        proxy[ii].train()
    student.train()
    loss_S = torch.tensor(0.0)
    optimizer_P, optimizer_G = optimizer
    sm = torch.nn.Softmax()

    w_locals = []
    for local in teacher:
        w_locals.append(copy.deepcopy(local.state_dict()))
    w_fedavg = FedAvg(w_locals)

    for ii in range(10):
        proxy[ii].load_state_dict(w_fedavg)

    for i in range(args.epoch_itrs):
        for k in range(2):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            for ii in range(10):
                optimizer_G[ii].zero_grad()
                fake = generator[ii](z)
                fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
                s_logit = proxy[ii](fake)
                t_logit = teacher[ii](fake)
                oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
                max_Gout = torch.max(torch.abs(fake))
                if max_Gout > 8.0:
                    loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0) + torch.pow(max_Gout - 8.0, 2.0)
                    print(max_Gout)
                else:
                    loss_G = torch.log(2.-oneMinus_P_S) + torch.pow(fake.var() - 1.0, 2.0)
                loss_G.backward()                   
                optimizer_G[ii].step()
        for j in range(5):
            z = torch.randn((args.batch_size, args.nz, 1, 1)).to(args.device)
            for ii in range(10):
                optimizer_P[ii].zero_grad()
                fake = generator[ii](z).detach()
                fake = fake.view(-1, args.img_size[0], args.img_size[1], args.img_size[2])
                s_logit = proxy[ii](fake)
                t_logit = teacher[ii](fake).detach()
                oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
                loss_S1 = torch.log(1. / (2. - oneMinus_P_S))

                diff_L2 = torch.FloatTensor([0.]).to(args.device)
                for ln, student_w, fedavg_w in zip(student.state_dict().keys(), student.parameters(), w_fedavg.values()):
                    diff_L2 += (fedavg_w - student_w).norm(2) * args.alpha_scale
                if epoch == 0:
                    loss_S2 = 0
                else:
                    loss_S2 = diff_L2

                loss_S = loss_S1 + loss_S2
                loss_S.backward()
                optimizer_P[ii].step()

        w_proxy = []
        for p in proxy:
            w_proxy.append(copy.deepcopy(p.state_dict()))
        w_fedavg = FedAvg(w_proxy)
        student.load_state_dict(w_fedavg)

        if args.verbose and i % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tG_Loss: {:.6f} S_loss: {:.6f}'.format(
                epoch, i, args.epoch_itrs, 100 * float(i) / float(args.epoch_itrs), loss_G.item(), loss_S.item()))        #vp.add_scalar('Loss_S', (epoch-1)*args.epoch_itrs+i, loss_S.item())
            #vp.add_scalar('Loss_G', (epoch-1)*args.epoch_itrs+i, loss_G.item())



MLP(
  (layer_input): Linear(in_features=1024, out_features=200, bias=True)
  (relu): ReLU()
  (layer_hidden1): Linear(in_features=200, out_features=200, bias=True)
  (layer_hidden2): Linear(in_features=200, out_features=10, bias=True)
)
/workspace/deepedge/models/DFAN.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn("reduction: 'mean' divides the total loss by both the batch size and the support size."
/workspace/deepedge/models/DFAN.py:154: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  oneMinus_P_S = torch.tanh(F.kl_div(F.log_softmax(s_logit, dim=1), sm(t_logit)))
Round   0, Devices participated 10, Average loss 0.551, Central accuracy on global test data 28.979, Ensemble accuracy on global test data 27.319, Local accuracy on global train data 19.631, Local accuracy on local train data 87.850


Round   1, Devices participated 10, Average loss 0.302, Central accuracy on global test data 21.729, Ensemble accuracy on global test data 18.872, Local accuracy on global train data 17.185, Local accuracy on local train data 91.200


Round   2, Devices participated 10, Average loss 0.287, Central accuracy on global test data 34.277, Ensemble accuracy on global test data 32.983, Local accuracy on global train data 19.077, Local accuracy on local train data 93.050


Round   3, Devices participated 10, Average loss 0.235, Central accuracy on global test data 41.162, Ensemble accuracy on global test data 42.969, Local accuracy on global train data 20.283, Local accuracy on local train data 94.317


Round   4, Devices participated 10, Average loss 0.210, Central accuracy on global test data 43.311, Ensemble accuracy on global test data 41.821, Local accuracy on global train data 18.560, Local accuracy on local train data 95.100


Round   5, Devices participated 10, Average loss 0.193, Central accuracy on global test data 37.329, Ensemble accuracy on global test data 37.744, Local accuracy on global train data 20.039, Local accuracy on local train data 94.950


Round   6, Devices participated 10, Average loss 0.194, Central accuracy on global test data 47.192, Ensemble accuracy on global test data 47.363, Local accuracy on global train data 18.645, Local accuracy on local train data 94.617


Round   7, Devices participated 10, Average loss 0.222, Central accuracy on global test data 58.838, Ensemble accuracy on global test data 60.571, Local accuracy on global train data 19.802, Local accuracy on local train data 93.483


Round   8, Devices participated 10, Average loss 0.164, Central accuracy on global test data 48.242, Ensemble accuracy on global test data 51.416, Local accuracy on global train data 18.601, Local accuracy on local train data 95.833


Round   9, Devices participated 10, Average loss 0.175, Central accuracy on global test data 45.435, Ensemble accuracy on global test data 49.170, Local accuracy on global train data 19.827, Local accuracy on local train data 94.467


Round  10, Devices participated 10, Average loss 0.170, Central accuracy on global test data 56.836, Ensemble accuracy on global test data 56.006, Local accuracy on global train data 21.331, Local accuracy on local train data 94.733


Round  11, Devices participated 10, Average loss 0.122, Central accuracy on global test data 54.199, Ensemble accuracy on global test data 54.688, Local accuracy on global train data 21.870, Local accuracy on local train data 97.117


Round  12, Devices participated 10, Average loss 0.219, Central accuracy on global test data 54.590, Ensemble accuracy on global test data 55.859, Local accuracy on global train data 22.222, Local accuracy on local train data 92.183


Round  13, Devices participated 10, Average loss 0.157, Central accuracy on global test data 52.490, Ensemble accuracy on global test data 55.615, Local accuracy on global train data 19.082, Local accuracy on local train data 95.333


Round  14, Devices participated 10, Average loss 0.236, Central accuracy on global test data 57.031, Ensemble accuracy on global test data 55.908, Local accuracy on global train data 24.766, Local accuracy on local train data 91.717


Round  15, Devices participated 10, Average loss 0.135, Central accuracy on global test data 58.960, Ensemble accuracy on global test data 59.082, Local accuracy on global train data 20.420, Local accuracy on local train data 95.967


Round  16, Devices participated 10, Average loss 0.161, Central accuracy on global test data 61.499, Ensemble accuracy on global test data 63.623, Local accuracy on global train data 22.429, Local accuracy on local train data 94.950


Round  17, Devices participated 10, Average loss 0.113, Central accuracy on global test data 54.175, Ensemble accuracy on global test data 53.467, Local accuracy on global train data 21.311, Local accuracy on local train data 97.017


Round  18, Devices participated 10, Average loss 0.198, Central accuracy on global test data 49.927, Ensemble accuracy on global test data 53.931, Local accuracy on global train data 22.444, Local accuracy on local train data 92.767


Round  19, Devices participated 10, Average loss 0.112, Central accuracy on global test data 57.422, Ensemble accuracy on global test data 60.034, Local accuracy on global train data 20.269, Local accuracy on local train data 96.967


Round  20, Devices participated 10, Average loss 0.154, Central accuracy on global test data 49.048, Ensemble accuracy on global test data 48.682, Local accuracy on global train data 21.890, Local accuracy on local train data 94.867


Round  21, Devices participated 10, Average loss 0.139, Central accuracy on global test data 52.148, Ensemble accuracy on global test data 54.443, Local accuracy on global train data 19.055, Local accuracy on local train data 95.350


Round  22, Devices participated 10, Average loss 0.133, Central accuracy on global test data 57.568, Ensemble accuracy on global test data 61.035, Local accuracy on global train data 23.088, Local accuracy on local train data 95.217


Round  23, Devices participated 10, Average loss 0.150, Central accuracy on global test data 43.970, Ensemble accuracy on global test data 41.870, Local accuracy on global train data 22.412, Local accuracy on local train data 96.050


Round  24, Devices participated 10, Average loss 0.160, Central accuracy on global test data 54.370, Ensemble accuracy on global test data 54.712, Local accuracy on global train data 22.263, Local accuracy on local train data 94.983


Round  25, Devices participated 10, Average loss 0.122, Central accuracy on global test data 57.178, Ensemble accuracy on global test data 57.983, Local accuracy on global train data 23.606, Local accuracy on local train data 96.433


Round  26, Devices participated 10, Average loss 0.139, Central accuracy on global test data 54.150, Ensemble accuracy on global test data 54.395, Local accuracy on global train data 23.943, Local accuracy on local train data 95.783


Round  27, Devices participated 10, Average loss 0.139, Central accuracy on global test data 50.098, Ensemble accuracy on global test data 49.194, Local accuracy on global train data 25.569, Local accuracy on local train data 95.733


Round  28, Devices participated 10, Average loss 0.111, Central accuracy on global test data 55.225, Ensemble accuracy on global test data 54.932, Local accuracy on global train data 27.170, Local accuracy on local train data 96.700


Round  29, Devices participated 10, Average loss 0.189, Central accuracy on global test data 62.622, Ensemble accuracy on global test data 63.281, Local accuracy on global train data 24.373, Local accuracy on local train data 93.983


Round  30, Devices participated 10, Average loss 0.140, Central accuracy on global test data 47.217, Ensemble accuracy on global test data 47.266, Local accuracy on global train data 20.720, Local accuracy on local train data 95.483


Round  31, Devices participated 10, Average loss 0.145, Central accuracy on global test data 54.858, Ensemble accuracy on global test data 51.196, Local accuracy on global train data 23.586, Local accuracy on local train data 95.300


q Round  32, Devices participated 10, Average loss 0.119, Central accuracy on global test data 63.184, Ensemble accuracy on global test data 62.842, Local accuracy on global train data 24.133, Local accuracy on local train data 96.600


Round  33, Devices participated 10, Average loss 0.138, Central accuracy on global test data 50.854, Ensemble accuracy on global test data 51.367, Local accuracy on global train data 21.570, Local accuracy on local train data 95.567


Round  34, Devices participated 10, Average loss 0.079, Central accuracy on global test data 55.054, Ensemble accuracy on global test data 56.079, Local accuracy on global train data 23.003, Local accuracy on local train data 97.783


tensor(8.8733, device='cuda:0', grad_fn=<MaxBackward1>)
Round  35, Devices participated 10, Average loss 0.127, Central accuracy on global test data 61.719, Ensemble accuracy on global test data 60.474, Local accuracy on global train data 23.372, Local accuracy on local train data 96.033


Round  36, Devices participated 10, Average loss 0.141, Central accuracy on global test data 57.178, Ensemble accuracy on global test data 56.665, Local accuracy on global train data 24.126, Local accuracy on local train data 95.433


Round  37, Devices participated 10, Average loss 0.194, Central accuracy on global test data 55.078, Ensemble accuracy on global test data 52.954, Local accuracy on global train data 24.397, Local accuracy on local train data 93.283

